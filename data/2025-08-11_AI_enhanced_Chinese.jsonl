{"id": "2508.05731", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05731", "abs": "https://arxiv.org/abs/2508.05731", "authors": ["Yuhang Liu", "Zeyu Liu", "Shuanghe Zhu", "Pengxiang Li", "Congkai Xie", "Jiasheng Wang", "Xueyu Hu", "Xiaotian Han", "Jianbo Yuan", "Xinyao Wang", "Shengyu Zhang", "Hongxia Yang", "Fei Wu"], "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization", "comment": "11 pages, 3 figures", "summary": "The emergence of Multimodal Large Language Models (MLLMs) has propelled the\ndevelopment of autonomous agents that operate on Graphical User Interfaces\n(GUIs) using pure visual input. A fundamental challenge is robustly grounding\nnatural language instructions. This requires a precise spatial alignment, which\naccurately locates the coordinates of each element, and, more critically, a\ncorrect semantic alignment, which matches the instructions to the functionally\nappropriate UI element. Although Reinforcement Learning with Verifiable Rewards\n(RLVR) has proven to be effective at improving spatial alignment for these\nMLLMs, we find that inefficient exploration bottlenecks semantic alignment,\nwhich prevent models from learning difficult semantic associations. To address\nthis exploration problem, we present Adaptive Exploration Policy Optimization\n(AEPO), a new policy optimization framework. AEPO employs a multi-answer\ngeneration strategy to enforce broader exploration, which is then guided by a\ntheoretically grounded Adaptive Exploration Reward (AER) function derived from\nfirst principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B\nand InfiGUI-G1-7B, establish new state-of-the-art results across multiple\nchallenging GUI grounding benchmarks, achieving significant relative\nimprovements of up to 9.0% against the naive RLVR baseline on benchmarks\ndesigned to test generalization and semantic understanding. Resources are\navailable at https://github.com/InfiXAI/InfiGUI-G1.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAEPO\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7b54\u6848\u751f\u6210\u548c\u81ea\u9002\u5e94\u63a2\u7d22\u5956\u52b1\u63d0\u5347MLLMs\u5728GUI\u4efb\u52a1\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3MLLMs\u5728GUI\u4efb\u52a1\u4e2d\u56e0\u63a2\u7d22\u6548\u7387\u4f4e\u5bfc\u81f4\u7684\u8bed\u4e49\u5bf9\u9f50\u74f6\u9888\u95ee\u9898\u3002", "method": "\u63d0\u51faAEPO\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u7b54\u6848\u751f\u6210\u7b56\u7565\u548c\u81ea\u9002\u5e94\u63a2\u7d22\u5956\u52b1\uff08AER\uff09\u51fd\u6570\u3002", "result": "\u5728\u591a\u4e2aGUI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u7ed3\u679c\uff0c\u76f8\u5bf9\u57fa\u7ebf\u63d0\u5347\u8fbe9.0%\u3002", "conclusion": "AEPO\u6709\u6548\u63d0\u5347\u8bed\u4e49\u5bf9\u9f50\u80fd\u529b\uff0c\u4e3aGUI\u4efb\u52a1\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05766", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2508.05766", "abs": "https://arxiv.org/abs/2508.05766", "authors": ["Bo Wen"], "title": "A Framework for Inherently Safer AGI through Language-Mediated Active Inference", "comment": null, "summary": "This paper proposes a novel framework for developing safe Artificial General\nIntelligence (AGI) by combining Active Inference principles with Large Language\nModels (LLMs). We argue that traditional approaches to AI safety, focused on\npost-hoc interpretability and reward engineering, have fundamental limitations.\nWe present an architecture where safety guarantees are integrated into the\nsystem's core design through transparent belief representations and\nhierarchical value alignment. Our framework leverages natural language as a\nmedium for representing and manipulating beliefs, enabling direct human\noversight while maintaining computational tractability. The architecture\nimplements a multi-agent system where agents self-organize according to Active\nInference principles, with preferences and safety constraints flowing through\nhierarchical Markov blankets. We outline specific mechanisms for ensuring\nsafety, including: (1) explicit separation of beliefs and preferences in\nnatural language, (2) bounded rationality through resource-aware free energy\nminimization, and (3) compositional safety through modular agent structures.\nThe paper concludes with a research agenda centered on the Abstraction and\nReasoning Corpus (ARC) benchmark, proposing experiments to validate our\nframework's safety properties. Our approach offers a path toward AGI\ndevelopment that is inherently safer, rather than retrofitted with safety\nmeasures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e3b\u52a8\u63a8\u7406\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u53d1\u5b89\u5168\u7684\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\uff0c\u5f3a\u8c03\u5c06\u5b89\u5168\u6027\u878d\u5165\u7cfb\u7edf\u6838\u5fc3\u8bbe\u8ba1\u3002", "motivation": "\u4f20\u7edfAI\u5b89\u5168\u65b9\u6cd5\uff08\u5982\u4e8b\u540e\u53ef\u89e3\u91ca\u6027\u548c\u5956\u52b1\u5de5\u7a0b\uff09\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u9700\u5c06\u5b89\u5168\u6027\u878d\u5165\u7cfb\u7edf\u6838\u5fc3\u8bbe\u8ba1\u3002", "method": "\u901a\u8fc7\u900f\u660e\u4fe1\u5ff5\u8868\u793a\u548c\u5206\u5c42\u4ef7\u503c\u5bf9\u9f50\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u8868\u793a\u548c\u64cd\u4f5c\u4fe1\u5ff5\uff0c\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u81ea\u7ec4\u7ec7\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u660e\u786e\u4fe1\u5ff5\u4e0e\u504f\u597d\u5206\u79bb\u3001\u8d44\u6e90\u611f\u77e5\u81ea\u7531\u80fd\u6700\u5c0f\u5316\u548c\u6a21\u5757\u5316\u7ed3\u6784\u7684\u6846\u67b6\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAGI\u5f00\u53d1\u63d0\u4f9b\u4e86\u4e00\u6761\u66f4\u5b89\u5168\u7684\u65b0\u8def\u5f84\uff0c\u5e76\u901a\u8fc7ARC\u57fa\u51c6\u9a8c\u8bc1\u5176\u5b89\u5168\u6027\u3002"}}
{"id": "2508.05776", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05776", "abs": "https://arxiv.org/abs/2508.05776", "authors": ["Thomas L. Griffiths", "Brenden M. Lake", "R. Thomas McCoy", "Ellie Pavlick", "Taylor W. Webb"], "title": "Whither symbols in the era of advanced neural networks?", "comment": null, "summary": "Some of the strongest evidence that human minds should be thought about in\nterms of symbolic systems has been the way they combine ideas, produce novelty,\nand learn quickly. We argue that modern neural networks -- and the artificial\nintelligence systems built upon them -- exhibit similar abilities. This\nundermines the argument that the cognitive processes and representations used\nby human minds are symbolic, although the fact that these neural networks are\ntypically trained on data generated by symbolic systems illustrates that such\nsystems play an important role in characterizing the abstract problems that\nhuman minds have to solve. This argument leads us to offer a new agenda for\nresearch on the symbolic basis of human thought.", "AI": {"tldr": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u5c55\u793a\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u601d\u7ef4\u7684\u7ec4\u5408\u3001\u521b\u65b0\u548c\u5feb\u901f\u5b66\u4e60\u80fd\u529b\uff0c\u6311\u6218\u4e86\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u662f\u7b26\u53f7\u5316\u7684\u89c2\u70b9\u3002", "motivation": "\u63a2\u8ba8\u795e\u7ecf\u7f51\u7edc\u662f\u5426\u5177\u5907\u7c7b\u4f3c\u4eba\u7c7b\u601d\u7ef4\u7684\u7b26\u53f7\u5316\u80fd\u529b\uff0c\u4ece\u800c\u91cd\u65b0\u5ba1\u89c6\u4eba\u7c7b\u601d\u7ef4\u7684\u7b26\u53f7\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u73b0\uff0c\u5bf9\u6bd4\u4eba\u7c7b\u601d\u7ef4\u7684\u7b26\u53f7\u5316\u7279\u5f81\u3002", "result": "\u795e\u7ecf\u7f51\u7edc\u5c55\u793a\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u601d\u7ef4\u7684\u7ec4\u5408\u548c\u521b\u65b0\u80fd\u529b\uff0c\u8868\u660e\u7b26\u53f7\u7cfb\u7edf\u5e76\u975e\u4eba\u7c7b\u8ba4\u77e5\u7684\u552f\u4e00\u89e3\u91ca\u3002", "conclusion": "\u63d0\u51fa\u65b0\u7684\u7814\u7a76\u8bae\u7a0b\uff0c\u91cd\u65b0\u63a2\u8ba8\u4eba\u7c7b\u601d\u7ef4\u7684\u7b26\u53f7\u57fa\u7840\u3002"}}
{"id": "2508.05792", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05792", "abs": "https://arxiv.org/abs/2508.05792", "authors": ["Kausik Lakkaraju", "Siva Likitha Valluru", "Biplav Srivastava"], "title": "Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making", "comment": null, "summary": "Current eXplainable AI (XAI) methods largely serve developers, often focusing\non justifying model outputs rather than supporting diverse stakeholder needs. A\nrecent shift toward Evaluative AI reframes explanation as a tool for hypothesis\ntesting, but still focuses primarily on operational organizations. We introduce\nHolistic-XAI (H-XAI), a unified framework that integrates causal rating methods\nwith traditional XAI methods to support explanation as an interactive,\nmulti-method process. H-XAI allows stakeholders to ask a series of questions,\ntest hypotheses, and compare model behavior against automatically constructed\nrandom and biased baselines. It combines instance-level and global\nexplanations, adapting to each stakeholder's goals, whether understanding\nindividual decisions, assessing group-level bias, or evaluating robustness\nunder perturbations. We demonstrate the generality of our approach through two\ncase studies spanning six scenarios: binary credit risk classification and\nfinancial time-series forecasting. H-XAI fills critical gaps left by existing\nXAI methods by combining causal ratings and post-hoc explanations to answer\nstakeholder-specific questions at both the individual decision level and the\noverall model level.", "AI": {"tldr": "H-XAI\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7ed3\u5408\u56e0\u679c\u8bc4\u7ea7\u4e0e\u4f20\u7edfXAI\u65b9\u6cd5\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u3001\u591a\u65b9\u6cd5\u7684\u89e3\u91ca\u8fc7\u7a0b\uff0c\u6ee1\u8db3\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\u3002", "motivation": "\u73b0\u6709XAI\u65b9\u6cd5\u4e3b\u8981\u670d\u52a1\u4e8e\u5f00\u53d1\u8005\uff0c\u7f3a\u4e4f\u5bf9\u591a\u6837\u5316\u5229\u76ca\u76f8\u5173\u8005\u9700\u6c42\u7684\u652f\u6301\u3002H-XAI\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u89e3\u91ca\u5de5\u5177\u3002", "method": "H-XAI\u6574\u5408\u56e0\u679c\u8bc4\u7ea7\u4e0e\u4f20\u7edfXAI\u65b9\u6cd5\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u95ee\u9898\u63d0\u95ee\u3001\u5047\u8bbe\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u968f\u673a\u548c\u504f\u7f6e\u57fa\u7ebf\u5bf9\u6bd4\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\uff08\u4fe1\u7528\u98ce\u9669\u5206\u7c7b\u548c\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff09\u5c55\u793a\u4e86H-XAI\u7684\u901a\u7528\u6027\u3002", "conclusion": "H-XAI\u586b\u8865\u4e86\u73b0\u6709XAI\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u7ed3\u5408\u56e0\u679c\u8bc4\u7ea7\u548c\u540e\u9a8c\u89e3\u91ca\uff0c\u6ee1\u8db3\u5229\u76ca\u76f8\u5173\u8005\u5728\u4e2a\u4f53\u51b3\u7b56\u548c\u6574\u4f53\u6a21\u578b\u5c42\u9762\u7684\u9700\u6c42\u3002"}}
{"id": "2508.05689", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05689", "abs": "https://arxiv.org/abs/2508.05689", "authors": ["Jinjia Peng", "Zeze Tao", "Huibing Wang", "Meng Wang", "Yang Wang"], "title": "Boosting Adversarial Transferability via Residual Perturbation Attack", "comment": "Accepted to ieee/cvf international conference on computer vision\n  (ICCV2025)", "summary": "Deep neural networks are susceptible to adversarial examples while suffering\nfrom incorrect predictions via imperceptible perturbations. Transfer-based\nattacks create adversarial examples for surrogate models and transfer these\nexamples to target models under black-box scenarios. Recent studies reveal that\nadversarial examples in flat loss landscapes exhibit superior transferability\nto alleviate overfitting on surrogate models. However, the prior arts overlook\nthe influence of perturbation directions, resulting in limited transferability.\nIn this paper, we propose a novel attack method, named Residual Perturbation\nAttack (ResPA), relying on the residual gradient as the perturbation direction\nto guide the adversarial examples toward the flat regions of the loss function.\nSpecifically, ResPA conducts an exponential moving average on the input\ngradients to obtain the first moment as the reference gradient, which\nencompasses the direction of historical gradients. Instead of heavily relying\non the local flatness that stems from the current gradients as the perturbation\ndirection, ResPA further considers the residual between the current gradient\nand the reference gradient to capture the changes in the global perturbation\ndirection. The experimental results demonstrate the better transferability of\nResPA than the existing typical transfer-based attack methods, while the\ntransferability can be further improved by combining ResPA with the current\ninput transformation methods. The code is available at\nhttps://github.com/ZezeTao/ResPA.", "AI": {"tldr": "ResPA\u662f\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b8b\u5dee\u68af\u5ea6\u5f15\u5bfc\u5bf9\u6297\u6837\u672c\u5411\u635f\u5931\u51fd\u6570\u7684\u5e73\u5766\u533a\u57df\u79fb\u52a8\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8fc1\u79fb\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u6270\u52a8\u65b9\u5411\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u6027\u53d7\u9650\u3002", "method": "\u5229\u7528\u6b8b\u5dee\u68af\u5ea6\u4f5c\u4e3a\u6270\u52a8\u65b9\u5411\uff0c\u7ed3\u5408\u5386\u53f2\u68af\u5ea6\u7684\u53c2\u8003\u65b9\u5411\uff0c\u4f18\u5316\u5bf9\u6297\u6837\u672c\u7684\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cResPA\u5728\u8fc1\u79fb\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u4e0e\u8f93\u5165\u53d8\u6362\u65b9\u6cd5\u7ed3\u5408\u540e\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "ResPA\u901a\u8fc7\u5168\u5c40\u6270\u52a8\u65b9\u5411\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u6027\u3002"}}
{"id": "2508.05855", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05855", "abs": "https://arxiv.org/abs/2508.05855", "authors": ["Zixia Wang", "Jia Hu", "Ronghui Mu"], "title": "Safety of Embodied Navigation: A Survey", "comment": null, "summary": "As large language models (LLMs) continue to advance and gain influence, the\ndevelopment of embodied AI has accelerated, drawing significant attention,\nparticularly in navigation scenarios. Embodied navigation requires an agent to\nperceive, interact with, and adapt to its environment while moving toward a\nspecified target in unfamiliar settings. However, the integration of embodied\nnavigation into critical applications raises substantial safety concerns. Given\ntheir deployment in dynamic, real-world environments, ensuring the safety of\nsuch systems is critical. This survey provides a comprehensive analysis of\nsafety in embodied navigation from multiple perspectives, encompassing attack\nstrategies, defense mechanisms, and evaluation methodologies. Beyond conducting\na comprehensive examination of existing safety challenges, mitigation\ntechnologies, and various datasets and metrics that assess effectiveness and\nrobustness, we explore unresolved issues and future research directions in\nembodied navigation safety. These include potential attack methods, mitigation\nstrategies, more reliable evaluation techniques, and the implementation of\nverification frameworks. By addressing these critical gaps, this survey aims to\nprovide valuable insights that can guide future research toward the development\nof safer and more reliable embodied navigation systems. Furthermore, the\nfindings of this study have broader implications for enhancing societal safety\nand increasing industrial efficiency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u5177\u8eab\u5bfc\u822a\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5206\u6790\u4e86\u653b\u51fb\u7b56\u7565\u3001\u9632\u5fa1\u673a\u5236\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u5177\u8eabAI\u7684\u53d1\u5c55\uff0c\u5177\u8eab\u5bfc\u822a\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u5206\u6790\u548c\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u591a\u89d2\u5ea6\u7efc\u5408\u5206\u6790\uff0c\u5305\u62ec\u653b\u51fb\u7b56\u7565\u3001\u9632\u5fa1\u6280\u672f\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u672a\u89e3\u51b3\u7684\u95ee\u9898\u548c\u672a\u6765\u65b9\u5411\u3002", "result": "\u8bba\u6587\u603b\u7ed3\u4e86\u73b0\u6709\u5b89\u5168\u6311\u6218\u3001\u7f13\u89e3\u6280\u672f\u53ca\u8bc4\u4f30\u5de5\u5177\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u5b89\u5168\u53ef\u9760\u7684\u5177\u8eab\u5bfc\u822a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5e76\u5bf9\u793e\u4f1a\u5b89\u5168\u548c\u5de5\u4e1a\u6548\u7387\u6709\u5e7f\u6cdb\u5f71\u54cd\u3002"}}
{"id": "2508.05732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05732", "abs": "https://arxiv.org/abs/2508.05732", "authors": ["Pinxuan Li", "Bing Cao", "Changqing Zhang", "Qinghua Hu"], "title": "Generalized Few-Shot Out-of-Distribution Detection", "comment": null, "summary": "Few-shot Out-of-Distribution (OOD) detection has emerged as a critical\nresearch direction in machine learning for practical deployment. Most existing\nFew-shot OOD detection methods suffer from insufficient generalization\ncapability for the open world. Due to the few-shot learning paradigm, the OOD\ndetection ability is often overfit to the limited training data itself, thus\ndegrading the performance on generalized data and performing inconsistently\nacross different scenarios. To address this challenge, we proposed a\nGeneralized Few-shot OOD Detection (GOOD) framework, which empowers the general\nknowledge of the OOD detection model with an auxiliary General Knowledge Model\n(GKM), instead of directly learning from few-shot data. We proceed to reveal\nthe few-shot OOD detection from a generalization perspective and theoretically\nderive the Generality-Specificity balance (GS-balance) for OOD detection, which\nprovably reduces the upper bound of generalization error with a general\nknowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE)\nmechanism to adaptively modulate the guidance of general knowledge. KDE\ndynamically aligns the output distributions of the OOD detection model to the\ngeneral knowledge model based on the Generalized Belief (G-Belief) of GKM,\nthereby boosting the GS-balance. Experiments on real-world OOD benchmarks\ndemonstrate our superiority. Codes will be available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u5c11\u6837\u672cOOD\u68c0\u6d4b\u6846\u67b6\uff08GOOD\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u901a\u7528\u77e5\u8bc6\u6a21\u578b\uff08GKM\uff09\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u6cdb\u5316\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5c11\u6837\u672cOOD\u68c0\u6d4b\u65b9\u6cd5\u56e0\u6570\u636e\u6709\u9650\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u6027\u80fd\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faGOOD\u6846\u67b6\uff0c\u5229\u7528GKM\u589e\u5f3a\u6a21\u578b\u901a\u7528\u77e5\u8bc6\uff0c\u5e76\u63d0\u51fa\u77e5\u8bc6\u52a8\u6001\u5d4c\u5165\uff08KDE\uff09\u673a\u5236\u81ea\u9002\u5e94\u8c03\u6574\u901a\u7528\u77e5\u8bc6\u6307\u5bfc\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u771f\u5b9eOOD\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "GOOD\u6846\u67b6\u901a\u8fc7GS\u5e73\u8861\u548cKDE\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672cOOD\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.05888", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.05888", "abs": "https://arxiv.org/abs/2508.05888", "authors": ["Sahil Bansal", "Sai Shruthi Sistla", "Aarti Arikatala", "Sebastian Schreiber"], "title": "Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning", "comment": null, "summary": "Effective tool retrieval is essential for AI agents to select from a vast\narray of tools when identifying and planning actions in the context of complex\nuser queries. Despite its central role in planning, this aspect remains\nunderexplored in the literature. Traditional approaches rely primarily on\nsimilarities between user queries and tool descriptions, which significantly\nlimits retrieval accuracy, specifically when handling multi-step user requests.\nTo address these limitations, we propose a Knowledge Graph (KG)-based tool\nretrieval framework that captures the semantic relationships between tools and\ntheir functional dependencies. Our retrieval algorithm leverages ensembles of\n1-hop ego tool graphs to model direct and indirect connections between tools,\nenabling more comprehensive and contextual tool selection for multi-step tasks.\nWe evaluate our approach on a synthetically generated internal dataset across\nsix defined user classes, extending previous work on coherent dialogue\nsynthesis and too retrieval benchmarks. Results demonstrate that our tool\ngraph-based method achieves 91.85% tool coverage on the micro-average Complete\nRecall metric, compared to 89.26% for re-ranked semantic-lexical hybrid\nretrieval, the strongest non-KG baseline in our experiments. These findings\nsupport our hypothesis that the structural information in the KG provides\ncomplementary signals to pure similarity matching, particularly for queries\nrequiring sequential tool composition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u7684\u5de5\u5177\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u6355\u6349\u5de5\u5177\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\u548c\u529f\u80fd\u4f9d\u8d56\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6b65\u4efb\u52a1\u4e2d\u7684\u5de5\u5177\u68c0\u7d22\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u5de5\u5177\u68c0\u7d22\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7528\u6237\u67e5\u8be2\u4e0e\u5de5\u5177\u63cf\u8ff0\u7684\u76f8\u4f3c\u6027\uff0c\u9650\u5236\u4e86\u591a\u6b65\u8bf7\u6c42\u7684\u5904\u7406\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u68c0\u7d22\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u4e2d\u76841-hop ego\u5de5\u5177\u56fe\u96c6\u5408\uff0c\u5efa\u6a21\u5de5\u5177\u95f4\u7684\u76f4\u63a5\u548c\u95f4\u63a5\u8fde\u63a5\uff0c\u5b9e\u73b0\u66f4\u5168\u9762\u7684\u5de5\u5177\u9009\u62e9\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\uff0c\u57fa\u4e8e\u5de5\u5177\u56fe\u7684\u65b9\u6cd5\u5728Complete Recall\u6307\u6807\u4e0a\u8fbe\u523091.85%\uff0c\u4f18\u4e8e\u975eKG\u57fa\u7ebf\u65b9\u6cd5\u768489.26%\u3002", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u4fe1\u606f\u4e3a\u7eaf\u76f8\u4f3c\u6027\u5339\u914d\u63d0\u4f9b\u4e86\u8865\u5145\u4fe1\u53f7\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u987a\u5e8f\u5de5\u5177\u7ec4\u5408\u7684\u67e5\u8be2\u3002"}}
{"id": "2508.05755", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05755", "abs": "https://arxiv.org/abs/2508.05755", "authors": ["Agnieszka Polowczyk", "Alicja Polowczyk", "Dawid Malarz", "Artur Kasymov", "Marcin Mazur", "Jacek Tabor", "Przemys\u0142aw Spurek"], "title": "UnGuide: Learning to Forget with LoRA-Guided Diffusion Models", "comment": null, "summary": "Recent advances in large-scale text-to-image diffusion models have heightened\nconcerns about their potential misuse, especially in generating harmful or\nmisleading content. This underscores the urgent need for effective machine\nunlearning, i.e., removing specific knowledge or concepts from pretrained\nmodels without compromising overall performance. One possible approach is\nLow-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models\nfor targeted unlearning. However, LoRA often inadvertently alters unrelated\ncontent, leading to diminished image fidelity and realism. To address this\nlimitation, we introduce UnGuide -- a novel approach which incorporates\nUnGuidance, a dynamic inference mechanism that leverages Classifier-Free\nGuidance (CFG) to exert precise control over the unlearning process. UnGuide\nmodulates the guidance scale based on the stability of a few first steps of\ndenoising processes, enabling selective unlearning by LoRA adapter. For prompts\ncontaining the erased concept, the LoRA module predominates and is\ncounterbalanced by the base model; for unrelated prompts, the base model\ngoverns generation, preserving content fidelity. Empirical results demonstrate\nthat UnGuide achieves controlled concept removal and retains the expressive\npower of diffusion models, outperforming existing LoRA-based methods in both\nobject erasure and explicit content removal tasks.", "AI": {"tldr": "UnGuide\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u63a8\u7406\u673a\u5236UnGuidance\uff0c\u901a\u8fc7\u8c03\u6574\u5f15\u5bfc\u5c3a\u5ea6\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\uff0c\u9009\u62e9\u6027\u9057\u5fd8\u7279\u5b9a\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u53ef\u80fd\u88ab\u6ee5\u7528\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u9700\u8981\u6709\u6548\u7684\u65b9\u6cd5\u5728\u4e0d\u5f71\u54cd\u6574\u4f53\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u79fb\u9664\u7279\u5b9a\u77e5\u8bc6\u3002", "method": "\u7ed3\u5408LoRA\u548cClassifier-Free Guidance\uff08CFG\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5f15\u5bfc\u5c3a\u5ea6\u5b9e\u73b0\u9009\u62e9\u6027\u9057\u5fd8\u3002", "result": "UnGuide\u5728\u6982\u5ff5\u79fb\u9664\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u56fe\u50cf\u7684\u771f\u5b9e\u6027\u548c\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u3002", "conclusion": "UnGuide\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u9057\u5fd8\u673a\u5236\uff0c\u89e3\u51b3\u4e86LoRA\u5728\u9057\u5fd8\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u5f71\u54cd\u65e0\u5173\u5185\u5bb9\u7684\u95ee\u9898\u3002"}}
{"id": "2508.05996", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05996", "abs": "https://arxiv.org/abs/2508.05996", "authors": ["Kaitao Chen", "Mianxin Liu", "Daoming Zong", "Chaoyue Ding", "Shaohao Rui", "Yankai Jiang", "Mu Zhou", "Xiaosong Wang"], "title": "Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making", "comment": "14 pages, 4 figures", "summary": "Complex medical decision-making involves cooperative workflows operated by\ndifferent clinicians. Designing AI multi-agent systems can expedite and augment\nhuman-level clinical decision-making. Existing multi-agent researches primarily\nfocus on language-only tasks, yet their extension to multimodal scenarios\nremains challenging. A blind combination of diverse vision-language models\n(VLMs) can amplify an erroneous outcome interpretation. VLMs in general are\nless capable in instruction following and importantly self-reflection, compared\nto large language models (LLMs) of comparable sizes. This disparity largely\nconstrains VLMs' ability in cooperative workflows. In this study, we propose\nMedOrch, a mediator-guided multi-agent collaboration framework for medical\nmultimodal decision-making. MedOrch employs an LLM-based mediator agent that\nenables multiple VLM-based expert agents to exchange and reflect on their\noutputs towards collaboration. We utilize multiple open-source general-purpose\nand domain-specific VLMs instead of costly GPT-series models, revealing the\nstrength of heterogeneous models. We show that the collaboration within\ndistinct VLM-based agents can surpass the capabilities of any individual agent.\nWe validate our approach on five medical vision question answering benchmarks,\ndemonstrating superior collaboration performance without model training. Our\nfindings underscore the value of mediator-guided multi-agent collaboration in\nadvancing medical multimodal intelligence. Our code will be made publicly\navailable.", "AI": {"tldr": "\u63d0\u51faMedOrch\u6846\u67b6\uff0c\u901a\u8fc7LLM\u4e2d\u4ecb\u534f\u8c03\u591aVLM\u4e13\u5bb6\u4ee3\u7406\u534f\u4f5c\uff0c\u63d0\u5347\u533b\u7597\u591a\u6a21\u6001\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u591a\u4ee3\u7406\u7814\u7a76\u591a\u9650\u4e8e\u8bed\u8a00\u4efb\u52a1\uff0c\u591a\u6a21\u6001\u573a\u666f\u4e0bVLM\u534f\u4f5c\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u89e3\u51b3\u9519\u8bef\u7ed3\u679c\u653e\u5927\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528LLM\u4e2d\u4ecb\u534f\u8c03\u591a\u4e2aVLM\u4e13\u5bb6\u4ee3\u7406\uff0c\u901a\u8fc7\u8f93\u51fa\u4ea4\u6362\u4e0e\u53cd\u601d\u5b9e\u73b0\u534f\u4f5c\uff0c\u907f\u514d\u6602\u8d35GPT\u6a21\u578b\u3002", "result": "\u5728\u4e94\u4e2a\u533b\u7597\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u4e0a\u9a8c\u8bc1\uff0c\u534f\u4f5c\u6027\u80fd\u8d85\u8d8a\u5355\u4e2a\u4ee3\u7406\uff0c\u65e0\u9700\u6a21\u578b\u8bad\u7ec3\u3002", "conclusion": "\u4e2d\u4ecb\u5f15\u5bfc\u7684\u591a\u4ee3\u7406\u534f\u4f5c\u53ef\u63a8\u52a8\u533b\u7597\u591a\u6a21\u6001\u667a\u80fd\u53d1\u5c55\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2508.05769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05769", "abs": "https://arxiv.org/abs/2508.05769", "authors": ["Seyed Hadi Seyed", "Ayberk Cansever", "David Hart"], "title": "Improving Masked Style Transfer using Blended Partial Convolution", "comment": null, "summary": "Artistic style transfer has long been possible with the advancements of\nconvolution- and transformer-based neural networks. Most algorithms apply the\nartistic style transfer to the whole image, but individual users may only need\nto apply a style transfer to a specific region in the image. The standard\npractice is to simply mask the image after the stylization. This work shows\nthat this approach tends to improperly capture the style features in the region\nof interest. We propose a partial-convolution-based style transfer network that\naccurately applies the style features exclusively to the region of interest.\nAdditionally, we present network-internal blending techniques that account for\nimperfections in the region selection. We show that this visually and\nquantitatively improves stylization using examples from the SA-1B dataset. Code\nis publicly available at https://github.com/davidmhart/StyleTransferMasked.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u90e8\u5206\u5377\u79ef\u7684\u98ce\u683c\u8fc1\u79fb\u7f51\u7edc\uff0c\u80fd\u591f\u7cbe\u786e\u5730\u5c06\u98ce\u683c\u7279\u5f81\u5e94\u7528\u4e8e\u611f\u5174\u8da3\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u5185\u90e8\u6df7\u5408\u6280\u672f\u6539\u5584\u533a\u57df\u9009\u62e9\u4e0d\u5b8c\u7f8e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5bf9\u6574\u4e2a\u56fe\u50cf\u8fdb\u884c\u98ce\u683c\u8fc1\u79fb\uff0c\u800c\u7528\u6237\u53ef\u80fd\u4ec5\u9700\u5bf9\u7279\u5b9a\u533a\u57df\u5e94\u7528\u98ce\u683c\u3002\u4f20\u7edf\u63a9\u7801\u65b9\u6cd5\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u611f\u5174\u8da3\u533a\u57df\u7684\u98ce\u683c\u7279\u5f81\u3002", "method": "\u91c7\u7528\u90e8\u5206\u5377\u79ef\u7684\u98ce\u683c\u8fc1\u79fb\u7f51\u7edc\uff0c\u7ed3\u5408\u7f51\u7edc\u5185\u90e8\u6df7\u5408\u6280\u672f\uff0c\u4ee5\u5904\u7406\u533a\u57df\u9009\u62e9\u4e0d\u5b8c\u7f8e\u7684\u60c5\u51b5\u3002", "result": "\u5728SA-1B\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u548c\u5b9a\u91cf\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u63a9\u7801\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5c40\u90e8\u98ce\u683c\u8fc1\u79fb\u4e2d\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u98ce\u683c\u8fc1\u79fb\u7684\u51c6\u786e\u6027\u548c\u89c6\u89c9\u6548\u679c\u3002"}}
{"id": "2508.06042", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06042", "abs": "https://arxiv.org/abs/2508.06042", "authors": ["Daechul Ahn", "San Kim", "Jonghyun Choi"], "title": "Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning", "comment": "COLM 2025", "summary": "Large Language Models (LLMs) have recently demonstrated impressive action\nsequence prediction capabilities but often struggle with dynamic, long-horizon\ntasks such as real-time strategic games. In a game such as StarCraftII (SC2),\nagents need to manage resource constraints and adapt to evolving battlefield\nsituations in a partially observable environment. This often overwhelms\nexisiting LLM-based approaches. To address these challenges, we propose a\nhierarchical multi-agent framework that employs specialized imitation learning\nagents under a meta-controller called Strategic Planner (SP). By expert\ndemonstrations, each specialized agent learns a distinctive strategy, such as\naerial support or defensive maneuvers, and produces coherent, structured\nmultistep action sequences. The SP then orchestrates these proposals into a\nsingle, environmentally adaptive plan that ensures local decisions aligning\nwith long-term strategies. We call this HIMA (Hierarchical Imitation\nMulti-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that\nencompasses all race match combinations in SC2. Our empirical results show that\nHIMA outperforms state of the arts in strategic clarity, adaptability, and\ncomputational efficiency, underscoring the potential of combining specialized\nimitation modules with meta-level orchestration to develop more robust,\ngeneral-purpose AI agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHIMA\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7ed3\u5408\u4e13\u5bb6\u6a21\u4eff\u5b66\u4e60\u548c\u5143\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u63d0\u5347LLM\u5728\u52a8\u6001\u957f\u671f\u4efb\u52a1\uff08\u5982\u300a\u661f\u9645\u4e89\u9738II\u300b\uff09\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709LLM\u5728\u52a8\u6001\u3001\u957f\u671f\u4efb\u52a1\uff08\u5982\u300a\u661f\u9645\u4e89\u9738II\u300b\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u89e3\u51b3\u8d44\u6e90\u7ba1\u7406\u548c\u73af\u5883\u9002\u5e94\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u62ec\u4e13\u5bb6\u6a21\u4eff\u5b66\u4e60\u667a\u80fd\u4f53\u548c\u5143\u63a7\u5236\u5668\uff08Strategic Planner\uff09\uff0c\u751f\u6210\u591a\u6b65\u52a8\u4f5c\u5e8f\u5217\u5e76\u534f\u8c03\u4e3a\u7edf\u4e00\u8ba1\u5212\u3002", "result": "HIMA\u5728\u6218\u7565\u6e05\u6670\u6027\u3001\u9002\u5e94\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u4e13\u5bb6\u6a21\u4eff\u6a21\u5757\u548c\u5143\u7ea7\u534f\u8c03\u53ef\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u901a\u7528AI\u667a\u80fd\u4f53\u3002"}}
{"id": "2508.05772", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05772", "abs": "https://arxiv.org/abs/2508.05772", "authors": ["Can Zhao", "Pengfei Guo", "Dong Yang", "Yucheng Tang", "Yufan He", "Benjamin Simon", "Mason Belue", "Stephanie Harmon", "Baris Turkbey", "Daguang Xu"], "title": "MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss", "comment": null, "summary": "Medical image synthesis is an important topic for both clinical and research\napplications. Recently, diffusion models have become a leading approach in this\narea. Despite their strengths, many existing methods struggle with (1) limited\ngeneralizability that only work for specific body regions or voxel spacings,\n(2) slow inference, which is a common issue for diffusion models, and (3) weak\nalignment with input conditions, which is a critical issue for medical imaging.\nMAISI, a previously proposed framework, addresses generalizability issues but\nstill suffers from slow inference and limited condition consistency. In this\nwork, we present MAISI-v2, the first accelerated 3D medical image synthesis\nframework that integrates rectified flow to enable fast and high quality\ngeneration. To further enhance condition fidelity, we introduce a novel\nregion-specific contrastive loss to enhance the sensitivity to region of\ninterest. Our experiments show that MAISI-v2 can achieve SOTA image quality\nwith $33 \\times$ acceleration for latent diffusion model. We also conducted a\ndownstream segmentation experiment to show that the synthetic images can be\nused for data augmentation. We release our code, training details, model\nweights, and a GUI demo to facilitate reproducibility and promote further\ndevelopment within the community.", "AI": {"tldr": "MAISI-v2\u662f\u4e00\u4e2a\u52a0\u901f\u76843D\u533b\u5b66\u56fe\u50cf\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408rectified flow\u5b9e\u73b0\u5feb\u901f\u9ad8\u8d28\u91cf\u751f\u6210\uff0c\u5e76\u5f15\u5165\u533a\u57df\u7279\u5f02\u6027\u5bf9\u6bd4\u635f\u5931\u63d0\u5347\u6761\u4ef6\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5408\u6210\u4e2d\u5b58\u5728\u6cdb\u5316\u6027\u5dee\u3001\u63a8\u7406\u901f\u5ea6\u6162\u548c\u6761\u4ef6\u5bf9\u9f50\u5f31\u7684\u95ee\u9898\u3002", "method": "\u6574\u5408rectified flow\u52a0\u901f\u751f\u6210\uff0c\u5e76\u63d0\u51fa\u533a\u57df\u7279\u5f02\u6027\u5bf9\u6bd4\u635f\u5931\u589e\u5f3a\u6761\u4ef6\u654f\u611f\u6027\u3002", "result": "MAISI-v2\u5b9e\u73b0\u4e8633\u500d\u52a0\u901f\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u5e76\u8fbe\u5230SOTA\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "MAISI-v2\u5728\u52a0\u901f\u548c\u6761\u4ef6\u4e00\u81f4\u6027\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u9002\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u7b49\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2508.06060", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06060", "abs": "https://arxiv.org/abs/2508.06060", "authors": ["Sankarshan Damle", "Boi Faltings"], "title": "LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences", "comment": "Published in the Proceedings of the 28th European Conference on\n  Artificial Intelligence (ECAI 2025)", "summary": "Large Language Models (LLMs) are increasingly expected to handle complex\ndecision-making tasks, yet their ability to perform structured resource\nallocation remains underexplored. Evaluating their reasoning is also difficult\ndue to data contamination and the static nature of existing benchmarks. We\npresent a dual-purpose framework leveraging Participatory Budgeting (PB) both\nas (i) a practical setting for LLM-based resource allocation and (ii) an\nadaptive benchmark for evaluating their reasoning capabilities. We task LLMs\nwith selecting project subsets under feasibility (e.g., budget) constraints via\nthree prompting strategies: greedy selection, direct optimization, and a\nhill-climbing-inspired refinement. We benchmark LLMs' allocations against a\nutility-maximizing oracle. Interestingly, we also test whether LLMs can infer\nstructured preferences from natural-language voter input or metadata, without\nexplicit votes. By comparing allocations based on inferred preferences to those\nfrom ground-truth votes, we evaluate LLMs' ability to extract preferences from\nopen-ended input. Our results underscore the role of prompt design and show\nthat LLMs hold promise for mechanism design with unstructured inputs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u7528\u9014\u6846\u67b6\uff0c\u5229\u7528\u53c2\u4e0e\u5f0f\u9884\u7b97\uff08PB\uff09\u4f5c\u4e3aLLM\u8d44\u6e90\u5206\u914d\u7684\u5b9e\u8df5\u573a\u666f\u548c\u8bc4\u4f30\u5176\u63a8\u7406\u80fd\u529b\u7684\u81ea\u9002\u5e94\u57fa\u51c6\u3002", "motivation": "\u63a2\u7d22LLM\u5728\u7ed3\u6784\u5316\u8d44\u6e90\u5206\u914d\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u7684\u6570\u636e\u6c61\u67d3\u548c\u9759\u6001\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff08\u8d2a\u5a6a\u9009\u62e9\u3001\u76f4\u63a5\u4f18\u5316\u548c\u722c\u5c71\u5f0f\u4f18\u5316\uff09\u8ba9LLM\u5728\u53ef\u884c\u6027\u7ea6\u675f\u4e0b\u9009\u62e9\u9879\u76ee\u5b50\u96c6\uff0c\u5e76\u4e0e\u6548\u7528\u6700\u5927\u5316\u57fa\u51c6\u5bf9\u6bd4\u3002", "result": "\u7ed3\u679c\u8868\u660e\u63d0\u793a\u8bbe\u8ba1\u5bf9LLM\u8868\u73b0\u81f3\u5173\u91cd\u8981\uff0c\u4e14LLM\u5728\u65e0\u7ed3\u6784\u8f93\u5165\u673a\u5236\u8bbe\u8ba1\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "LLM\u5728\u8d44\u6e90\u5206\u914d\u548c\u504f\u597d\u63a8\u65ad\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u63d0\u793a\u8bbe\u8ba1\u662f\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2508.05783", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05783", "abs": "https://arxiv.org/abs/2508.05783", "authors": ["Mengyu Li", "Guoyao Shen", "Chad W. Farris", "Xin Zhang"], "title": "Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks", "comment": "30 pages, 8 figures, 7 tables", "summary": "Machine learning using transformers has shown great potential in medical\nimaging, but its real-world applicability remains limited due to the scarcity\nof annotated data. In this study, we propose a practical framework for the\nfew-shot deployment of pretrained MRI transformers in diverse brain imaging\ntasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a\nlarge-scale, multi-cohort brain MRI dataset comprising over 31 million slices,\nwe obtain highly transferable latent representations that generalize well\nacross tasks and datasets. For high-level tasks such as classification, a\nfrozen MAE encoder combined with a lightweight linear head achieves\nstate-of-the-art accuracy in MRI sequence identification with minimal\nsupervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a\nhybrid architecture that fuses multiscale CNN features with pretrained MAE\nembeddings. This model consistently outperforms other strong baselines in both\nskull stripping and multi-class anatomical segmentation under data-limited\nconditions. With extensive quantitative and qualitative evaluations, our\nframework demonstrates efficiency, stability, and scalability, suggesting its\nsuitability for low-resource clinical environments and broader neuroimaging\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3MRI\u53d8\u6362\u5668\u7684\u5c11\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8111\u6210\u50cf\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u63d0\u5347\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5c11\u6837\u672c\u6761\u4ef6\u4e0b\u7684\u9002\u7528\u6027\u3002", "method": "\u91c7\u7528Masked Autoencoder (MAE)\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u7ebf\u6027\u5934\u6216\u6df7\u5408\u67b6\u6784MAE-FUnet\u3002", "result": "\u5728\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u6761\u4ef6\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u6846\u67b6\u9ad8\u6548\u3001\u7a33\u5b9a\u4e14\u53ef\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u4e34\u5e8a\u73af\u5883\u548c\u795e\u7ecf\u5f71\u50cf\u5e94\u7528\u3002"}}
{"id": "2508.06062", "categories": ["cs.AI", "cs.LG", "cs.LO", "68T27, 68T30"], "pdf": "https://arxiv.org/pdf/2508.06062", "abs": "https://arxiv.org/abs/2508.06062", "authors": ["Evgenii E. Vityaev", "Andrei Mantsivoda"], "title": "Don't Forget Imagination!", "comment": "14 pages, 2 figures", "summary": "Cognitive imagination is a type of imagination that plays a key role in human\nthinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to\nmentally visualize coherent and holistic systems of concepts and causal links\nthat serve as semantic contexts for reasoning, decision making and prediction.\nOur position is that the role of cognitive imagination is still greatly\nunderestimated, and this creates numerous problems and diminishes the current\ncapabilities of AI. For instance, when reasoning, humans rely on imaginary\ncontexts to retrieve background info. They also constantly return to the\ncontext for semantic verification that their reasoning is still reasonable.\nThus, reasoning without imagination is blind. This paper is a call for greater\nattention to cognitive imagination as the next promising breakthrough in\nartificial intelligence. As an instrument for simulating cognitive imagination,\nwe propose semantic models -- a new approach to mathematical models that can\nlearn, like neural networks, and are based on probabilistic causal\nrelationships. Semantic models can simulate cognitive imagination because they\nensure the consistency of imaginary contexts and implement a glass-box approach\nthat allows the context to be manipulated as a holistic and coherent system of\ninterrelated facts glued together with causal relations.", "AI": {"tldr": "\u8bba\u6587\u547c\u5401\u91cd\u89c6\u8ba4\u77e5\u60f3\u8c61\u529b\u5728\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u8bed\u4e49\u6a21\u578b\u4f5c\u4e3a\u6a21\u62df\u8ba4\u77e5\u60f3\u8c61\u529b\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u8ba4\u77e5\u60f3\u8c61\u529b\u5728\u4eba\u7c7b\u601d\u7ef4\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u4f46\u76ee\u524d\u88ab\u4f4e\u4f30\uff0c\u9650\u5236\u4e86AI\u7684\u80fd\u529b\u3002\u8bba\u6587\u65e8\u5728\u5f3a\u8c03\u5176\u91cd\u8981\u6027\u5e76\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u6a21\u578b\uff0c\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u56e0\u679c\u5173\u7cfb\u7684\u6570\u5b66\u6a21\u578b\uff0c\u80fd\u591f\u5b66\u4e60\u548c\u6a21\u62df\u8ba4\u77e5\u60f3\u8c61\u529b\u3002", "result": "\u8bed\u4e49\u6a21\u578b\u80fd\u591f\u786e\u4fdd\u60f3\u8c61\u4e0a\u4e0b\u6587\u7684\u8fde\u8d2f\u6027\uff0c\u5e76\u5b9e\u73b0\u900f\u660e\u64cd\u4f5c\uff0c\u4e3aAI\u63a8\u7406\u63d0\u4f9b\u65b0\u5de5\u5177\u3002", "conclusion": "\u8ba4\u77e5\u60f3\u8c61\u529b\u662fAI\u53d1\u5c55\u7684\u5173\u952e\u7a81\u7834\u70b9\uff0c\u8bed\u4e49\u6a21\u578b\u4e3a\u5176\u6a21\u62df\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2508.05813", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05813", "abs": "https://arxiv.org/abs/2508.05813", "authors": ["Raphael Du Sablon", "David Hart"], "title": "Optimization-Free Style Transfer for 3D Gaussian Splats", "comment": null, "summary": "The task of style transfer for 3D Gaussian splats has been explored in many\nprevious works, but these require reconstructing or fine-tuning the splat while\nincorporating style information or optimizing a feature extraction network on\nthe splat representation. We propose a reconstruction- and optimization-free\napproach to stylizing 3D Gaussian splats. This is done by generating a graph\nstructure across the implicit surface of the splat representation. A\nfeed-forward, surface-based stylization method is then used and interpolated\nback to the individual splats in the scene. This allows for any style image and\n3D Gaussian splat to be used without any additional training or optimization.\nThis also allows for fast stylization of splats, achieving speeds under 2\nminutes even on consumer-grade hardware. We demonstrate the quality results\nthis approach achieves and compare to other 3D Gaussian splat style transfer\nmethods. Code is publicly available at\nhttps://github.com/davidmhart/FastSplatStyler.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u5efa\u6216\u4f18\u5316\u76843D\u9ad8\u65af\u6837\u6761\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u56fe\u7ed3\u6784\u5e76\u63d2\u503c\u5b9e\u73b0\u5feb\u901f\u98ce\u683c\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u91cd\u5efa\u6216\u4f18\u5316\u6837\u6761\u6216\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\uff0c\u9650\u5236\u4e86\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "method": "\u751f\u6210\u6837\u6761\u9690\u5f0f\u8868\u9762\u7684\u56fe\u7ed3\u6784\uff0c\u4f7f\u7528\u524d\u9988\u8868\u9762\u98ce\u683c\u5316\u65b9\u6cd5\u5e76\u63d2\u503c\u56de\u6837\u6761\u3002", "result": "\u5b9e\u73b0\u4e86\u5feb\u901f\u98ce\u683c\u8fc1\u79fb\uff082\u5206\u949f\u5185\uff09\uff0c\u652f\u6301\u4efb\u610f\u98ce\u683c\u56fe\u50cf\u548c\u6837\u6761\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u7075\u6d3b\uff0c\u9002\u7528\u4e8e\u6d88\u8d39\u7ea7\u786c\u4ef6\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.06064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06064", "abs": "https://arxiv.org/abs/2508.06064", "authors": ["Harold Silv\u00e8re Kiossou", "Siegfried Nijssen", "Pierre Schaus"], "title": "A Generic Complete Anytime Beam Search for Optimal Decision Tree", "comment": null, "summary": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees.", "AI": {"tldr": "CA-DL8.5\u662f\u4e00\u79cd\u901a\u7528\u7684\u3001\u5b8c\u6574\u7684\u3001\u968f\u65f6\u53ef\u7528\u7684\u6ce2\u675f\u641c\u7d22\u7b97\u6cd5\uff0c\u6269\u5c55\u4e86DL8.5\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u73b0\u6709\u7684\u968f\u65f6\u7b56\u7565\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u65b9\u6cd5\u5728\u968f\u65f6\u884c\u4e3a\u4e0a\u7684\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6bd4\u8f83\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\u6765\u6539\u8fdb\u51b3\u7b56\u6811\u5b66\u4e60\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "method": "CA-DL8.5\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u6574\u5408\u591a\u79cd\u542f\u53d1\u5f0f\u548c\u677e\u5f1b\u673a\u5236\uff0c\u7ed3\u5408DL8.5\u7684\u9ad8\u6548\u526a\u679d\u548c\u7f13\u5b58\u6280\u672f\uff0c\u91c7\u7528\u91cd\u542f\u6ce2\u675f\u641c\u7d22\u9010\u6b65\u653e\u5bbd\u526a\u679d\u6807\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eLDS\u542f\u53d1\u5f0f\u7684CA-DL8.5\u5728\u968f\u65f6\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u53d8\u4f53\u548cBlossom\u7b97\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u6574\u6027\u548c\u6700\u4f18\u6027\u3002", "conclusion": "CA-DL8.5\u4e3a\u51b3\u7b56\u6811\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.05819", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05819", "abs": "https://arxiv.org/abs/2508.05819", "authors": ["Jong-Ik Park", "Carlee Joe-Wong", "Gary K. Fedder"], "title": "MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses", "comment": null, "summary": "Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from\nmultiple 2D images, even those taken with unknown camera poses. However, they\nstill miss the fine-detailed structures that matter in industrial inspection,\ne.g., detecting sub-micron defects on a production line or analyzing chips with\nScanning Electron Microscopy (SEM). In these scenarios, the sensor resolution\nis fixed and compute budgets are tight, so the only way to expose fine\nstructure is to add zoom-in images; yet, this breaks the multi-view consistency\nthat pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF\n(MZEN), the first NeRF framework that natively handles multi-zoom image sets.\nMZEN (i) augments the pin-hole camera model with an explicit, learnable zoom\nscalar that scales the focal length, and (ii) introduces a novel pose strategy:\nwide-field images are solved first to establish a global metric frame, and\nzoom-in images are then pose-primed to the nearest wide-field counterpart via a\nzoom-consistent crop-and-match procedure before joint refinement. Across eight\nforward-facing scenes$\\unicode{x2013}$synthetic TCAD models, real SEM of\nmicro-structures, and BLEFF objects$\\unicode{x2013}$MZEN consistently\noutperforms pose-free baselines and even high-resolution variants, boosting\nPSNR by up to $28 \\%$, SSIM by $10 \\%$, and reducing LPIPS by up to $222 \\%$.\nMZEN, therefore, extends NeRF to real-world factory settings, preserving global\naccuracy while capturing the micron-level details essential for industrial\ninspection.", "AI": {"tldr": "MZEN\u662f\u4e00\u79cd\u6539\u8fdb\u7684NeRF\u6846\u67b6\uff0c\u901a\u8fc7\u5904\u7406\u591a\u7f29\u653e\u56fe\u50cf\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de5\u4e1a\u68c0\u6d4b\u4e2d\u7684\u7ec6\u8282\u6355\u6349\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfNeRF\u65b9\u6cd5\u5728\u5de5\u4e1a\u68c0\u6d4b\u4e2d\u65e0\u6cd5\u6355\u6349\u5fae\u7c73\u7ea7\u7ec6\u8282\uff0c\u4e14\u591a\u7f29\u653e\u56fe\u50cf\u4f1a\u7834\u574f\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u3002", "method": "MZEN\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u7f29\u653e\u6807\u91cf\uff0c\u5e76\u91c7\u7528\u65b0\u7684\u59ff\u6001\u7b56\u7565\uff0c\u5148\u89e3\u51b3\u5e7f\u89d2\u56fe\u50cf\uff0c\u518d\u901a\u8fc7\u7f29\u653e\u4e00\u81f4\u7684\u88c1\u526a\u5339\u914d\u5904\u7406\u7f29\u653e\u56fe\u50cf\u3002", "result": "\u5728\u591a\u4e2a\u573a\u666f\u4e2d\uff0cMZEN\u663e\u8457\u63d0\u5347\u4e86PSNR\u3001SSIM\uff0c\u5e76\u964d\u4f4e\u4e86LPIPS\u3002", "conclusion": "MZEN\u6269\u5c55\u4e86NeRF\u5728\u5de5\u4e1a\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5168\u5c40\u7cbe\u5ea6\u548c\u5fae\u7c73\u7ea7\u7ec6\u8282\u6355\u6349\u80fd\u529b\u3002"}}
{"id": "2508.06074", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06074", "abs": "https://arxiv.org/abs/2508.06074", "authors": ["Siyi Lu", "Run Liu", "Dongsheng Yang", "Lei He"], "title": "ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception", "comment": null, "summary": "Autonomous driving systems face significant challenges in perceiving complex\nenvironments and making real-time decisions. Traditional modular approaches,\nwhile offering interpretability, suffer from error propagation and coordination\nissues, whereas end-to-end learning systems can simplify the design but face\ncomputational bottlenecks. This paper presents a novel approach to autonomous\ndriving using deep reinforcement learning (DRL) that integrates bird's-eye view\n(BEV) perception for enhanced real-time decision-making. We introduce the\n\\texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction\nnetwork that combines BEV-based perception with the Mamba framework for\ntemporal feature modeling. This integration allows the system to encode vehicle\nsurroundings and road features in a unified coordinate system and accurately\nmodel long-range dependencies. Building on this, we propose the\n\\texttt{ME$^3$-BEV} framework, which utilizes the \\texttt{Mamba-BEV} model as a\nfeature input for end-to-end DRL, achieving superior performance in dynamic\nurban driving scenarios. We further enhance the interpretability of the model\nby visualizing high-dimensional features through semantic segmentation,\nproviding insight into the learned representations. Extensive experiments on\nthe CARLA simulator demonstrate that \\texttt{ME$^3$-BEV} outperforms existing\nmodels across multiple metrics, including collision rate and trajectory\naccuracy, offering a promising solution for real-time autonomous driving.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9e1f\u77b0\u56fe\u611f\u77e5\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u578b\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6ME\u00b3-BEV\uff0c\u901a\u8fc7Mamba-BEV\u6a21\u578b\u9ad8\u6548\u63d0\u53d6\u65f6\u7a7a\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u57ce\u5e02\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6a21\u5757\u5316\u65b9\u6cd5\u5b58\u5728\u8bef\u5dee\u4f20\u64ad\u548c\u534f\u8c03\u95ee\u9898\uff0c\u7aef\u5230\u7aef\u5b66\u4e60\u7cfb\u7edf\u5219\u9762\u4e34\u8ba1\u7b97\u74f6\u9888\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408BEV\u611f\u77e5\u548cDRL\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b9e\u65f6\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u63d0\u51faMamba-BEV\u6a21\u578b\uff0c\u7ed3\u5408BEV\u611f\u77e5\u548cMamba\u6846\u67b6\u8fdb\u884c\u65f6\u7a7a\u7279\u5f81\u63d0\u53d6\uff1b\u8fdb\u4e00\u6b65\u8bbe\u8ba1ME\u00b3-BEV\u6846\u67b6\uff0c\u5c06Mamba-BEV\u4f5c\u4e3a\u7aef\u5230\u7aefDRL\u7684\u8f93\u5165\u3002", "result": "\u5728CARLA\u6a21\u62df\u5668\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cME\u00b3-BEV\u5728\u78b0\u649e\u7387\u548c\u8f68\u8ff9\u7cbe\u5ea6\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "ME\u00b3-BEV\u4e3a\u5b9e\u65f6\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05829", "abs": "https://arxiv.org/abs/2508.05829", "authors": ["Guoping Xu", "Hua-Chieh Shao", "You Zhang"], "title": "TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios", "comment": "23 pages, 5 figures", "summary": "Promptable video object segmentation and tracking (VOST) has seen significant\nadvances with the emergence of foundation models like Segment Anything Model 2\n(SAM2); however, their application in surgical video analysis remains\nchallenging due to complex motion dynamics and the redundancy of memory that\nimpedes effective learning. In this work, we propose TSMS-SAM2, a novel\nframework that enhances promptable VOST in surgical videos by addressing\nchallenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2\nintroduces two key strategies: multi-temporal-scale video sampling augmentation\nto improve robustness against motion variability, and a memory splitting and\npruning mechanism that organizes and filters past frame features for more\nefficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018\ndatasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73,\nrespectively, outperforming prior SAM-based and task-specific methods.\nExtensive ablation studies confirm the effectiveness of multiscale temporal\naugmentation and memory splitting, highlighting the framework's potential for\nrobust, efficient segmentation in complex surgical scenarios. Our source code\nwill be available at https://github.com/apple1986/TSMS-SAM2.", "AI": {"tldr": "TSMS-SAM2\u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u65f6\u95f4\u5c3a\u5ea6\u89c6\u9891\u91c7\u6837\u589e\u5f3a\u548c\u5185\u5b58\u5206\u5272\u4fee\u526a\u673a\u5236\uff0c\u63d0\u5347\u4e86\u624b\u672f\u89c6\u9891\u4e2d\u53ef\u63d0\u793a\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u548c\u8ddf\u8e2a\u7684\u6027\u80fd\u3002", "motivation": "\u624b\u672f\u89c6\u9891\u5206\u6790\u4e2d\uff0c\u590d\u6742\u8fd0\u52a8\u52a8\u6001\u548c\u5185\u5b58\u5197\u4f59\u963b\u788d\u4e86\u57fa\u7840\u6a21\u578b\uff08\u5982SAM2\uff09\u7684\u6709\u6548\u5b66\u4e60\u3002", "method": "\u91c7\u7528\u591a\u65f6\u95f4\u5c3a\u5ea6\u89c6\u9891\u91c7\u6837\u589e\u5f3a\u548c\u5185\u5b58\u5206\u5272\u4fee\u526a\u673a\u5236\uff0c\u4f18\u5316\u5bf9\u8c61\u8fd0\u52a8\u548c\u5185\u5b58\u7ba1\u7406\u3002", "result": "\u5728EndoVis2017\u548cEndoVis2018\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523095.24\u548c86.73\u7684\u5e73\u5747Dice\u5206\u6570\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TSMS-SAM2\u5728\u590d\u6742\u624b\u672f\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u5206\u5272\u6f5c\u529b\u3002"}}
{"id": "2508.06091", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06091", "abs": "https://arxiv.org/abs/2508.06091", "authors": ["Stan P Hauke", "Przemys\u0142aw Andrzej Wa\u0142\u0119ga"], "title": "Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2", "comment": "18 pages", "summary": "In recent years, there has been growing interest in understanding the\nexpressive power of graph neural networks (GNNs) by relating them to logical\nlanguages. This research has been been initialised by an influential result of\nBarcel\\'o et al. (2020), who showed that the graded modal logic (or a guarded\nfragment of the logic C2), characterises the logical expressiveness of\naggregate-combine GNNs. As a ``challenging open problem'' they left the\nquestion whether full C2 characterises the logical expressiveness of\naggregate-combine-readout GNNs. This question has remained unresolved despite\nseveral attempts. In this paper, we solve the above open problem by proving\nthat the logical expressiveness of aggregate-combine-readout GNNs strictly\nexceeds that of C2. This result holds over both undirected and directed graphs.\nBeyond its implications for GNNs, our work also leads to purely logical\ninsights on the expressive power of infinitary logics.", "AI": {"tldr": "\u8bba\u6587\u89e3\u51b3\u4e86GNNs\u903b\u8f91\u8868\u8fbe\u80fd\u529b\u662f\u5426\u8d85\u8fc7C2\u7684\u95ee\u9898\uff0c\u8bc1\u660e\u5176\u4e25\u683c\u8d85\u8fc7C2\u3002", "motivation": "\u7814\u7a76GNNs\u7684\u903b\u8f91\u8868\u8fbe\u80fd\u529b\uff0c\u89e3\u51b3Barcel\u00f3\u7b49\u4eba\u63d0\u51fa\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\uff0c\u6bd4\u8f83GNNs\u4e0eC2\u903b\u8f91\u7684\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u8bc1\u660eGNNs\u7684\u903b\u8f91\u8868\u8fbe\u80fd\u529b\u4e25\u683c\u8d85\u8fc7C2\u3002", "conclusion": "GNNs\u7684\u8868\u8fbe\u80fd\u529b\u8d85\u8d8aC2\uff0c\u540c\u65f6\u5bf9\u65e0\u7a77\u903b\u8f91\u7684\u8868\u8fbe\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2508.05851", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05851", "abs": "https://arxiv.org/abs/2508.05851", "authors": ["Ka-Wai Yung", "Felix J. S. Bragman", "Jialang Xu", "Imanol Luengo", "Danail Stoyanov", "Evangelos B. Mazomenos"], "title": "Temporal Cluster Assignment for Efficient Real-Time Video Segmentation", "comment": null, "summary": "Vision Transformers have substantially advanced the capabilities of\nsegmentation models across both image and video domains. Among them, the Swin\nTransformer stands out for its ability to capture hierarchical, multi-scale\nrepresentations, making it a popular backbone for segmentation in videos.\nHowever, despite its window-attention scheme, it still incurs a high\ncomputational cost, especially in larger variants commonly used for dense\nprediction in videos. This remains a major bottleneck for real-time,\nresource-constrained applications. Whilst token reduction methods have been\nproposed to alleviate this, the window-based attention mechanism of Swin\nrequires a fixed number of tokens per window, limiting the applicability of\nconventional pruning techniques. Meanwhile, training-free token clustering\napproaches have shown promise in image segmentation while maintaining window\nconsistency. Nevertheless, they fail to exploit temporal redundancy, missing a\nkey opportunity to further optimize video segmentation performance. We\nintroduce Temporal Cluster Assignment (TCA), a lightweight and effective,\nfine-tuning-free strategy that enhances token clustering by leveraging temporal\ncoherence across frames. Instead of indiscriminately dropping redundant tokens,\nTCA refines token clusters using temporal correlations, thereby retaining\nfine-grained details while significantly reducing computation. Extensive\nevaluations on YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and a private surgical\nvideo dataset show that TCA consistently boosts the accuracy-speed trade-off of\nexisting clustering-based methods. Our results demonstrate that TCA generalizes\ncompetently across both natural and domain-specific videos.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTCA\u7684\u8f7b\u91cf\u7ea7\u7b56\u7565\uff0c\u901a\u8fc7\u5229\u7528\u65f6\u95f4\u4e00\u81f4\u6027\u4f18\u5316\u89c6\u9891\u5206\u5272\u4e2d\u7684\u4ee4\u724c\u805a\u7c7b\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "Swin Transformer\u5728\u89c6\u9891\u5206\u5272\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u4ee4\u724c\u51cf\u5c11\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5229\u7528\u65f6\u95f4\u5197\u4f59\u3002", "method": "TCA\u901a\u8fc7\u65f6\u95f4\u76f8\u5173\u6027\u4f18\u5316\u4ee4\u724c\u805a\u7c7b\uff0c\u4fdd\u7559\u7ec6\u8282\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cTCA\u63d0\u5347\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7cbe\u5ea6\u4e0e\u901f\u5ea6\u5e73\u8861\u3002", "conclusion": "TCA\u5728\u81ea\u7136\u548c\u7279\u5b9a\u9886\u57df\u89c6\u9891\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2508.06110", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06110", "abs": "https://arxiv.org/abs/2508.06110", "authors": ["Yiran Rex Ma"], "title": "PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion", "comment": "Accepted at IJCNN 2025", "summary": "Table reasoning, including tabular QA and fact verification, often depends on\nannotated data or complex data augmentation, limiting flexibility and\ngeneralization. LLMs, despite their versatility, often underperform compared to\nsimple supervised models. To approach these issues, we introduce PanelTR, a\nframework utilizing LLM agent scientists for robust table reasoning through a\nstructured scientific approach. PanelTR's workflow involves agent scientists\nconducting individual investigations, engaging in self-review, and\nparticipating in collaborative peer-review discussions. This process, driven by\nfive scientist personas, enables semantic-level transfer without relying on\ndata augmentation or parametric optimization. Experiments across four\nbenchmarks show that PanelTR outperforms vanilla LLMs and rivals fully\nsupervised models, all while remaining independent of training data. Our\nfindings indicate that structured scientific methodology can effectively handle\ncomplex tasks beyond table reasoning with flexible semantic understanding in a\nzero-shot context.", "AI": {"tldr": "PanelTR\u6846\u67b6\u901a\u8fc7LLM\u4ee3\u7406\u79d1\u5b66\u5bb6\u8fdb\u884c\u8868\u683c\u63a8\u7406\uff0c\u65e0\u9700\u6570\u636e\u589e\u5f3a\u6216\u53c2\u6570\u4f18\u5316\uff0c\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e2d\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u6216\u590d\u6742\u6570\u636e\u589e\u5f3a\u7684\u95ee\u9898\uff0c\u63d0\u5347LLM\u5728\u8868\u683c\u63a8\u7406\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u4e94\u4e2a\u79d1\u5b66\u5bb6\u89d2\u8272\u7684\u4ee3\u7406\u8fdb\u884c\u72ec\u7acb\u8c03\u67e5\u3001\u81ea\u6211\u5ba1\u67e5\u548c\u534f\u4f5c\u540c\u884c\u8bc4\u5ba1\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u666e\u901aLLM\uff0c\u5ab2\u7f8e\u5168\u76d1\u7763\u6a21\u578b\u3002", "conclusion": "\u7ed3\u6784\u5316\u79d1\u5b66\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u590d\u6742\u4efb\u52a1\uff0c\u5177\u6709\u7075\u6d3b\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2508.05852", "categories": ["cs.CV", "I.5.4"], "pdf": "https://arxiv.org/pdf/2508.05852", "abs": "https://arxiv.org/abs/2508.05852", "authors": ["Kaiser Hamid", "Khandakar Ashrafi Akbar", "Nade Liang"], "title": "VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments", "comment": null, "summary": "Driver visual attention prediction is a critical task in autonomous driving\nand human-computer interaction (HCI) research. Most prior studies focus on\nestimating attention allocation at a single moment in time, typically using\nstatic RGB images such as driving scene pictures. In this work, we propose a\nvision-language framework that models the changing landscape of drivers' gaze\nthrough natural language, using few-shot and zero-shot learning on single RGB\nimages. We curate and refine high-quality captions from the BDD-A dataset using\nhuman-in-the-loop feedback, then fine-tune LLaVA to align visual perception\nwith attention-centric scene understanding. Our approach integrates both\nlow-level cues and top-down context (e.g., route semantics, risk anticipation),\nenabling language-based descriptions of gaze behavior. We evaluate performance\nacross training regimes (few shot, and one-shot) and introduce domain-specific\nmetrics for semantic alignment and response diversity. Results show that our\nfine-tuned model outperforms general-purpose VLMs in attention shift detection\nand interpretability. To our knowledge, this is among the first attempts to\ngenerate driver visual attention allocation and shifting predictions in natural\nlanguage, offering a new direction for explainable AI in autonomous driving.\nOur approach provides a foundation for downstream tasks such as behavior\nforecasting, human-AI teaming, and multi-agent coordination.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u9884\u6d4b\u9a7e\u9a76\u5458\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u53d8\u5316\uff0c\u5e76\u5728\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u5b66\u4e60\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u9759\u6001\u56fe\u50cf\u4e2d\u7684\u6ce8\u610f\u529b\u5206\u914d\uff0c\u800c\u5ffd\u7565\u4e86\u52a8\u6001\u53d8\u5316\u7684\u6ce8\u610f\u529b\u666f\u89c2\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5efa\u6a21\u9a7e\u9a76\u5458\u7684\u89c6\u7ebf\u53d8\u5316\u3002", "method": "\u5229\u7528BDD-A\u6570\u636e\u96c6\u7684\u9ad8\u8d28\u91cf\u6807\u6ce8\uff0c\u901a\u8fc7\u4eba\u7c7b\u53cd\u9988\u4f18\u5316\uff0c\u5e76\u5fae\u8c03LLaVA\u6a21\u578b\uff0c\u7ed3\u5408\u4f4e\u5c42\u7ebf\u7d22\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff08\u5982\u8def\u7ebf\u8bed\u4e49\u3001\u98ce\u9669\u9884\u6d4b\uff09\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u6ce8\u610f\u529b\u8f6c\u79fb\u68c0\u6d4b\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u4f18\u4e8e\u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e86\u9886\u57df\u7279\u5b9a\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u54cd\u5e94\u591a\u6837\u6027\u6307\u6807\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5c1d\u8bd5\u7528\u81ea\u7136\u8bed\u8a00\u751f\u6210\u9a7e\u9a76\u5458\u89c6\u89c9\u6ce8\u610f\u529b\u5206\u914d\u548c\u8f6c\u79fb\u9884\u6d4b\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u53ef\u89e3\u91caAI\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.06111", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06111", "abs": "https://arxiv.org/abs/2508.06111", "authors": ["Dewi S. W. Gould", "Bruno Mlodozeniec", "Samuel F. Brown"], "title": "SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges", "comment": "7 pages and appendices", "summary": "Evaluating the capabilities and risks of foundation models is paramount, yet\ncurrent methods demand extensive domain expertise, hindering their scalability\nas these models rapidly evolve. We introduce SKATE: a novel evaluation\nframework in which large language models (LLMs) compete by generating and\nsolving verifiable tasks for one another. Our core insight is to treat\nevaluation as a game: models act as both task-setters and solvers, incentivized\nto create questions which highlight their own strengths while exposing others'\nweaknesses. SKATE offers several key advantages, balancing scalability,\nopen-endedness, and objectivity. It is fully automated, data-free, and\nscalable, requiring no human input or domain expertise. By using verifiable\ntasks rather than LLM judges, scoring is objective. Unlike domain-limited\nprogrammatically-generated benchmarks (e.g. chess-playing or spatial\nreasoning), having LLMs creatively pose challenges enables open-ended and\nscalable evaluation. As a proof of concept, we introduce LLM-set\ncode-output-prediction (COP) challenges as a verifiable and extensible\nframework in which to test our approach. Using a TrueSkill-based ranking\nsystem, we evaluate six frontier LLMs and find that: (1) weaker models can\nreliably differentiate and score stronger ones, (2) LLM-based systems are\ncapable of self-preferencing behavior, generating questions that align with\ntheir own capabilities, and (3) SKATE automatically surfaces fine-grained\ncapability differences between models. Our findings are an important step\ntowards general, scalable evaluation frameworks which can keep pace with LLM\nprogress.", "AI": {"tldr": "SKATE\u662f\u4e00\u79cd\u65b0\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u76f8\u4e92\u751f\u6210\u548c\u89e3\u51b3\u53ef\u9a8c\u8bc1\u4efb\u52a1\u6765\u8bc4\u4f30\u5176\u80fd\u529b\uff0c\u5177\u6709\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u548c\u5ba2\u89c2\u6027\u7b49\u4f18\u52bf\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u96be\u4ee5\u8ddf\u4e0a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "SKATE\u6846\u67b6\u5c06\u8bc4\u4f30\u89c6\u4e3a\u6e38\u620f\uff0c\u6a21\u578b\u65e2\u4f5c\u4e3a\u4efb\u52a1\u751f\u6210\u8005\u53c8\u4f5c\u4e3a\u89e3\u51b3\u8005\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u9a8c\u8bc1\u4efb\u52a1\uff08\u5982\u4ee3\u7801\u8f93\u51fa\u9884\u6d4b\u6311\u6218\uff09\u6765\u76f8\u4e92\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSKATE\u80fd\u6709\u6548\u533a\u5206\u6a21\u578b\u80fd\u529b\uff0c\u63ed\u793a\u6a21\u578b\u95f4\u7684\u7ec6\u5fae\u5dee\u5f02\uff0c\u5e76\u89c2\u5bdf\u5230\u6a21\u578b\u7684\u81ea\u504f\u597d\u884c\u4e3a\u3002", "conclusion": "SKATE\u4e3a\u901a\u7528\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u6846\u67b6\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u80fd\u591f\u8ddf\u4e0a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u6b65\u4f10\u3002"}}
{"id": "2508.05857", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05857", "abs": "https://arxiv.org/abs/2508.05857", "authors": ["Qiaomu Miao", "Vivek Raju Golani", "Jingyi Xu", "Progga Paromita Dutta", "Minh Hoai", "Dimitris Samaras"], "title": "Multi-view Gaze Target Estimation", "comment": "Accepted to ICCV 2025", "summary": "This paper presents a method that utilizes multiple camera views for the gaze\ntarget estimation (GTE) task. The approach integrates information from\ndifferent camera views to improve accuracy and expand applicability, addressing\nlimitations in existing single-view methods that face challenges such as face\nocclusion, target ambiguity, and out-of-view targets. Our method processes a\npair of camera views as input, incorporating a Head Information Aggregation\n(HIA) module for leveraging head information from both views for more accurate\ngaze estimation, an Uncertainty-based Gaze Selection (UGS) for identifying the\nmost reliable gaze output, and an Epipolar-based Scene Attention (ESA) module\nfor cross-view background information sharing. This approach significantly\noutperforms single-view baselines, especially when the second camera provides a\nclear view of the person's face. Additionally, our method can estimate the gaze\ntarget in the first view using the image of the person in the second view only,\na capability not possessed by single-view GTE methods. Furthermore, the paper\nintroduces a multi-view dataset for developing and evaluating multi-view GTE\nmethods. Data and code are available at\nhttps://www3.cs.stonybrook.edu/~cvl/multiview_gte.html", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u89c6\u89d2\u76f8\u673a\u8fdb\u884c\u89c6\u7ebf\u76ee\u6807\u4f30\u8ba1\uff08GTE\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u4e0d\u540c\u89c6\u89d2\u7684\u4fe1\u606f\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u9002\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u5355\u89c6\u89d2\u65b9\u6cd5\u5b58\u5728\u9762\u90e8\u906e\u6321\u3001\u76ee\u6807\u6a21\u7cca\u548c\u89c6\u7ebf\u5916\u76ee\u6807\u7b49\u5c40\u9650\u6027\uff0c\u591a\u89c6\u89d2\u65b9\u6cd5\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5934\u90e8\u4fe1\u606f\u805a\u5408\uff08HIA\uff09\u6a21\u5757\u3001\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u89c6\u7ebf\u9009\u62e9\uff08UGS\uff09\u6a21\u5757\u548c\u57fa\u4e8e\u6781\u7ebf\u7684\u573a\u666f\u6ce8\u610f\u529b\uff08ESA\uff09\u6a21\u5757\u3002", "result": "\u5728\u591a\u89c6\u89d2\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u5355\u89c6\u89d2\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u7b2c\u4e8c\u89c6\u89d2\u63d0\u4f9b\u6e05\u6670\u9762\u90e8\u89c6\u56fe\u65f6\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86GTE\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u8fd8\u6269\u5c55\u4e86\u5355\u89c6\u89d2\u65e0\u6cd5\u5b9e\u73b0\u7684\u529f\u80fd\uff0c\u5982\u4ec5\u901a\u8fc7\u7b2c\u4e8c\u89c6\u89d2\u56fe\u50cf\u4f30\u8ba1\u7b2c\u4e00\u89c6\u89d2\u7684\u89c6\u7ebf\u76ee\u6807\u3002"}}
{"id": "2508.06129", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06129", "abs": "https://arxiv.org/abs/2508.06129", "authors": ["Bachtiar Herdianto", "Romain Billot", "Flavien Lucas", "Marc Sevaux"], "title": "Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem", "comment": "22 pages, 14 figures", "summary": "The Vehicle Routing Problem (VRP) is a complex optimization problem with\nnumerous real-world applications, mostly solved using metaheuristic algorithms\ndue to its $\\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely\non human-crafted designs developed through empirical studies. However, recent\nresearch shows that machine learning methods can be used the structural\ncharacteristics of solutions in combinatorial optimization, thereby aiding in\ndesigning more efficient algorithms, particularly for solving VRP. Building on\nthis advancement, this study extends the previous research by conducting a\nsensitivity analysis using multiple classifier models that are capable of\npredicting the quality of VRP solutions. Hence, by leveraging explainable AI,\nthis research is able to extend the understanding of how these models make\ndecisions. Finally, our findings indicate that while feature importance varies,\ncertain features consistently emerge as strong predictors. Furthermore, we\npropose a unified framework able of ranking feature impact across different\nscenarios to illustrate this finding. These insights highlight the potential of\nfeature importance analysis as a foundation for developing a guidance mechanism\nof metaheuristic algorithms for solving the VRP.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u53ef\u89e3\u91caAI\u5206\u6790VRP\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u9884\u6d4b\u6a21\u578b\uff0c\u53d1\u73b0\u67d0\u4e9b\u7279\u5f81\u5bf9\u9884\u6d4b\u5177\u6709\u4e00\u81f4\u6027\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u7edf\u4e00\u6846\u67b6\u6765\u8bc4\u4f30\u7279\u5f81\u91cd\u8981\u6027\u3002", "motivation": "\u4f20\u7edf\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u80fd\u5229\u7528\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u7ed3\u6784\u7279\u5f81\uff0c\u63d0\u9ad8\u7b97\u6cd5\u6548\u7387\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u5206\u7c7b\u5668\u6a21\u578b\u8fdb\u884c\u654f\u611f\u6027\u5206\u6790\uff0c\u7ed3\u5408\u53ef\u89e3\u91caAI\u6280\u672f\u7406\u89e3\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "\u7279\u5f81\u91cd\u8981\u6027\u56e0\u6a21\u578b\u800c\u5f02\uff0c\u4f46\u67d0\u4e9b\u7279\u5f81\u59cb\u7ec8\u662f\u5f3a\u9884\u6d4b\u56e0\u5b50\u3002", "conclusion": "\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u53ef\u4e3a\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u63d0\u4f9b\u6307\u5bfc\u673a\u5236\u7684\u57fa\u7840\u3002"}}
{"id": "2508.05898", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05898", "abs": "https://arxiv.org/abs/2508.05898", "authors": ["Hamidreza Dastmalchi", "Aijun An", "Ali cheraghian"], "title": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates", "comment": "BMVC2025", "summary": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA.", "AI": {"tldr": "ETTA\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u9012\u5f52\u66f4\u65b0\u548c\u81ea\u9002\u5e94\u96c6\u6210\u6a21\u5757\u63d0\u5347CLIP\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u73b0\u6709\u7f13\u5b58\u65b9\u6cd5\u4ec5\u5b58\u50a8\u9ad8\u7f6e\u4fe1\u5ea6\u6837\u672c\uff0c\u9650\u5236\u4e86\u51b3\u7b56\u8fb9\u754c\u3002", "method": "ETTA\u5f15\u5165\u9012\u5f52\u66f4\u65b0\u6a21\u5757\u52a8\u6001\u6574\u5408\u6240\u6709\u6d4b\u8bd5\u6837\u672c\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u96c6\u6210\u6a21\u5757\u51cf\u5c11\u5bf9\u63d0\u793a\u7684\u4f9d\u8d56\uff0c\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cETTA\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u51c6\u786e\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709TTA\u6a21\u578b\u3002", "conclusion": "ETTA\u4e3a\u6d4b\u8bd5\u65f6\u9002\u5e94\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.06145", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06145", "abs": "https://arxiv.org/abs/2508.06145", "authors": ["Byeonghun Bang", "Jongsuk Yoon", "Dong-Jin Chang", "Seho Park", "Yong Oh Lee"], "title": "Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications", "comment": null, "summary": "The versatility of large language models (LLMs) has been explored across\nvarious sectors, but their application in healthcare poses challenges,\nparticularly in the domain of pharmaceutical contraindications where accurate\nand reliable information is required. This study enhances the capability of\nLLMs to address contraindications effectively by implementing a Retrieval\nAugmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base\nmodel, and the text-embedding-3-small model for embeddings, our approach\nintegrates Langchain to orchestrate a hybrid retrieval system with re-ranking.\nThis system leverages Drug Utilization Review (DUR) data from public databases,\nfocusing on contraindications for specific age groups, pregnancy, and\nconcomitant drug use. The dataset includes 300 question-answer pairs across\nthree categories, with baseline model accuracy ranging from 0.49 to 0.57.\nPost-integration of the RAG pipeline, we observed a significant improvement in\nmodel accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications\nrelated to age groups, pregnancy, and concomitant drug use, respectively. The\nresults indicate that augmenting LLMs with a RAG framework can substantially\nreduce uncertainty in prescription and drug intake decisions by providing more\nprecise and reliable drug contraindication information.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u836f\u7269\u7981\u5fcc\u9886\u57df\u7684\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u533b\u7597\u5065\u5eb7\u9886\u57df\u4e2d\u7684\u4fe1\u606f\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "\u836f\u7269\u7981\u5fcc\u4fe1\u606f\u7684\u51c6\u786e\u6027\u5bf9\u533b\u7597\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46LLMs\u5728\u6b64\u9886\u57df\u7684\u5e94\u7528\u5b58\u5728\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7RAG\u6846\u67b6\u63d0\u5347LLMs\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528OpenAI\u7684GPT-4o-mini\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\uff0c\u7ed3\u5408text-embedding-3-small\u6a21\u578b\u548cLangchain\u5de5\u5177\uff0c\u6784\u5efa\u6df7\u5408\u68c0\u7d22\u7cfb\u7edf\uff0c\u5e76\u5229\u7528\u516c\u5171\u6570\u636e\u5e93\u4e2d\u7684\u836f\u7269\u4f7f\u7528\u5ba1\u67e5\uff08DUR\uff09\u6570\u636e\u3002", "result": "RAG\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u51c6\u786e\u6027\uff0c\u5728\u5e74\u9f84\u7ec4\u3001\u598a\u5a20\u548c\u8054\u5408\u7528\u836f\u4e09\u4e2a\u7c7b\u522b\u7684\u7981\u5fcc\u4fe1\u606f\u4e2d\uff0c\u51c6\u786e\u7387\u5206\u522b\u8fbe\u52300.94\u30010.87\u548c0.89\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cRAG\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u836f\u7269\u7981\u5fcc\u4fe1\u606f\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u5904\u65b9\u548c\u7528\u836f\u51b3\u7b56\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u4f9d\u636e\u3002"}}
{"id": "2508.05899", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.05899", "abs": "https://arxiv.org/abs/2508.05899", "authors": ["Zixuan Bian", "Ruohan Ren", "Yue Yang", "Chris Callison-Burch"], "title": "HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing", "comment": null, "summary": "3D scene generation plays a crucial role in gaming, artistic creation,\nvirtual reality and many other domains. However, current 3D scene design still\nrelies heavily on extensive manual effort from creators, and existing automated\nmethods struggle to generate open-domain scenes or support flexible editing. As\na result, generating 3D worlds directly from text has garnered increasing\nattention. In this paper, we introduce HOLODECK 2.0, an advanced\nvision-language-guided framework for 3D world generation with support for\ninteractive scene editing based on human feedback. HOLODECK 2.0 can generate\ndiverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and\ncyberpunk styles) that exhibit high semantic fidelity to fine-grained input\ndescriptions, suitable for both indoor and open-domain environments. HOLODECK\n2.0 leverages vision-language models (VLMs) to identify and parse the objects\nrequired in a scene and generates corresponding high-quality assets via\nstate-of-the-art 3D generative models. It then iteratively applies spatial\nconstraints derived from the VLMs to achieve semantically coherent and\nphysically plausible layouts. Human evaluations and CLIP-based assessments\ndemonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely\naligned with detailed textual descriptions, consistently outperforming\nbaselines across indoor and open-domain scenarios. Additionally, we provide\nediting capabilities that flexibly adapt to human feedback, supporting layout\nrefinement and style-consistent object edits. Finally, we present a practical\napplication of HOLODECK 2.0 in procedural game modeling, generating visually\nrich and immersive environments, potentially boosting efficiency.", "AI": {"tldr": "HOLODECK 2.0\u662f\u4e00\u4e2a\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u5f15\u5bfc\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u62103D\u573a\u666f\u5e76\u652f\u6301\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u4ea4\u4e92\u5f0f\u7f16\u8f91\u3002", "motivation": "\u5f53\u524d3D\u573a\u666f\u8bbe\u8ba1\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u5f00\u653e\u57df\u573a\u666f\u6216\u652f\u6301\u7075\u6d3b\u7f16\u8f91\uff0c\u56e0\u6b64\u9700\u8981\u76f4\u63a5\u4ece\u6587\u672c\u751f\u62103D\u4e16\u754c\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u8bc6\u522b\u548c\u89e3\u6790\u573a\u666f\u4e2d\u7684\u5bf9\u8c61\uff0c\u901a\u8fc7\u5148\u8fdb\u76843D\u751f\u6210\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u8d44\u4ea7\uff0c\u5e76\u5e94\u7528\u7a7a\u95f4\u7ea6\u675f\u5b9e\u73b0\u8bed\u4e49\u4e00\u81f4\u548c\u7269\u7406\u5408\u7406\u7684\u5e03\u5c40\u3002", "result": "HOLODECK 2.0\u80fd\u751f\u6210\u591a\u6837\u4e14\u98ce\u683c\u4e30\u5bcc\u76843D\u573a\u666f\uff0c\u5728\u4eba\u7c7b\u8bc4\u4f30\u548cCLIP\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HOLODECK 2.0\u57283D\u573a\u666f\u751f\u6210\u548c\u7f16\u8f91\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u6e38\u620f\u5efa\u6a21\u7b49\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.06225", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06225", "abs": "https://arxiv.org/abs/2508.06225", "authors": ["Zailong Tian", "Zhuoheng Han", "Yanzhe Chen", "Haozhe Xu", "Xi Yang", "richeng xuan", "Hongfeng Wang", "Lizi Liao"], "title": "Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution", "comment": null, "summary": "Large Language Models (LLMs) are widely used as automated judges, where\npractical value depends on both accuracy and trustworthy, risk-aware judgments.\nExisting approaches predominantly focus on accuracy, overlooking the necessity\nof well-calibrated confidence, which is vital for adaptive and reliable\nevaluation pipelines. In this work, we advocate a shift from accuracy-centric\nevaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing\nthe necessity of well-calibrated confidence for trustworthy and adaptive\nevaluation. We systematically identify the **Overconfidence Phenomenon** in\ncurrent LLM-as-a-Judges, where predicted confidence significantly overstates\nactual correctness, undermining reliability in practical deployment. To\nquantify this phenomenon, we introduce **TH-Score**, a novel metric measuring\nconfidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an\nensemble framework that transforms LLMs into reliable, risk-aware evaluators.\nExtensive experiments demonstrate that our approach substantially improves\ncalibration and enables adaptive, confidence-driven evaluation pipelines,\nachieving superior reliability and accuracy compared to existing baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4ece\u4ee5\u51c6\u786e\u6027\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u8f6c\u5411\u4ee5\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u98ce\u9669\u611f\u77e5LLM-as-a-Judge\u7cfb\u7edf\uff0c\u5f3a\u8c03\u6821\u51c6\u7f6e\u4fe1\u5ea6\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86TH-Score\u548c\u65b0\u6846\u67b6LLM-as-a-Fuser\u3002", "motivation": "\u73b0\u6709LLM-as-a-Judge\u7cfb\u7edf\u8fc7\u4e8e\u5173\u6ce8\u51c6\u786e\u6027\uff0c\u5ffd\u89c6\u4e86\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u5b9e\u9645\u90e8\u7f72\u4e2d\u53ef\u9760\u6027\u4e0d\u8db3\u3002", "method": "\u5f15\u5165TH-Score\u91cf\u5316\u7f6e\u4fe1\u5ea6-\u51c6\u786e\u6027\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u51faLLM-as-a-Fuser\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u65b9\u6cd5\u63d0\u5347\u6821\u51c6\u6027\u548c\u98ce\u9669\u611f\u77e5\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u6821\u51c6\u6027\uff0c\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u3001\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u8bc4\u4f30\u6d41\u7a0b\uff0c\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u98ce\u9669\u611f\u77e5\u8bc4\u4f30\u662f\u63d0\u5347LLM-as-a-Judge\u7cfb\u7edf\u53ef\u9760\u6027\u7684\u5173\u952e\uff0cTH-Score\u548cLLM-as-a-Fuser\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05903", "abs": "https://arxiv.org/abs/2508.05903", "authors": ["Lang Nie", "Yuan Mei", "Kang Liao", "Yunqiu Xu", "Chunyu Lin", "Bin Xiao"], "title": "Robust Image Stitching with Optimal Plane", "comment": "* Equal contribution", "summary": "We present \\textit{RopStitch}, an unsupervised deep image stitching framework\nwith both robustness and naturalness. To ensure the robustness of\n\\textit{RopStitch}, we propose to incorporate the universal prior of content\nperception into the image stitching model by a dual-branch architecture. It\nseparately captures coarse and fine features and integrates them to achieve\nhighly generalizable performance across diverse unseen real-world scenes.\nConcretely, the dual-branch model consists of a pretrained branch to capture\nsemantically invariant representations and a learnable branch to extract\nfine-grained discriminative features, which are then merged into a whole by a\ncontrollable factor at the correlation level. Besides, considering that content\nalignment and structural preservation are often contradictory to each other, we\npropose a concept of virtual optimal planes to relieve this conflict. To this\nend, we model this problem as a process of estimating homography decomposition\ncoefficients, and design an iterative coefficient predictor and minimal\nsemantic distortion constraint to identify the optimal plane. This scheme is\nfinally incorporated into \\textit{RopStitch} by warping both views onto the\noptimal plane bidirectionally. Extensive experiments across various datasets\ndemonstrate that \\textit{RopStitch} significantly outperforms existing methods,\nparticularly in scene robustness and content naturalness. The code is available\nat {\\color{red}https://github.com/MmelodYy/RopStitch}.", "AI": {"tldr": "RopStitch\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u6df1\u5ea6\u56fe\u50cf\u62fc\u63a5\u6846\u67b6\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u81ea\u7136\u6027\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u548c\u865a\u62df\u6700\u4f18\u5e73\u9762\u6982\u5ff5\u5b9e\u73b0\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf\u62fc\u63a5\u4e2d\u5185\u5bb9\u5bf9\u9f50\u4e0e\u7ed3\u6784\u4fdd\u7559\u7684\u77db\u76fe\uff0c\u5e76\u63d0\u9ad8\u6a21\u578b\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\u5206\u522b\u6355\u83b7\u7c97\u7c92\u5ea6\u4e0e\u7ec6\u7c92\u5ea6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u865a\u62df\u6700\u4f18\u5e73\u9762\u6982\u5ff5\u4f18\u5316\u62fc\u63a5\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u573a\u666f\u9c81\u68d2\u6027\u548c\u5185\u5bb9\u81ea\u7136\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "RopStitch\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u548c\u4f18\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u81ea\u7136\u7684\u56fe\u50cf\u62fc\u63a5\u3002"}}
{"id": "2508.06226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06226", "abs": "https://arxiv.org/abs/2508.06226", "authors": ["Yumeng Fu", "Jiayin Zhu", "Lingling Zhang", "Bo Zhao", "Shaoxuan Ma", "Yushun Zhang", "Yanrui Wu", "Wenjun Wu"], "title": "GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines", "comment": null, "summary": "Geometry problem solving (GPS) requires models to master diagram\ncomprehension, logical reasoning, knowledge application, numerical computation,\nand auxiliary line construction. This presents a significant challenge for\nMultimodal Large Language Models (MLLMs). However, existing benchmarks for\nevaluating MLLM geometry skills overlook auxiliary line construction and lack\nfine-grained process evaluation, making them insufficient for assessing MLLMs'\nlong-step reasoning abilities. To bridge these gaps, we present the GeoLaux\nbenchmark, comprising 2,186 geometry problems, incorporating both calculation\nand proving questions. Notably, the problems require an average of 6.51\nreasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary\nline construction. Building on the dataset, we design a novel five-dimensional\nevaluation strategy assessing answer correctness, process correctness, process\nquality, auxiliary line impact, and error causes. Extensive experiments on 13\nleading MLLMs (including thinking models and non-thinking models) yield three\npivotal findings: First, models exhibit substantial performance degradation in\nextended reasoning steps (nine models demonstrate over 50% performance drop).\nSecond, compared to calculation problems, MLLMs tend to take shortcuts when\nsolving proving problems. Third, models lack auxiliary line awareness, and\nenhancing this capability proves particularly beneficial for overall geometry\nreasoning improvement. These findings establish GeoLaux as both a benchmark for\nevaluating MLLMs' long-step geometric reasoning with auxiliary lines and a\nguide for capability advancement. Our dataset and code are included in\nsupplementary materials and will be released.", "AI": {"tldr": "GeoLaux\u57fa\u51c6\u6d4b\u8bd5\u586b\u8865\u4e86\u73b0\u6709MLLM\u51e0\u4f55\u80fd\u529b\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u91cd\u70b9\u5173\u6ce8\u8f85\u52a9\u7ebf\u6784\u5efa\u548c\u957f\u6b65\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5ffd\u89c6\u8f85\u52a9\u7ebf\u6784\u5efa\u548c\u7ec6\u7c92\u5ea6\u8fc7\u7a0b\u8bc4\u4f30\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30MLLM\u7684\u51e0\u4f55\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faGeoLaux\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2,186\u9053\u51e0\u4f55\u9898\uff0c\u8bbe\u8ba1\u4e94\u7ef4\u8bc4\u4f30\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0MLLM\u5728\u957f\u6b65\u63a8\u7406\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u7f3a\u4e4f\u8f85\u52a9\u7ebf\u610f\u8bc6\u3002", "conclusion": "GeoLaux\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdbMLLM\u51e0\u4f55\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u548c\u6307\u5357\u3002"}}
{"id": "2508.05907", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05907", "abs": "https://arxiv.org/abs/2508.05907", "authors": ["Ilya Chugunov"], "title": "Neural Field Representations of Mobile Computational Photography", "comment": "PhD thesis", "summary": "Over the past two decades, mobile imaging has experienced a profound\ntransformation, with cell phones rapidly eclipsing all other forms of digital\nphotography in popularity. Today's cell phones are equipped with a diverse\nrange of imaging technologies - laser depth ranging, multi-focal camera arrays,\nand split-pixel sensors - alongside non-visual sensors such as gyroscopes,\naccelerometers, and magnetometers. This, combined with on-board integrated\nchips for image and signal processing, makes the cell phone a versatile\npocket-sized computational imaging platform. Parallel to this, we have seen in\nrecent years how neural fields - small neural networks trained to map\ncontinuous spatial input coordinates to output signals - enable the\nreconstruction of complex scenes without explicit data representations such as\npixel arrays or point clouds. In this thesis, I demonstrate how carefully\ndesigned neural field models can compactly represent complex geometry and\nlighting effects. Enabling applications such as depth estimation, layer\nseparation, and image stitching directly from collected in-the-wild mobile\nphotography data. These methods outperform state-of-the-art approaches without\nrelying on complex pre-processing steps, labeled ground truth data, or machine\nlearning priors. Instead, they leverage well-constructed, self-regularized\nmodels that tackle challenging inverse problems through stochastic gradient\ndescent, fitting directly to raw measurements from a smartphone.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u795e\u7ecf\u573a\u6a21\u578b\uff0c\u4ece\u79fb\u52a8\u6444\u5f71\u6570\u636e\u4e2d\u9ad8\u6548\u91cd\u5efa\u590d\u6742\u573a\u666f\uff0c\u65e0\u9700\u590d\u6742\u9884\u5904\u7406\u6216\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u5df2\u6210\u4e3a\u5f3a\u5927\u7684\u8ba1\u7b97\u6210\u50cf\u5e73\u53f0\uff0c\u7ed3\u5408\u795e\u7ecf\u573a\u6280\u672f\uff0c\u53ef\u4ee5\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u9884\u5904\u7406\u548c\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u81ea\u6b63\u5219\u5316\u7684\u795e\u7ecf\u573a\u6a21\u578b\uff0c\u901a\u8fc7\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u76f4\u63a5\u62df\u5408\u667a\u80fd\u624b\u673a\u7684\u539f\u59cb\u6d4b\u91cf\u6570\u636e\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6df1\u5ea6\u4f30\u8ba1\u3001\u56fe\u5c42\u5206\u79bb\u548c\u56fe\u50cf\u62fc\u63a5\u7b49\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u795e\u7ecf\u573a\u6a21\u578b\u4e3a\u79fb\u52a8\u8ba1\u7b97\u6210\u50cf\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u5148\u9a8c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06230", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06230", "abs": "https://arxiv.org/abs/2508.06230", "authors": ["Ruben Sharma", "Sebastijan Duman\u010di\u0107", "Ross D. King", "Andrew Cropper"], "title": "Learning Logical Rules using Minimum Message Length", "comment": null, "summary": "Unifying probabilistic and logical learning is a key challenge in AI. We\nintroduce a Bayesian inductive logic programming approach that learns minimum\nmessage length programs from noisy data. Our approach balances hypothesis\ncomplexity and data fit through priors, which explicitly favour more general\nprograms, and a likelihood that favours accurate programs. Our experiments on\nseveral domains, including game playing and drug design, show that our method\nsignificantly outperforms previous methods, notably those that learn minimum\ndescription length programs. Our results also show that our approach is\ndata-efficient and insensitive to example balance, including the ability to\nlearn from exclusively positive examples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8d1d\u53f6\u65af\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u5047\u8bbe\u590d\u6742\u6027\u548c\u6570\u636e\u62df\u5408\uff0c\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u5b66\u4e60\u6700\u5c0f\u6d88\u606f\u957f\u5ea6\u7a0b\u5e8f\u3002", "motivation": "\u7edf\u4e00\u6982\u7387\u548c\u903b\u8f91\u5b66\u4e60\u662fAI\u9886\u57df\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5148\u9a8c\u548c\u4f3c\u7136\u51fd\u6570\uff0c\u5e73\u8861\u5047\u8bbe\u590d\u6742\u6027\u548c\u6570\u636e\u62df\u5408\uff0c\u5b66\u4e60\u6700\u5c0f\u6d88\u606f\u957f\u5ea6\u7a0b\u5e8f\u3002", "result": "\u5728\u6e38\u620f\u548c\u836f\u7269\u8bbe\u8ba1\u7b49\u591a\u4e2a\u9886\u57df\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u4e14\u6570\u636e\u9ad8\u6548\u3001\u5bf9\u793a\u4f8b\u5e73\u8861\u4e0d\u654f\u611f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u4ece\u4ec5\u6709\u7684\u6b63\u4f8b\u4e2d\u5b66\u4e60\u3002"}}
{"id": "2508.05922", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05922", "abs": "https://arxiv.org/abs/2508.05922", "authors": ["Sri Ramana Saketh Vasanthawada", "Pengkun Liu", "Pingbo Tang"], "title": "Enhancing Construction Site Analysis and Understanding with 3D Segmentation", "comment": null, "summary": "Monitoring construction progress is crucial yet resource-intensive, prompting\nthe exploration of computer-vision-based methodologies for enhanced efficiency\nand scalability. Traditional data acquisition methods, primarily focusing on\nindoor environments, falter in construction site's complex, cluttered, and\ndynamically changing conditions. This paper critically evaluates the\napplication of two advanced 3D segmentation methods, Segment Anything Model\n(SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained\ninitially on indoor datasets, both models' adaptability and performance are\nassessed in real-world construction settings, highlighting the gap in current\nsegmentation approaches due to the absence of benchmarks for outdoor scenarios.\nThrough a comparative analysis, this study not only showcases the relative\neffectiveness of SAM and Mask3D but also addresses the critical need for\ntailored segmentation workflows capable of extracting actionable insights from\nconstruction site data, thereby advancing the field towards more automated and\nprecise monitoring techniques.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86SAM\u548cMask3D\u4e24\u79cd3D\u5206\u5272\u65b9\u6cd5\u5728\u590d\u6742\u5efa\u7b51\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u6307\u51fa\u5f53\u524d\u65b9\u6cd5\u7f3a\u4e4f\u6237\u5916\u573a\u666f\u57fa\u51c6\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5efa\u7b51\u8fdb\u5ea6\u76d1\u6d4b\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u9002\u5e94\u590d\u6742\u591a\u53d8\u7684\u5efa\u7b51\u73af\u5883\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83SAM\u548cMask3D\u5728\u5ba4\u5185\u5916\u5efa\u7b51\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u5176\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u5206\u5272\u65b9\u6cd5\u5728\u6237\u5916\u573a\u666f\u4e2d\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u5b9a\u5236\u5316\u5de5\u4f5c\u6d41\u4ee5\u63d0\u53d6\u6709\u6548\u6570\u636e\u3002", "conclusion": "\u7814\u7a76\u63a8\u52a8\u4e86\u5efa\u7b51\u76d1\u6d4b\u6280\u672f\u5411\u66f4\u81ea\u52a8\u5316\u548c\u7cbe\u786e\u5316\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2508.06263", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06263", "abs": "https://arxiv.org/abs/2508.06263", "authors": ["Andrew Cropper", "David M. Cerna", "Matti J\u00e4rvisalo"], "title": "Symmetry breaking for inductive logic programming", "comment": null, "summary": "The goal of inductive logic programming is to search for a hypothesis that\ngeneralises training data and background knowledge. The challenge is searching\nvast hypothesis spaces, which is exacerbated because many logically equivalent\nhypotheses exist. To address this challenge, we introduce a method to break\nsymmetries in the hypothesis space. We implement our idea in answer set\nprogramming. Our experiments on multiple domains, including visual reasoning\nand game playing, show that our approach can reduce solving times from over an\nhour to just 17 seconds.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6253\u7834\u5047\u8bbe\u7a7a\u95f4\u5bf9\u79f0\u6027\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u7684\u6548\u7387\u3002", "motivation": "\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u4e2d\u5047\u8bbe\u7a7a\u95f4\u5e9e\u5927\u4e14\u5b58\u5728\u5927\u91cf\u903b\u8f91\u7b49\u4ef7\u5047\u8bbe\uff0c\u5bfc\u81f4\u641c\u7d22\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u901a\u8fc7\u6253\u7834\u5047\u8bbe\u7a7a\u95f4\u7684\u5bf9\u79f0\u6027\uff0c\u5e76\u5728\u7b54\u6848\u96c6\u7f16\u7a0b\u4e2d\u5b9e\u73b0\u8be5\u65b9\u6cd5\u3002", "result": "\u5728\u89c6\u89c9\u63a8\u7406\u548c\u6e38\u620f\u7b49\u591a\u4e2a\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6c42\u89e3\u65f6\u95f4\u4ece\u8d85\u8fc7\u4e00\u5c0f\u65f6\u7f29\u77ed\u81f317\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u4e2d\u7684\u641c\u7d22\u6548\u7387\u95ee\u9898\uff0c\u5177\u6709\u663e\u8457\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.05950", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05950", "abs": "https://arxiv.org/abs/2508.05950", "authors": ["Yanxing Liang", "Yinghui Wang", "Jinlong Yang", "Wei Li"], "title": "A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image", "comment": null, "summary": "The lack of spatial dimensional information remains a challenge in normal\nestimation from a single image. Recent diffusion-based methods have\ndemonstrated significant potential in 2D-to-3D implicit mapping, they rely on\ndata-driven statistical priors and miss the explicit modeling of light-surface\ninteraction, leading to multi-view normal direction conflicts. Moreover, the\ndiscrete sampling mechanism of diffusion models causes gradient discontinuity\nin differentiable rendering reconstruction modules, preventing 3D geometric\nerrors from being backpropagated to the normal generation network, thereby\nforcing existing methods to depend on dense normal annotations. This paper\nproposes SINGAD, a novel Self-supervised framework from a single Image for\nNormal estimation via 3D GAussian splatting guided Diffusion. By integrating\nphysics-driven light-interaction modeling and a differentiable rendering-based\nreprojection strategy, our framework directly converts 3D geometric errors into\nnormal optimization signals, solving the challenges of multi-view geometric\ninconsistency and data dependency. Specifically, the framework constructs a\nlight-interaction-driven 3DGS reparameterization model to generate multi-scale\ngeometric features consistent with light transport principles, ensuring\nmulti-view normal consistency. A cross-domain feature fusion module is designed\nwithin a conditional diffusion model, embedding geometric priors to constrain\nnormal generation while maintaining accurate geometric error propagation.\nFurthermore, a differentiable 3D reprojection loss strategy is introduced for\nself-supervised optimization that minimizes geometric error between the\nreconstructed and input image, eliminating dependence on annotated normal\ndatasets. Quantitative evaluations on the Google Scanned Objects dataset\ndemonstrate that our method outperforms state-of-the-art approaches across\nmultiple metrics.", "AI": {"tldr": "SINGAD\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc73D\u9ad8\u65af\u6e85\u5c04\u5f15\u5bfc\u7684\u6269\u6563\u65b9\u6cd5\u4ece\u5355\u5f20\u56fe\u50cf\u4f30\u8ba1\u6cd5\u7ebf\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u51e0\u4f55\u4e0d\u4e00\u81f4\u6027\u548c\u6570\u636e\u4f9d\u8d56\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u9a71\u52a8\u7684\u7edf\u8ba1\u5148\u9a8c\uff0c\u7f3a\u4e4f\u5bf9\u5149-\u8868\u9762\u4ea4\u4e92\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u5bfc\u81f4\u591a\u89c6\u89d2\u6cd5\u7ebf\u65b9\u5411\u51b2\u7a81\uff0c\u4e14\u6269\u6563\u6a21\u578b\u7684\u79bb\u6563\u91c7\u6837\u673a\u5236\u5bfc\u81f4\u68af\u5ea6\u4e0d\u8fde\u7eed\uff0c\u65e0\u6cd5\u53cd\u5411\u4f20\u64ad3D\u51e0\u4f55\u8bef\u5dee\u3002", "method": "\u7ed3\u5408\u7269\u7406\u9a71\u52a8\u7684\u5149\u4ea4\u4e92\u5efa\u6a21\u548c\u57fa\u4e8e\u53ef\u5fae\u5206\u6e32\u67d3\u7684\u91cd\u6295\u5f71\u7b56\u7565\uff0c\u6784\u5efa\u5149\u4ea4\u4e92\u9a71\u52a8\u76843DGS\u91cd\u53c2\u6570\u5316\u6a21\u578b\uff0c\u8bbe\u8ba1\u8de8\u57df\u7279\u5f81\u878d\u5408\u6a21\u5757\u548c\u53ef\u5fae\u52063D\u91cd\u6295\u5f71\u635f\u5931\u7b56\u7565\u3002", "result": "\u5728Google Scanned Objects\u6570\u636e\u96c6\u4e0a\u7684\u5b9a\u91cf\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SINGAD\u901a\u8fc7\u81ea\u76d1\u7763\u4f18\u5316\u548c\u51e0\u4f55\u8bef\u5dee\u4f20\u64ad\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u56fe\u50cf\u6cd5\u7ebf\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.06296", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06296", "abs": "https://arxiv.org/abs/2508.06296", "authors": ["Pierre Peign\u00e9 - Lefebvre", "Quentin Feuillade-Montixi", "Tom David", "Nicolas Miailhe"], "title": "LLM Robustness Leaderboard v1 --Technical report", "comment": null, "summary": "This technical report accompanies the LLM robustness leaderboard published by\nPRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior\nElicitation Tool (BET), an AI system performing automated red-teaming through\nDynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)\nagainst 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we\npropose a fine-grained robustness metric estimating the average number of\nattempts required to elicit harmful behaviors, revealing that attack difficulty\nvaries by over 300-fold across models despite universal vulnerability. We\nintroduce primitive-level vulnerability analysis to identify which jailbreaking\ntechniques are most effective for specific hazard categories. Our collaborative\nevaluation with trusted third parties from the AI Safety Network demonstrates\npractical pathways for distributed robustness assessment across the community.", "AI": {"tldr": "PRISM Eval\u5f00\u53d1\u4e86BET\u5de5\u5177\uff0c\u901a\u8fc7\u52a8\u6001\u5bf9\u6297\u4f18\u5316\u5b9e\u73b0100%\u653b\u51fb\u6210\u529f\u7387\uff0c\u5e76\u63d0\u51fa\u7ec6\u7c92\u5ea6\u9c81\u68d2\u6027\u6307\u6807\u3002", "motivation": "\u8bc4\u4f30LLM\u7684\u9c81\u68d2\u6027\uff0c\u63ed\u793a\u4e0d\u540c\u6a21\u578b\u5728\u5bf9\u6297\u653b\u51fb\u4e2d\u7684\u8106\u5f31\u6027\u5dee\u5f02\u3002", "method": "\u4f7f\u7528BET\u5de5\u5177\u8fdb\u884c\u81ea\u52a8\u5316\u7ea2\u961f\u6d4b\u8bd5\uff0c\u7ed3\u5408\u52a8\u6001\u5bf9\u6297\u4f18\u5316\u548c\u539f\u59cb\u7ea7\u6f0f\u6d1e\u5206\u6790\u3002", "result": "\u572841\u4e2a\u5148\u8fdbLLM\u4e2d\uff0c37\u4e2a\u88ab100%\u653b\u7834\uff0c\u653b\u51fb\u96be\u5ea6\u5dee\u5f02\u8fbe300\u500d\u3002", "conclusion": "\u63d0\u51fa\u4e86\u5206\u5e03\u5f0f\u9c81\u68d2\u6027\u8bc4\u4f30\u7684\u5b9e\u7528\u8def\u5f84\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u6539\u8fdbLLM\u5b89\u5168\u6027\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.05954", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05954", "abs": "https://arxiv.org/abs/2508.05954", "authors": ["Han Lin", "Jaemin Cho", "Amir Zadeh", "Chuan Li", "Mohit Bansal"], "title": "Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents", "comment": "Project Page: https://bifrost-1.github.io", "summary": "There is growing interest in integrating high-fidelity visual synthesis\ncapabilities into large language models (LLMs) without compromising their\nstrong reasoning capabilities. Existing methods that directly train LLMs or\nbridge LLMs and diffusion models usually suffer from costly training since the\nbackbone LLMs have not seen image representations during pretraining. We\npresent Bifrost-1, a unified framework that bridges pretrained multimodal LLMs\n(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent\nvariables, which are natively aligned with the MLLM's CLIP visual encoder.\nThese patch-level image embeddings are integrated into the diffusion model with\na lightweight adaptation of its ControlNet. To retain the original multimodal\nreasoning capabilities of MLLMs, we equip the MLLM with a visual generation\nbranch initialized from the original MLLM parameters when predicting the\npatch-level image embeddings. By seamlessly integrating pretrained MLLMs and\ndiffusion models with patch-level CLIP latents, our framework enables\nhigh-fidelity controllable image generation with significant training\nefficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or\nbetter performance than previous methods in terms of visual fidelity and\nmultimodal understanding, with substantially lower compute during training. We\nalso provide comprehensive ablation studies showing the effectiveness of our\ndesign choices.", "AI": {"tldr": "Bifrost-1\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u7684\u591a\u6a21\u6001LLM\u4e0e\u6269\u6563\u6a21\u578b\u7ed3\u5408\uff0c\u5229\u7528CLIP\u56fe\u50cf\u5d4c\u5165\u4f5c\u4e3a\u6f5c\u5728\u53d8\u91cf\uff0c\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u548c\u9ad8\u4fdd\u771f\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6574\u5408LLM\u548c\u6269\u6563\u6a21\u578b\u65f6\u9762\u4e34\u9ad8\u6210\u672c\u8bad\u7ec3\u95ee\u9898\uff0c\u56e0\u4e3aLLM\u5728\u9884\u8bad\u7ec3\u4e2d\u672a\u63a5\u89e6\u56fe\u50cf\u8868\u793a\u3002", "method": "\u4f7f\u7528patch\u7ea7CLIP\u56fe\u50cf\u5d4c\u5165\u4f5c\u4e3a\u6f5c\u5728\u53d8\u91cf\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7ControlNet\u9002\u914d\u6269\u6563\u6a21\u578b\uff0c\u5e76\u4fdd\u7559MLLM\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "result": "Bifrost-1\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u591a\u6a21\u6001\u7406\u89e3\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86\u9884\u8bad\u7ec3MLLM\u548c\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u53ef\u63a7\u7684\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2508.06326", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06326", "abs": "https://arxiv.org/abs/2508.06326", "authors": ["Nathaniel Virgo", "Martin Biehl", "Manuel Baltieri", "Matteo Capucci"], "title": "A \"good regulator theorem\" for embodied agents", "comment": "Accepted at the Artificial Life conference 2025 (ALife 2025). 10\n  pages, 1 figure", "summary": "In a classic paper, Conant and Ashby claimed that \"every good regulator of a\nsystem must be a model of that system.\" Artificial Life has produced many\nexamples of systems that perform tasks with apparently no model in sight; these\nsuggest Conant and Ashby's theorem doesn't easily generalise beyond its\nrestricted setup. Nevertheless, here we show that a similar intuition can be\nfleshed out in a different way: whenever an agent is able to perform a\nregulation task, it is possible for an observer to interpret it as having\n\"beliefs\" about its environment, which it \"updates\" in response to sensory\ninput. This notion of belief updating provides a notion of model that is more\nsophisticated than Conant and Ashby's, as well as a theorem that is more\nbroadly applicable. However, it necessitates a change in perspective, in that\nthe observer plays an essential role in the theory: models are not a mere\nproperty of the system but are imposed on it from outside. Our theorem holds\nregardless of whether the system is regulating its environment in a classic\ncontrol theory setup, or whether it's regulating its own internal state; the\nmodel is of its environment either way. The model might be trivial, however,\nand this is how the apparent counterexamples are resolved.", "AI": {"tldr": "\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86Conant\u548cAshby\u7684\u5b9a\u7406\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u5e7f\u4e49\u7684\u6a21\u578b\u6982\u5ff5\uff0c\u8ba4\u4e3a\u89c2\u5bdf\u8005\u5728\u5b9a\u4e49\u6a21\u578b\u65f6\u8d77\u5173\u952e\u4f5c\u7528\u3002", "motivation": "Conant\u548cAshby\u7684\u5b9a\u7406\u5728\u66f4\u5e7f\u6cdb\u7684\u7cfb\u7edf\u4e2d\u96be\u4ee5\u9002\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u6a21\u578b\u5b9a\u4e49\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u89c2\u5bdf\u8005\u7684\u89c6\u89d2\uff0c\u5c06\u6a21\u578b\u5b9a\u4e49\u4e3a\u5916\u90e8\u5f3a\u52a0\u4e8e\u7cfb\u7edf\u7684\u4fe1\u5ff5\u66f4\u65b0\u8fc7\u7a0b\u3002", "result": "\u8bc1\u660e\u4e86\u65e0\u8bba\u7cfb\u7edf\u662f\u8c03\u8282\u73af\u5883\u8fd8\u662f\u5185\u90e8\u72b6\u6001\uff0c\u90fd\u53ef\u4ee5\u88ab\u89e3\u91ca\u4e3a\u5177\u6709\u6a21\u578b\u3002", "conclusion": "\u6a21\u578b\u7684\u5b58\u5728\u4f9d\u8d56\u4e8e\u89c2\u5bdf\u8005\u7684\u89e3\u91ca\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u539f\u6709\u5b9a\u7406\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.05976", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05976", "abs": "https://arxiv.org/abs/2508.05976", "authors": ["Zhihao Zhu", "Yifan Zheng", "Siyu Pan", "Yaohui Jin", "Yao Mu"], "title": "PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation", "comment": "Accepted to ICCV 2025. 8 pages main paper, 8 figures, plus\n  supplementary material", "summary": "The fragmentation between high-level task semantics and low-level geometric\nfeatures remains a persistent challenge in robotic manipulation. While\nvision-language models (VLMs) have shown promise in generating affordance-aware\nvisual representations, the lack of semantic grounding in canonical spaces and\nreliance on manual annotations severely limit their ability to capture dynamic\nsemantic-affordance relationships. To address these, we propose Primitive-Aware\nSemantic Grounding (PASG), a closed-loop framework that introduces: (1)\nAutomatic primitive extraction through geometric feature aggregation, enabling\ncross-category detection of keypoints and axes; (2) VLM-driven semantic\nanchoring that dynamically couples geometric primitives with functional\naffordances and task-relevant description; (3) A spatial-semantic reasoning\nbenchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's\neffectiveness in practical robotic manipulation tasks across diverse scenarios,\nachieving performance comparable to manual annotations. PASG achieves a\nfiner-grained semantic-affordance understanding of objects, establishing a\nunified paradigm for bridging geometric primitives with task semantics in\nrobotic manipulation.", "AI": {"tldr": "PASG\u6846\u67b6\u901a\u8fc7\u51e0\u4f55\u7279\u5f81\u805a\u5408\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u52a8\u6001\u8026\u5408\u51e0\u4f55\u57fa\u5143\u4e0e\u529f\u80fd\u53ef\u4f9b\u6027\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8bed\u4e49\u4e0e\u51e0\u4f55\u7279\u5f81\u7684\u5272\u88c2\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u9ad8\u5c42\u4efb\u52a1\u8bed\u4e49\u4e0e\u4f4e\u5c42\u51e0\u4f55\u7279\u5f81\u4e4b\u95f4\u7684\u5272\u88c2\u95ee\u9898\u9650\u5236\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e94\u7528\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u8bed\u4e49\u63a5\u5730\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPASG\u6846\u67b6\uff0c\u5305\u62ec\u81ea\u52a8\u57fa\u5143\u63d0\u53d6\u3001VLM\u9a71\u52a8\u7684\u8bed\u4e49\u951a\u5b9a\u548c\u7a7a\u95f4\u8bed\u4e49\u63a8\u7406\u57fa\u51c6\u3002", "result": "PASG\u5728\u591a\u6837\u5316\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u63a5\u8fd1\u4eba\u5de5\u6807\u6ce8\u3002", "conclusion": "PASG\u5b9e\u73b0\u4e86\u5bf9\u7269\u4f53\u66f4\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49-\u53ef\u4f9b\u6027\u7406\u89e3\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u51e0\u4f55\u57fa\u5143\u4e0e\u4efb\u52a1\u8bed\u4e49\u63d0\u4f9b\u4e86\u7edf\u4e00\u8303\u5f0f\u3002"}}
{"id": "2508.06348", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06348", "abs": "https://arxiv.org/abs/2508.06348", "authors": ["Mille Mei Zhen Loo", "Gert Luzkov", "Paolo Burelli"], "title": "AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games", "comment": null, "summary": "Cheating in online video games compromises the integrity of gaming\nexperiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face\nsignificant challenges in keeping pace with evolving cheating methods without\nimposing invasive measures on users' systems. This paper presents\nAntiCheatPT\\_256, a transformer-based machine learning model designed to detect\ncheating behaviour in Counter-Strike 2 using gameplay data. To support this, we\nintroduce and publicly release CS2CD: A labelled dataset of 795 matches. Using\nthis dataset, 90,707 context windows were created and subsequently augmented to\naddress class imbalance. The transformer model, trained on these windows,\nachieved an accuracy of 89.17\\% and an AUC of 93.36\\% on an unaugmented test\nset. This approach emphasizes reproducibility and real-world applicability,\noffering a robust baseline for future research in data-driven cheat detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578bAntiCheatPT_256\uff0c\u7528\u4e8e\u68c0\u6d4b\u300a\u53cd\u6050\u7cbe\u82f12\u300b\u4e2d\u7684\u4f5c\u5f0a\u884c\u4e3a\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6CS2CD\u3002\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5728\u7ebf\u6e38\u620f\u4e2d\u7684\u4f5c\u5f0a\u884c\u4e3a\u7834\u574f\u4e86\u6e38\u620f\u4f53\u9a8c\u7684\u516c\u5e73\u6027\uff0c\u73b0\u6709\u53cd\u4f5c\u5f0a\u7cfb\u7edf\uff08\u5982VAC\uff09\u96be\u4ee5\u5728\u4e0d\u4fb5\u72af\u7528\u6237\u9690\u79c1\u7684\u60c5\u51b5\u4e0b\u5e94\u5bf9\u4e0d\u65ad\u6f14\u53d8\u7684\u4f5c\u5f0a\u624b\u6bb5\u3002", "method": "\u4f7f\u7528\u516c\u5f00\u6570\u636e\u96c6CS2CD\uff08\u5305\u542b795\u573a\u6bd4\u8d5b\uff09\uff0c\u751f\u6210\u5e76\u589e\u5f3a90,707\u4e2a\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u8bad\u7ec3Transformer\u6a21\u578bAntiCheatPT_256\u3002", "result": "\u6a21\u578b\u5728\u672a\u589e\u5f3a\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523089.17%\u7684\u51c6\u786e\u7387\u548c93.36%\u7684AUC\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5f3a\u8c03\u53ef\u91cd\u590d\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6027\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u4f5c\u5f0a\u68c0\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2508.05982", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05982", "abs": "https://arxiv.org/abs/2508.05982", "authors": ["Qingyang Liu", "Bingjie Gao", "Weiheng Huang", "Jun Zhang", "Zhongqian Sun", "Yang Wei", "Zelin Peng", "Qianli Ma", "Shuai Yang", "Zhaohe Liao", "Haonan Zhao", "Li Niu"], "title": "AnimateScene: Camera-controllable Animation in Any Scene", "comment": null, "summary": "3D scene reconstruction and 4D human animation have seen rapid progress and\nbroad adoption in recent years. However, seamlessly integrating reconstructed\nscenes with 4D human animation to produce visually engaging results remains\nchallenging. One key difficulty lies in placing the human at the correct\nlocation and scale within the scene while avoiding unrealistic\ninterpenetration. Another challenge is that the human and the background may\nexhibit different lighting and style, leading to unrealistic composites. In\naddition, appealing character motion videos are often accompanied by camera\nmovements, which means that the viewpoints need to be reconstructed along a\nspecified trajectory. We present AnimateScene, which addresses the above issues\nin a unified framework. First, we design an accurate placement module that\nautomatically determines a plausible 3D position for the human and prevents any\ninterpenetration within the scene during motion. Second, we propose a\ntraining-free style alignment method that adapts the 4D human representation to\nmatch the background's lighting and style, achieving coherent visual\nintegration. Finally, we design a joint post-reconstruction method for both the\n4D human and the 3D scene that allows camera trajectories to be inserted,\nenabling the final rendered video to feature visually appealing camera\nmovements. Extensive experiments show that AnimateScene generates dynamic scene\nvideos with high geometric detail and spatiotemporal coherence across various\ncamera and action combinations.", "AI": {"tldr": "AnimateScene\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u51b33D\u573a\u666f\u91cd\u5efa\u4e0e4D\u4eba\u4f53\u52a8\u753b\u65e0\u7f1d\u96c6\u6210\u7684\u6311\u6218\uff0c\u5305\u62ec\u4f4d\u7f6e\u653e\u7f6e\u3001\u98ce\u683c\u5bf9\u9f50\u548c\u76f8\u673a\u8f68\u8ff9\u63d2\u5165\u3002", "motivation": "\u5c06\u91cd\u5efa\u76843D\u573a\u666f\u4e0e4D\u4eba\u4f53\u52a8\u753b\u65e0\u7f1d\u96c6\u6210\u5b58\u5728\u4f4d\u7f6e\u3001\u5149\u7167\u548c\u76f8\u673a\u8f68\u8ff9\u7684\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u7cbe\u786e\u653e\u7f6e\u6a21\u5757\u3001\u65e0\u8bad\u7ec3\u98ce\u683c\u5bf9\u9f50\u65b9\u6cd5\u53ca\u8054\u5408\u540e\u91cd\u5efa\u65b9\u6cd5\u3002", "result": "\u751f\u6210\u5177\u6709\u9ad8\u51e0\u4f55\u7ec6\u8282\u548c\u65f6\u7a7a\u4e00\u81f4\u6027\u7684\u52a8\u6001\u573a\u666f\u89c6\u9891\u3002", "conclusion": "AnimateScene\u6709\u6548\u89e3\u51b3\u4e86\u96c6\u6210\u95ee\u9898\uff0c\u5e76\u652f\u6301\u591a\u6837\u5316\u76f8\u673a\u548c\u52a8\u4f5c\u7ec4\u5408\u3002"}}
{"id": "2508.06352", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06352", "abs": "https://arxiv.org/abs/2508.06352", "authors": ["Christian Meske", "Justin Brenne", "Erdi Uenal", "Sabahat Oelcer", "Ayseguel Doganguen"], "title": "From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI", "comment": null, "summary": "Current explainable AI (XAI) approaches prioritize algorithmic transparency\nand present explanations in abstract, non-adaptive formats that often fail to\nsupport meaningful end-user understanding. This paper introduces \"Explanatory\nAI\" as a complementary paradigm that leverages generative AI capabilities to\nserve as explanatory partners for human understanding rather than providers of\nalgorithmic transparency. While XAI reveals algorithmic decision processes for\nmodel validation, Explanatory AI addresses contextual reasoning to support\nhuman decision-making in sociotechnical contexts. We develop a definition and\nsystematic eight-dimensional conceptual model distinguishing Explanatory AI\nthrough narrative communication, adaptive personalization, and progressive\ndisclosure principles. Empirical validation through Rapid Contextual Design\nmethodology with healthcare professionals demonstrates that users consistently\nprefer context-sensitive, multimodal explanations over technical transparency.\nOur findings reveal the practical urgency for AI systems designed for human\ncomprehension rather than algorithmic introspection, establishing a\ncomprehensive research agenda for advancing user-centered AI explanation\napproaches across diverse domains and cultural contexts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u89e3\u91ca\u6027AI\u201d\u4f5c\u4e3a\u53ef\u89e3\u91caAI\uff08XAI\uff09\u7684\u8865\u5145\u8303\u5f0f\uff0c\u5f3a\u8c03\u901a\u8fc7\u751f\u6210\u5f0fAI\u80fd\u529b\u652f\u6301\u4eba\u7c7b\u7406\u89e3\uff0c\u800c\u975e\u4ec5\u63d0\u4f9b\u7b97\u6cd5\u900f\u660e\u5ea6\u3002", "motivation": "\u5f53\u524dXAI\u65b9\u6cd5\u8fc7\u4e8e\u5173\u6ce8\u7b97\u6cd5\u900f\u660e\u5ea6\uff0c\u63d0\u4f9b\u7684\u89e3\u91ca\u62bd\u8c61\u4e14\u975e\u81ea\u9002\u5e94\uff0c\u96be\u4ee5\u6ee1\u8db3\u7ec8\u7aef\u7528\u6237\u7684\u7406\u89e3\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u516b\u7ef4\u6982\u5ff5\u6a21\u578b\uff0c\u5f3a\u8c03\u53d9\u4e8b\u6c9f\u901a\u3001\u81ea\u9002\u5e94\u4e2a\u6027\u5316\u548c\u6e10\u8fdb\u62ab\u9732\u539f\u5219\uff0c\u5e76\u901a\u8fc7\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u7684\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u3002", "result": "\u7528\u6237\u66f4\u504f\u597d\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u591a\u6a21\u6001\u89e3\u91ca\uff0c\u800c\u975e\u6280\u672f\u900f\u660e\u5ea6\u3002", "conclusion": "AI\u7cfb\u7edf\u5e94\u8bbe\u8ba1\u4e3a\u652f\u6301\u4eba\u7c7b\u7406\u89e3\uff0c\u800c\u975e\u4ec5\u5173\u6ce8\u7b97\u6cd5\u5185\u7701\uff0c\u4e3a\u8de8\u9886\u57df\u548c\u6587\u5316\u80cc\u666f\u7684\u7528\u6237\u4e2d\u5fc3\u89e3\u91ca\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7814\u7a76\u8bae\u7a0b\u3002"}}
{"id": "2508.05989", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05989", "abs": "https://arxiv.org/abs/2508.05989", "authors": ["Younjoon Chung", "Hyoungseob Park", "Patrick Rim", "Xiaoran Zhang", "Jihe He", "Ziyao Zeng", "Safa Cicek", "Byung-Woo Hong", "James S. Duncan", "Alex Wong"], "title": "ETA: Energy-based Test-time Adaptation for Depth Completion", "comment": null, "summary": "We propose a method for test-time adaptation of pretrained depth completion\nmodels. Depth completion models, trained on some ``source'' data, often predict\nerroneous outputs when transferred to ``target'' data captured in novel\nenvironmental conditions due to a covariate shift. The crux of our method lies\nin quantifying the likelihood of depth predictions belonging to the source data\ndistribution. The challenge is in the lack of access to out-of-distribution\n(target) data prior to deployment. Hence, rather than making assumptions\nregarding the target distribution, we utilize adversarial perturbations as a\nmechanism to explore the data space. This enables us to train an energy model\nthat scores local regions of depth predictions as in- or out-of-distribution.\nWe update the parameters of pretrained depth completion models at test time to\nminimize energy, effectively aligning test-time predictions to those of the\nsource distribution. We call our method ``Energy-based Test-time Adaptation'',\nor ETA for short. We evaluate our method across three indoor and three outdoor\ndatasets, where ETA improve over the previous state-of-the-art method by an\naverage of 6.94% for outdoors and 10.23% for indoors. Project Page:\nhttps://fuzzythecat.github.io/eta.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff08ETA\uff09\uff0c\u7528\u4e8e\u8c03\u6574\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u8865\u5168\u6a21\u578b\u4ee5\u9002\u5e94\u65b0\u73af\u5883\u6570\u636e\u3002", "motivation": "\u6df1\u5ea6\u8865\u5168\u6a21\u578b\u5728\u4ece\u6e90\u6570\u636e\u8f6c\u79fb\u5230\u76ee\u6807\u6570\u636e\u65f6\uff0c\u7531\u4e8e\u534f\u53d8\u91cf\u504f\u79fb\uff0c\u9884\u6d4b\u7ed3\u679c\u53ef\u80fd\u4e0d\u51c6\u786e\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u76ee\u6807\u6570\u636e\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u5047\u8bbe\u76ee\u6807\u5206\u5e03\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5bf9\u6297\u6027\u6270\u52a8\u63a2\u7d22\u6570\u636e\u7a7a\u95f4\uff0c\u8bad\u7ec3\u80fd\u91cf\u6a21\u578b\u4ee5\u8bc4\u4f30\u6df1\u5ea6\u9884\u6d4b\u7684\u5c40\u90e8\u533a\u57df\u662f\u5426\u5c5e\u4e8e\u6e90\u6570\u636e\u5206\u5e03\u3002\u5728\u6d4b\u8bd5\u65f6\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u4ee5\u6700\u5c0f\u5316\u80fd\u91cf\uff0c\u4f7f\u9884\u6d4b\u4e0e\u6e90\u5206\u5e03\u5bf9\u9f50\u3002", "result": "\u5728\u4e09\u4e2a\u5ba4\u5185\u548c\u4e09\u4e2a\u5ba4\u5916\u6570\u636e\u96c6\u4e0a\uff0cETA\u5e73\u5747\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u9ad8\u4e866.94%\uff08\u5ba4\u5916\uff09\u548c10.23%\uff08\u5ba4\u5185\uff09\u3002", "conclusion": "ETA\u901a\u8fc7\u80fd\u91cf\u6a21\u578b\u548c\u6d4b\u8bd5\u65f6\u53c2\u6570\u66f4\u65b0\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6df1\u5ea6\u8865\u5168\u6a21\u578b\u5728\u65b0\u73af\u5883\u4e2d\u7684\u9002\u5e94\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2508.06368", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06368", "abs": "https://arxiv.org/abs/2508.06368", "authors": ["Claudia dAmato", "Giuseppe Rubini", "Francesco Didio", "Donato Francioso", "Fatima Zahra Amara", "Nicola Fanizzi"], "title": "Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned", "comment": null, "summary": "Legal decision-making process requires the availability of comprehensive and\ndetailed legislative background knowledge and up-to-date information on legal\ncases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a\nvaluable tool to facilitate access to legal information, to be queried and\nexploited for the purpose, and to enable advanced reasoning and machine\nlearning applications. Indeed, legal KGs may act as knowledge intensive\ncomponent to be used by pre-dictive machine learning solutions supporting the\ndecision process of the legal expert. Nevertheless, a few KGs can be found in\nthe legal domain. To fill this gap, we developed a legal KG targeting legal\ncases of violence against women, along with clear adopted methodologies.\nSpecifically, the paper introduces two complementary approaches for automated\nlegal KG construction; a systematic bottom-up approach, customized for the\nlegal domain, and a new solution leveraging Large Language Models. Starting\nfrom legal sentences publicly available from the European Court of Justice, the\nsolutions integrate structured data extraction, ontology development, and\nsemantic enrichment to produce KGs tailored for legal cases involving violence\nagainst women. After analyzing and comparing the results of the two approaches,\nthe developed KGs are validated via suitable competency questions. The obtained\nKG may be impactful for multiple purposes: can improve the accessibility to\nlegal information both to humans and machine, can enable complex queries and\nmay constitute an important knowledge component to be possibly exploited by\nmachine learning tools tailored for predictive justice.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u6784\u5efa\u6cd5\u5f8b\u77e5\u8bc6\u56fe\u8c31\u7684\u65b9\u6cd5\uff0c\u586b\u8865\u6cd5\u5f8b\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u7684\u7a7a\u767d\uff0c\u5e76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6cd5\u5f8b\u51b3\u7b56\u9700\u8981\u5168\u9762\u7684\u6cd5\u5f8b\u80cc\u666f\u77e5\u8bc6\u548c\u6700\u65b0\u6848\u4f8b\u4fe1\u606f\uff0c\u4f46\u6cd5\u5f8b\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u7a00\u7f3a\uff0c\u56e0\u6b64\u5f00\u53d1\u9488\u5bf9\u66b4\u529b\u4fb5\u5bb3\u5987\u5973\u6848\u4ef6\u7684\u6cd5\u5f8b\u77e5\u8bc6\u56fe\u8c31\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u81ea\u4e0b\u800c\u4e0a\u65b9\u6cd5\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u6570\u636e\u63d0\u53d6\u3001\u672c\u4f53\u5f00\u53d1\u548c\u8bed\u4e49\u589e\u5f3a\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u6784\u5efa\u7684\u6cd5\u5f8b\u77e5\u8bc6\u56fe\u8c31\u901a\u8fc7\u9a8c\u8bc1\uff0c\u53ef\u63d0\u5347\u6cd5\u5f8b\u4fe1\u606f\u53ef\u8bbf\u95ee\u6027\uff0c\u652f\u6301\u590d\u6742\u67e5\u8be2\u548c\u673a\u5668\u5b66\u4e60\u5de5\u5177\u3002", "conclusion": "\u5f00\u53d1\u7684\u6cd5\u5f8b\u77e5\u8bc6\u56fe\u8c31\u5bf9\u6cd5\u5f8b\u4fe1\u606f\u8bbf\u95ee\u548c\u9884\u6d4b\u6027\u53f8\u6cd5\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2508.05990", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05990", "abs": "https://arxiv.org/abs/2508.05990", "authors": ["Haichao Wang", "Xinyue Xi", "Jiangtao Wen", "Yuxing Han"], "title": "Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision", "comment": null, "summary": "The efficiency of video computer vision system remains a challenging task due\nto the high temporal redundancy inside a video. Existing works have been\nproposed for efficient vision computer vision. However, they do not fully\nreduce the temporal redundancy and neglect the front end computation overhead.\nIn this paper, we propose an efficient video computer vision system. First,\nimage signal processor is removed and Bayer-format data is directly fed into\nvideo computer vision models, thus saving the front end computation. Second,\ninstead of optical flow models and video codecs, a fast block matching-based\nmotion estimation algorithm is proposed specifically for efficient video\ncomputer vision, with a MV refinement module. To correct the error,\ncontext-aware block refinement network is introduced to refine regions with\nlarge error. To further balance the accuracy and efficiency, a frame selection\nstrategy is employed. Experiments on multiple video computer vision tasks\ndemonstrate that our method achieves significant acceleration with slight\nperformance loss.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u89c6\u9891\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\uff0c\u901a\u8fc7\u53bb\u9664\u56fe\u50cf\u4fe1\u53f7\u5904\u7406\u5668\u548c\u5f15\u5165\u5feb\u901f\u5757\u5339\u914d\u8fd0\u52a8\u4f30\u8ba1\u7b97\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u51cf\u5c11\u89c6\u9891\u4e2d\u7684\u65f6\u95f4\u5197\u4f59\uff0c\u4e14\u5ffd\u89c6\u4e86\u524d\u7aef\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u76f4\u63a5\u8f93\u5165Bayer\u683c\u5f0f\u6570\u636e\uff0c\u91c7\u7528\u5feb\u901f\u5757\u5339\u914d\u8fd0\u52a8\u4f30\u8ba1\u7b97\u6cd5\u548cMV\u7ec6\u5316\u6a21\u5757\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u611f\u77e5\u5757\u7ec6\u5316\u7f51\u7edc\u548c\u5e27\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u5b9e\u73b0\u663e\u8457\u52a0\u901f\uff0c\u6027\u80fd\u635f\u5931\u8f7b\u5fae\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2508.06443", "categories": ["cs.AI", "cs.CY", "cs.ET", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.06443", "abs": "https://arxiv.org/abs/2508.06443", "authors": ["Debabrota Basu", "Udvas Das"], "title": "The Fair Game: Auditing & Debiasing AI Algorithms Over Time", "comment": null, "summary": "An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify\ndifferent types of bias (also known as unfairness) exhibited in the predictions\nof ML algorithms, and to design new algorithms to mitigate them. Often, the\ndefinitions of bias used in the literature are observational, i.e. they use the\ninput and output of a pre-trained algorithm to quantify a bias under concern.\nIn reality,these definitions are often conflicting in nature and can only be\ndeployed if either the ground truth is known or only in retrospect after\ndeploying the algorithm. Thus,there is a gap between what we want Fair ML to\nachieve and what it does in a dynamic social environment. Hence, we propose an\nalternative dynamic mechanism,\"Fair Game\",to assure fairness in the predictions\nof an ML algorithm and to adapt its predictions as the society interacts with\nthe algorithm over time. \"Fair Game\" puts together an Auditor and a Debiasing\nalgorithm in a loop around an ML algorithm. The \"Fair Game\" puts these two\ncomponents in a loop by leveraging Reinforcement Learning (RL). RL algorithms\ninteract with an environment to take decisions, which yields new observations\n(also known as data/feedback) from the environment and in turn, adapts future\ndecisions. RL is already used in algorithms with pre-fixed long-term fairness\ngoals. \"Fair Game\" provides a unique framework where the fairness goals can be\nadapted over time by only modifying the auditor and the different biases it\nquantifies. Thus,\"Fair Game\" aims to simulate the evolution of ethical and\nlegal frameworks in the society by creating an auditor which sends feedback to\na debiasing algorithm deployed around an ML system. This allows us to develop a\nflexible and adaptive-over-time framework to build Fair ML systems pre- and\npost-deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u673a\u5236\u201cFair Game\u201d\uff0c\u901a\u8fc7\u7ed3\u5408\u5ba1\u8ba1\u5458\u548c\u53bb\u504f\u7b97\u6cd5\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u516c\u5e73\u6027\u76ee\u6807\u968f\u65f6\u95f4\u8c03\u6574\u3002", "motivation": "\u73b0\u6709\u516c\u5e73\u6027\u5b9a\u4e49\u591a\u4e3a\u89c2\u5bdf\u6027\u4e14\u76f8\u4e92\u51b2\u7a81\uff0c\u96be\u4ee5\u5728\u52a8\u6001\u793e\u4f1a\u73af\u5883\u4e2d\u5b9e\u9645\u5e94\u7528\uff0c\u9700\u8981\u4e00\u79cd\u7075\u6d3b\u9002\u5e94\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u5ba1\u8ba1\u5458\u548c\u53bb\u504f\u7b97\u6cd5\u5faa\u73af\u7ed3\u5408\uff0c\u52a8\u6001\u8c03\u6574\u516c\u5e73\u6027\u76ee\u6807\u3002", "result": "\u201cFair Game\u201d\u80fd\u591f\u6a21\u62df\u793e\u4f1a\u4f26\u7406\u548c\u6cd5\u5f8b\u6846\u67b6\u7684\u6f14\u53d8\uff0c\u63d0\u4f9b\u7075\u6d3b\u7684\u524d\u540e\u90e8\u7f72\u516c\u5e73\u6027\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u516c\u5e73\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u52a8\u6001\u9002\u5e94\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5e94\u5bf9\u793e\u4f1a\u73af\u5883\u7684\u6301\u7eed\u53d8\u5316\u3002"}}
{"id": "2508.05991", "categories": ["cs.CV", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.05991", "abs": "https://arxiv.org/abs/2508.05991", "authors": ["Juewen Hu", "Yexin Li", "Jiulin Li", "Shuo Chen", "Pring Wong"], "title": "ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge", "comment": null, "summary": "Emotion recognition plays a vital role in enhancing human-computer\ninteraction. In this study, we tackle the MER-SEMI challenge of the MER2025\ncompetition by proposing a novel multimodal emotion recognition framework. To\naddress the issue of data scarcity, we leverage large-scale pre-trained models\nto extract informative features from visual, audio, and textual modalities.\nSpecifically, for the visual modality, we design a dual-branch visual encoder\nthat captures both global frame-level features and localized facial\nrepresentations. For the textual modality, we introduce a context-enriched\nmethod that employs large language models to enrich emotional cues within the\ninput text. To effectively integrate these multimodal features, we propose a\nfusion strategy comprising two key components, i.e., self-attention mechanisms\nfor dynamic modality weighting, and residual connections to preserve original\nrepresentations. Beyond architectural design, we further refine noisy labels in\nthe training set by a multi-source labeling strategy. Our approach achieves a\nsubstantial performance improvement over the official baseline on the\nMER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to\n78.63%, thereby validating the effectiveness of the proposed framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u53cc\u5206\u652f\u89c6\u89c9\u7f16\u7801\u5668\u548c\u4e0a\u4e0b\u6587\u4e30\u5bcc\u6587\u672c\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u6b8b\u5dee\u8fde\u63a5\u8fdb\u884c\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86MER2025-SEMI\u6570\u636e\u96c6\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u4ee5\u589e\u5f3a\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u53d6\u89c6\u89c9\u3001\u97f3\u9891\u548c\u6587\u672c\u7279\u5f81\uff1b\u8bbe\u8ba1\u53cc\u5206\u652f\u89c6\u89c9\u7f16\u7801\u5668\u548c\u4e0a\u4e0b\u6587\u4e30\u5bcc\u6587\u672c\u65b9\u6cd5\uff1b\u91c7\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u6b8b\u5dee\u8fde\u63a5\u8fdb\u884c\u7279\u5f81\u878d\u5408\uff1b\u901a\u8fc7\u591a\u6e90\u6807\u7b7e\u7b56\u7565\u4f18\u5316\u8bad\u7ec3\u96c6\u6807\u7b7e\u3002", "result": "\u5728MER2025-SEMI\u6570\u636e\u96c6\u4e0a\uff0c\u52a0\u6743F-score\u8fbe\u523087.49%\uff0c\u663e\u8457\u4f18\u4e8e\u5b98\u65b9\u57fa\u7ebf\uff0878.63%\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.06454", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.06454", "abs": "https://arxiv.org/abs/2508.06454", "authors": ["Joshua Caiata", "Ben Armstrong", "Kate Larson"], "title": "What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting", "comment": "41 pages", "summary": "Committee-selection problems arise in many contexts and applications, and\nthere has been increasing interest within the social choice research community\non identifying which properties are satisfied by different multi-winner voting\nrules. In this work, we propose a data-driven framework to evaluate how\nfrequently voting rules violate axioms across diverse preference distributions\nin practice, shifting away from the binary perspective of axiom satisfaction\ngiven by worst-case analysis. Using this framework, we analyze the relationship\nbetween multi-winner voting rules and their axiomatic performance under several\npreference distributions. We then show that neural networks, acting as voting\nrules, can outperform traditional rules in minimizing axiom violations. Our\nresults suggest that data-driven approaches to social choice can inform the\ndesign of new voting systems and support the continuation of data-driven\nresearch in social choice.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6295\u7968\u89c4\u5219\u5728\u4e0d\u540c\u504f\u597d\u5206\u5e03\u4e0b\u8fdd\u53cd\u516c\u7406\u7684\u9891\u7387\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5c55\u793a\u4e86\u5176\u5728\u51cf\u5c11\u516c\u7406\u8fdd\u53cd\u65b9\u9762\u7684\u4f18\u52bf\u3002", "motivation": "\u7814\u7a76\u591a\u8d62\u5bb6\u6295\u7968\u89c4\u5219\u5728\u5b9e\u9645\u504f\u597d\u5206\u5e03\u4e2d\u7684\u516c\u7406\u8868\u73b0\uff0c\u4ee5\u8865\u5145\u6700\u574f\u60c5\u51b5\u5206\u6790\u7684\u4e8c\u5143\u89c6\u89d2\u3002", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u5206\u6790\u591a\u79cd\u504f\u597d\u5206\u5e03\u4e0b\u6295\u7968\u89c4\u5219\u4e0e\u516c\u7406\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u5e76\u5f15\u5165\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u6295\u7968\u89c4\u5219\u3002", "result": "\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u6295\u7968\u89c4\u5219\u5728\u51cf\u5c11\u516c\u7406\u8fdd\u53cd\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u89c4\u5219\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u53ef\u4e3a\u65b0\u6295\u7968\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4fe1\u606f\uff0c\u5e76\u652f\u6301\u793e\u4f1a\u9009\u62e9\u9886\u57df\u7684\u6570\u636e\u9a71\u52a8\u7814\u7a76\u3002"}}
{"id": "2508.05994", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05994", "abs": "https://arxiv.org/abs/2508.05994", "authors": ["Huadong Wu", "Yi Fu", "Yunhao Li", "Yuan Gao", "Kang Du"], "title": "EvoMakeup: High-Fidelity and Controllable Makeup Editing with MakeupQuad", "comment": null, "summary": "Facial makeup editing aims to realistically transfer makeup from a reference\nto a target face. Existing methods often produce low-quality results with\ncoarse makeup details and struggle to preserve both identity and makeup\nfidelity, mainly due to the lack of structured paired data -- where source and\nresult share identity, and reference and result share identical makeup. To\naddress this, we introduce MakeupQuad, a large-scale, high-quality dataset with\nnon-makeup faces, references, edited results, and textual makeup descriptions.\nBuilding on this, we propose EvoMakeup, a unified training framework that\nmitigates image degradation during multi-stage distillation, enabling iterative\nimprovement of both data and model quality. Although trained solely on\nsynthetic data, EvoMakeup generalizes well and outperforms prior methods on\nreal-world benchmarks. It supports high-fidelity, controllable, multi-task\nmakeup editing -- including full-face and partial reference-based editing, as\nwell as text-driven makeup editing -- within a single model. Experimental\nresults demonstrate that our method achieves superior makeup fidelity and\nidentity preservation, effectively balancing both aspects. Code and dataset\nwill be released upon acceptance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEvoMakeup\u7684\u7edf\u4e00\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u4fdd\u771f\u5ea6\u7684\u9762\u90e8\u5316\u5986\u7f16\u8f91\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u84b8\u998f\u907f\u514d\u56fe\u50cf\u9000\u5316\uff0c\u5e76\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5316\u5986\u7ec6\u8282\u548c\u8eab\u4efd\u4fdd\u6301\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u7f3a\u4e4f\u7ed3\u6784\u5316\u914d\u5bf9\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e86MakeupQuad\u6570\u636e\u96c6\u548cEvoMakeup\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u84b8\u998f\u8fed\u4ee3\u63d0\u5347\u6570\u636e\u4e0e\u6a21\u578b\u8d28\u91cf\u3002", "result": "EvoMakeup\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u9ad8\u4fdd\u771f\u3001\u53ef\u63a7\u7684\u591a\u4efb\u52a1\u5316\u5986\u7f16\u8f91\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5316\u5986\u4fdd\u771f\u5ea6\u548c\u8eab\u4efd\u4fdd\u6301\u4e0a\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.06009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06009", "abs": "https://arxiv.org/abs/2508.06009", "authors": ["Jun Feng", "Zixin Wang", "Zhentao Zhang", "Yue Guo", "Zhihan Zhou", "Xiuyi Chen", "Zhenyang Li", "Dawei Yin"], "title": "MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models", "comment": "29 pages, 16 figures", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in visual mathematical reasoning across various existing\nbenchmarks. However, these benchmarks are predominantly based on clean or\nprocessed multimodal inputs, without incorporating the images provided by\nreal-world Kindergarten through 12th grade (K-12) educational users. To address\nthis gap, we introduce MathReal, a meticulously curated dataset comprising\n2,000 mathematical questions with images captured by handheld mobile devices in\nauthentic scenarios. Each question is an image, containing the question text\nand visual element. We systematically classify the real images into three\nprimary categories: image quality degradation, perspective variation, and\nirrelevant content interference, which are further delineated into 14\nsubcategories. Additionally, MathReal spans five core knowledge and ability\ncategories, which encompass three question types and are divided into three\ndifficulty levels. To comprehensively evaluate the multimodal mathematical\nreasoning abilities of state-of-the-art MLLMs in real-world scenarios, we\ndesign six experimental settings that enable a systematic analysis of their\nperformance. Through extensive experimentation, we find that the\nproblem-solving abilities of existing MLLMs are significantly challenged in\nrealistic educational contexts. Based on this, we conduct a thorough analysis\nof their performance and error patterns, providing insights into their\nrecognition, comprehension, and reasoning capabilities, and outlining\ndirections for future improvements. Data and code:\nhttps://github.com/junfeng0288/MathReal.", "AI": {"tldr": "MathReal\u662f\u4e00\u4e2a\u771f\u5b9e\u6559\u80b2\u573a\u666f\u4e0b\u7684\u6570\u5b66\u95ee\u9898\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u57fa\u4e8e\u5e72\u51c0\u6216\u5904\u7406\u8fc7\u7684\u591a\u6a21\u6001\u8f93\u5165\uff0c\u7f3a\u4e4f\u771f\u5b9eK-12\u6559\u80b2\u7528\u6237\u63d0\u4f9b\u7684\u56fe\u50cf\u6570\u636e\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b2000\u4e2a\u6570\u5b66\u95ee\u9898\u7684MathReal\u6570\u636e\u96c6\uff0c\u5206\u7c7b\u4e3a\u56fe\u50cf\u8d28\u91cf\u3001\u89c6\u89d2\u53d8\u5316\u548c\u65e0\u5173\u5185\u5bb9\u5e72\u6270\u4e09\u7c7b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u516d\u79cd\u5b9e\u9a8c\u8bbe\u7f6e\u3002", "result": "\u73b0\u6709MLLMs\u5728\u771f\u5b9e\u6559\u80b2\u573a\u666f\u4e2d\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u53d7\u5230\u663e\u8457\u6311\u6218\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790MLLMs\u7684\u8868\u73b0\u548c\u9519\u8bef\u6a21\u5f0f\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2508.06014", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06014", "abs": "https://arxiv.org/abs/2508.06014", "authors": ["Minsu Kim", "Subin Jeon", "In Cho", "Mijin Yoo", "Seon Joo Kim"], "title": "ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors", "comment": "10 pages, 6 Figures, ICCV 2025", "summary": "Recent advances in novel view synthesis (NVS) have enabled real-time\nrendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle\nwith artifacts and missing regions when rendering from viewpoints that deviate\nfrom the training trajectory, limiting seamless scene exploration. To address\nthis, we propose a 3DGS-based pipeline that generates additional training views\nto enhance reconstruction. We introduce an information-gain-driven virtual\ncamera placement strategy to maximize scene coverage, followed by video\ndiffusion priors to refine rendered results. Fine-tuning 3D Gaussians with\nthese enhanced views significantly improves reconstruction quality. To evaluate\nour method, we present Wild-Explore, a benchmark designed for challenging scene\nexploration. Experiments demonstrate that our approach outperforms existing\n3DGS-based methods, enabling high-quality, artifact-free rendering from\narbitrary viewpoints.\n  https://exploregs.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u7ba1\u9053\uff0c\u901a\u8fc7\u751f\u6210\u989d\u5916\u8bad\u7ec3\u89c6\u56fe\u548c\u865a\u62df\u76f8\u673a\u653e\u7f6e\u7b56\u7565\uff0c\u63d0\u5347\u65b0\u89c6\u89d2\u5408\u6210\uff08NVS\uff09\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u504f\u79bb\u8bad\u7ec3\u8f68\u8ff9\u7684\u89c6\u89d2\u4e0b\u6e32\u67d3\u65f6\u5b58\u5728\u4f2a\u5f71\u548c\u7f3a\u5931\u533a\u57df\uff0c\u9650\u5236\u4e86\u573a\u666f\u7684\u65e0\u7f1d\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u4fe1\u606f\u589e\u76ca\u9a71\u52a8\u7684\u865a\u62df\u76f8\u673a\u653e\u7f6e\u7b56\u7565\u548c\u89c6\u9891\u6269\u6563\u5148\u9a8c\u6765\u4f18\u5316\u6e32\u67d3\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7\u589e\u5f3a\u89c6\u56fe\u5fae\u8c033D\u9ad8\u65af\u3002", "result": "\u5728Wild-Explore\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u67093DGS\u65b9\u6cd5\uff0c\u652f\u6301\u4efb\u610f\u89c6\u89d2\u7684\u9ad8\u8d28\u91cf\u65e0\u4f2a\u5f71\u6e32\u67d3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e863DGS\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u63a2\u7d22\u3002"}}
{"id": "2508.06021", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06021", "abs": "https://arxiv.org/abs/2508.06021", "authors": ["Utku Ozbulak", "Michaela Cohrs", "Hristo L. Svilenov", "Joris Vankerschaver", "Wesley De Neve"], "title": "Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis", "comment": null, "summary": "Sub-visible particle analysis using flow imaging microscopy combined with\ndeep learning has proven effective in identifying particle types, enabling the\ndistinction of harmless components such as silicone oil from protein particles.\nHowever, the scarcity of available data and severe imbalance between particle\ntypes within datasets remain substantial hurdles when applying multi-class\nclassifiers to such problems, often forcing researchers to rely on less\neffective methods. The aforementioned issue is particularly challenging for\nparticle types that appear unintentionally and in lower numbers, such as\nsilicone oil and air bubbles, as opposed to protein particles, where obtaining\nlarge numbers of images through controlled settings is comparatively\nstraightforward. In this work, we develop a state-of-the-art diffusion model to\naddress data imbalance by generating high-fidelity images that can augment\ntraining datasets, enabling the effective training of multi-class deep neural\nnetworks. We validate this approach by demonstrating that the generated samples\nclosely resemble real particle images in terms of visual quality and structure.\nTo assess the effectiveness of using diffusion-generated images in training\ndatasets, we conduct large-scale experiments on a validation dataset comprising\n500,000 protein particle images and demonstrate that this approach improves\nclassification performance with no negligible downside. Finally, to promote\nopen research and reproducibility, we publicly release both our diffusion\nmodels and the trained multi-class deep neural network classifiers, along with\na straightforward interface for easy integration into future studies, at\nhttps://github.com/utkuozbulak/svp-generative-ai.", "AI": {"tldr": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408\u6d41\u5f0f\u6210\u50cf\u663e\u5fae\u955c\u8fdb\u884c\u4e9a\u53ef\u89c1\u9897\u7c92\u5206\u6790\uff0c\u6709\u6548\u533a\u5206\u65e0\u5bb3\u6210\u5206\uff08\u5982\u7845\u6cb9\uff09\u4e0e\u86cb\u767d\u8d28\u9897\u7c92\uff0c\u4f46\u6570\u636e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u9650\u5236\u4e86\u591a\u5206\u7c7b\u5668\u7684\u5e94\u7528\u3002\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u5148\u8fdb\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u589e\u5f3a\u6570\u636e\u96c6\uff0c\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4e9a\u53ef\u89c1\u9897\u7c92\u5206\u6790\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5c24\u5176\u662f\u7845\u6cb9\u548c\u6c14\u6ce1\u7b49\u4f4e\u6570\u91cf\u9897\u7c92\u7c7b\u578b\u3002", "method": "\u5f00\u53d1\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u4fdd\u771f\u56fe\u50cf\uff0c\u7528\u4e8e\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u751f\u6210\u7684\u6837\u672c\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u7ed3\u6784\u4e0a\u4e0e\u771f\u5b9e\u9897\u7c92\u56fe\u50cf\u76f8\u4f3c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5206\u7c7b\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u80fd\u6709\u6548\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u76f8\u5173\u6a21\u578b\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.06032", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06032", "abs": "https://arxiv.org/abs/2508.06032", "authors": ["Kiran Chhatre", "Christopher Peters", "Srikrishna Karanam"], "title": "Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts", "comment": "16 pages, 11 figures", "summary": "Existing methods for human parsing into body parts and clothing often use\nfixed mask categories with broad labels that obscure fine-grained clothing\ntypes. Recent open-vocabulary segmentation approaches leverage pretrained\ntext-to-image (T2I) diffusion model features for strong zero-shot transfer, but\ntypically group entire humans into a single person category, failing to\ndistinguish diverse clothing or detailed body parts. To address this, we\npropose Spectrum, a unified network for part-level pixel parsing (body parts\nand clothing) and instance-level grouping. While diffusion-based\nopen-vocabulary models generalize well across tasks, their internal\nrepresentations are not specialized for detailed human parsing. We observe\nthat, unlike diffusion models with broad representations, image-driven 3D\ntexture generators maintain faithful correspondence to input images, enabling\nstronger representations for parsing diverse clothing and body parts. Spectrum\nintroduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model --\nobtained by fine-tuning a T2I model on 3D human texture maps -- for improved\nalignment with body parts and clothing. From an input image, we extract\nhuman-part internal features via the I2Tx diffusion model and generate\nsemantically valid masks aligned to diverse clothing categories through\nprompt-guided grounding. Once trained, Spectrum produces semantic segmentation\nmaps for every visible body part and clothing category, ignoring standalone\ngarments or irrelevant objects, for any number of humans in the scene. We\nconduct extensive cross-dataset experiments -- separately assessing body parts,\nclothing parts, unseen clothing categories, and full-body masks -- and\ndemonstrate that Spectrum consistently outperforms baseline methods in\nprompt-based segmentation.", "AI": {"tldr": "Spectrum\u662f\u4e00\u4e2a\u7edf\u4e00\u7f51\u7edc\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u7684\u4eba\u4f53\u90e8\u4f4d\u548c\u8863\u7269\u89e3\u6790\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u56fe\u50cf\u5230\u7eb9\u7406\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u66f4\u597d\u7684\u8bed\u4e49\u5206\u5272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4eba\u4f53\u89e3\u6790\u4e2d\u901a\u5e38\u4f7f\u7528\u56fa\u5b9a\u6807\u7b7e\uff0c\u65e0\u6cd5\u533a\u5206\u7ec6\u7c92\u5ea6\u7684\u8863\u7269\u7c7b\u578b\u6216\u8be6\u7ec6\u7684\u8eab\u4f53\u90e8\u4f4d\uff0c\u800c\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u65b9\u6cd5\u867d\u7136\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7ec6\u8282\u7684\u4e13\u95e8\u4f18\u5316\u3002", "method": "Spectrum\u5229\u7528\u6539\u8fdb\u7684\u56fe\u50cf\u5230\u7eb9\u7406\uff08I2Tx\uff09\u6269\u6563\u6a21\u578b\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u63d0\u793a\u5f15\u5bfc\u751f\u6210\u8bed\u4e49\u6709\u6548\u7684\u63a9\u7801\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u89e3\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSpectrum\u5728\u8eab\u4f53\u90e8\u4f4d\u3001\u8863\u7269\u7c7b\u522b\u548c\u672a\u89c1\u8fc7\u7684\u8863\u7269\u5206\u5272\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Spectrum\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u7ec6\u7c92\u5ea6\u89e3\u6790\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u4f53\u548c\u8863\u7269\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2508.06033", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06033", "abs": "https://arxiv.org/abs/2508.06033", "authors": ["Yiming Gong", "Zhen Zhu", "Minjia Zhang"], "title": "InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow", "comment": "ICCV 2025", "summary": "We propose a fast text-guided image editing method called InstantEdit based\non the RectifiedFlow framework, which is structured as a few-step editing\nprocess that preserves critical content while following closely to textual\ninstructions. Our approach leverages the straight sampling trajectories of\nRectifiedFlow by introducing a specialized inversion strategy called PerRFI. To\nmaintain consistent while editable results for RectifiedFlow model, we further\npropose a novel regeneration method, Inversion Latent Injection, which\neffectively reuses latent information obtained during inversion to facilitate\nmore coherent and detailed regeneration. Additionally, we propose a\nDisentangled Prompt Guidance technique to balance editability with detail\npreservation, and integrate a Canny-conditioned ControlNet to incorporate\nstructural cues and suppress artifacts. Evaluation on the PIE image editing\ndataset demonstrates that InstantEdit is not only fast but also achieves better\nqualitative and quantitative results compared to state-of-the-art few-step\nediting methods.", "AI": {"tldr": "InstantEdit\u662f\u4e00\u79cd\u57fa\u4e8eRectifiedFlow\u6846\u67b6\u7684\u5feb\u901f\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7PerRFI\u53cd\u8f6c\u7b56\u7565\u548cInversion Latent Injection\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u4e00\u81f4\u7684\u7f16\u8f91\u6548\u679c\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u5feb\u901f\u4e14\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u7559\u5173\u952e\u5185\u5bb9\u5e76\u7d27\u5bc6\u9075\u5faa\u6587\u672c\u6307\u4ee4\u3002", "method": "\u7ed3\u5408PerRFI\u53cd\u8f6c\u7b56\u7565\u3001Inversion Latent Injection\u518d\u751f\u65b9\u6cd5\u548cDisentangled Prompt Guidance\u6280\u672f\uff0c\u5e76\u96c6\u6210Canny-conditioned ControlNet\u4ee5\u589e\u5f3a\u7ed3\u6784\u63a7\u5236\u3002", "result": "\u5728PIE\u6570\u636e\u96c6\u4e0a\uff0cInstantEdit\u5728\u901f\u5ea6\u548c\u7f16\u8f91\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "InstantEdit\u901a\u8fc7\u521b\u65b0\u7684\u6280\u672f\u7ec4\u5408\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u7f16\u8f91\uff0c\u4e3a\u6587\u672c\u5f15\u5bfc\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.06036", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06036", "abs": "https://arxiv.org/abs/2508.06036", "authors": ["Jun Xie", "Yingjian Zhu", "Feng Chen", "Zhenghao Zhang", "Xiaohui Fan", "Hongzhu Yi", "Xinming Wang", "Chen Yu", "Yue Bi", "Zhaoran Zhao", "Xiongjun Guan", "Zhepeng Wang"], "title": "More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment", "comment": null, "summary": "In this paper, we present our solution for the semi-supervised learning track\n(MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the\nprinciple that \"more is better,\" to construct a robust Mixture of Experts (MoE)\nemotion recognition system. Our approach integrates a diverse range of input\nmodalities as independent experts, including novel signals such as knowledge\nfrom large Vision-Language Models (VLMs) and temporal Action Unit (AU)\ninformation. To effectively utilize unlabeled data, we introduce a\nconsensus-based pseudo-labeling strategy, generating high-quality labels from\nthe agreement between a baseline model and Gemini, which are then used in a\ntwo-stage training paradigm. Finally, we employ a multi-expert voting ensemble\ncombined with a rule-based re-ranking process to correct prediction bias and\nbetter align the outputs with human preferences. Evaluated on the MER2025-SEMI\nchallenge dataset, our method achieves an F1-score of 0.8772 on the test set,\nranking 2nd in the track. Our code is available at\nhttps://github.com/zhuyjan/MER2025-MRAC25.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u6df7\u5408\u4e13\u5bb6\u60c5\u611f\u8bc6\u522b\u7cfb\u7edf\uff0c\u7ed3\u5408\u591a\u79cd\u8f93\u5165\u6a21\u6001\u548c\u4f2a\u6807\u7b7e\u7b56\u7565\uff0c\u6700\u7ec8\u5728MER2025-SEMI\u6311\u6218\u4e2d\u6392\u540d\u7b2c\u4e8c\u3002", "motivation": "\u89e3\u51b3\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u60c5\u611f\u8bc6\u522b\u95ee\u9898\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cd\u6a21\u6001\u548c\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u52a8\u4f5c\u5355\u5143\u4fe1\u606f\uff0c\u4f7f\u7528\u5171\u8bc6\u4f2a\u6807\u7b7e\u7b56\u7565\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u6700\u540e\u901a\u8fc7\u591a\u4e13\u5bb6\u6295\u7968\u548c\u89c4\u5219\u91cd\u6392\u4f18\u5316\u9884\u6d4b\u3002", "result": "\u5728MER2025-SEMI\u6d4b\u8bd5\u96c6\u4e0aF1\u5f97\u5206\u4e3a0.8772\uff0c\u6392\u540d\u7b2c\u4e8c\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u6574\u5408\u4e86\u591a\u79cd\u6a21\u6001\u548c\u534a\u76d1\u7763\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2508.06038", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06038", "abs": "https://arxiv.org/abs/2508.06038", "authors": ["Huanyu Wang", "Jushi Kai", "Haoli Bai", "Lu Hou", "Bo Jiang", "Ziwei He", "Zhouhan Lin"], "title": "Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models", "comment": "12 pages, 4 figures", "summary": "Vision-Language Models (VLMs) typically replace the predefined image\nplaceholder token (<image>) in textual instructions with visual features from\nan image encoder, forming the input to a backbone Large Language Model (LLM).\nHowever, the large number of vision tokens significantly increases the context\nlength, leading to high computational overhead and inference latency. While\nprevious efforts mitigate this by selecting only important visual features or\nleveraging learnable queries to reduce token count, they often compromise\nperformance or introduce substantial extra costs. In response, we propose\nFourier-VLM, a simple yet efficient method that compresses visual\nrepresentations in the frequency domain. Our approach is motivated by the\nobservation that vision features output from the vision encoder exhibit\nconcentrated energy in low-frequency components. Leveraging this, we apply a\nlow-pass filter to the vision features using a two-dimentional Discrete Cosine\nTransform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier\nTransform (FFT) operator with a time complexity of $\\mathcal{O}(n\\log n)$,\nminimizing the extra computational cost while introducing no additional\nparameters. Extensive experiments across various image-based benchmarks\ndemonstrate that Fourier-VLM achieves competitive performance with strong\ngeneralizability across both LLaVA and Qwen-VL architectures. Crucially, it\nreduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%\ncompared to LLaVA-v1.5, highlighting the superior efficiency and practicality.", "AI": {"tldr": "Fourier-VLM\u901a\u8fc7\u9891\u57df\u538b\u7f29\u89c6\u89c9\u8868\u793a\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u5927\u91cf\u89c6\u89c9\u6807\u8bb0\u589e\u52a0\u4e86\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u5f00\u9500\u548c\u63a8\u7406\u5ef6\u8fdf\u3002", "method": "\u5229\u7528\u89c6\u89c9\u7279\u5f81\u5728\u4f4e\u9891\u5206\u91cf\u96c6\u4e2d\u7684\u7279\u6027\uff0c\u901a\u8fc7\u4e8c\u7ef4\u79bb\u6563\u4f59\u5f26\u53d8\u6362\uff08DCT\uff09\u5e94\u7528\u4f4e\u901a\u6ee4\u6ce2\u5668\u538b\u7f29\u89c6\u89c9\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u7406FLOPs\u51cf\u5c1183.8%\uff0c\u751f\u6210\u901f\u5ea6\u63d0\u534731.2%\u3002", "conclusion": "Fourier-VLM\u5728\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u67b6\u6784\u3002"}}
{"id": "2508.06044", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06044", "abs": "https://arxiv.org/abs/2508.06044", "authors": ["Huimin Wu", "Xiaojian Ma", "Haozhe Zhao", "Yanpeng Zhao", "Qing Li"], "title": "NEP: Autoregressive Image Editing via Next Editing Token Prediction", "comment": "The project page is: https://nep-bigai.github.io/", "summary": "Text-guided image editing involves modifying a source image based on a\nlanguage instruction and, typically, requires changes to only small local\nregions. However, existing approaches generate the entire target image rather\nthan selectively regenerate only the intended editing areas. This results in\n(1) unnecessary computational costs and (2) a bias toward reconstructing\nnon-editing regions, which compromises the quality of the intended edits. To\nresolve these limitations, we propose to formulate image editing as Next\nEditing-token Prediction (NEP) based on autoregressive image generation, where\nonly regions that need to be edited are regenerated, thus avoiding unintended\nmodification to the non-editing areas. To enable any-region editing, we propose\nto pre-train an any-order autoregressive text-to-image (T2I) model. Once\ntrained, it is capable of zero-shot image editing and can be easily adapted to\nNEP for image editing, which achieves a new state-of-the-art on widely used\nimage editing benchmarks. Moreover, our model naturally supports test-time\nscaling (TTS) through iteratively refining its generation in a zero-shot\nmanner. The project page is: https://nep-bigai.github.io/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u7684Next Editing-token Prediction\uff08NEP\uff09\u65b9\u6cd5\uff0c\u4ec5\u91cd\u65b0\u751f\u6210\u9700\u8981\u7f16\u8f91\u7684\u533a\u57df\uff0c\u907f\u514d\u4e86\u5bf9\u975e\u7f16\u8f91\u533a\u57df\u7684\u4e0d\u5fc5\u8981\u4fee\u6539\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u6574\u4e2a\u76ee\u6807\u56fe\u50cf\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u7f16\u8f91\u8d28\u91cf\u53d7\u9650\u3002", "method": "\u901a\u8fc7\u9884\u8bad\u7ec3\u4efb\u610f\u987a\u5e8f\u81ea\u56de\u5f52\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u56fe\u50cf\u7f16\u8f91\uff0c\u5e76\u9002\u5e94NEP\u65b9\u6cd5\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u56fe\u50cf\u7f16\u8f91\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u6700\u4f18\u6027\u80fd\uff0c\u652f\u6301\u96f6\u6837\u672c\u6d4b\u8bd5\u65f6\u7f29\u653e\uff08TTS\uff09\u3002", "conclusion": "NEP\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u7f16\u8f91\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u652f\u6301\u7075\u6d3b\u7684\u533a\u57df\u7f16\u8f91\u548c\u8fed\u4ee3\u4f18\u5316\u3002"}}
{"id": "2508.06072", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06072", "abs": "https://arxiv.org/abs/2508.06072", "authors": ["Zijian Chen", "Lirong Deng", "Zhengyu Chen", "Kaiwei Zhang", "Qi Jia", "Yuan Tian", "Yucheng Zhu", "Guangtao Zhai"], "title": "Can Large Models Fool the Eye? A New Turing Test for Biological Animation", "comment": "24 pages, 10 figures", "summary": "Evaluating the abilities of large models and manifesting their gaps are\nchallenging. Current benchmarks adopt either ground-truth-based score-form\nevaluation on static datasets or indistinct textual chatbot-style human\npreferences collection, which may not provide users with immediate, intuitive,\nand perceptible feedback on performance differences. In this paper, we\nintroduce BioMotion Arena, a novel framework for evaluating large language\nmodels (LLMs) and multimodal large language models (MLLMs) via visual\nanimation. Our methodology draws inspiration from the inherent visual\nperception of motion patterns characteristic of living organisms that utilizes\npoint-light source imaging to amplify the performance discrepancies between\nmodels. Specifically, we employ a pairwise comparison evaluation and collect\nmore than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion\nvariants. Data analyses show that the crowd-sourced human votes are in good\nagreement with those of expert raters, demonstrating the superiority of our\nBioMotion Arena in offering discriminative feedback. We also find that over\n90\\% of evaluated models, including the cutting-edge open-source InternVL3 and\nproprietary Claude-4 series, fail to produce fundamental humanoid point-light\ngroups, much less smooth and biologically plausible motions. This enables\nBioMotion Arena to serve as a challenging benchmark for performance\nvisualization and a flexible evaluation framework without restrictions on\nground-truth.", "AI": {"tldr": "BioMotion Arena\u901a\u8fc7\u89c6\u89c9\u52a8\u753b\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5229\u7528\u70b9\u5149\u6e90\u6210\u50cf\u653e\u5927\u6a21\u578b\u6027\u80fd\u5dee\u5f02\uff0c\u63d0\u4f9b\u76f4\u89c2\u53cd\u9988\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u63d0\u4f9b\u76f4\u89c2\u3001\u5373\u65f6\u7684\u6027\u80fd\u5dee\u5f02\u53cd\u9988\uff0cBioMotion Arena\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u6210\u5bf9\u6bd4\u8f83\u8bc4\u4f30\uff0c\u6536\u96c645k+\u6295\u7968\uff0c\u5206\u679053\u4e2a\u4e3b\u6d41LLMs\u548cMLLMs\u572890\u79cd\u751f\u7269\u8fd0\u52a8\u53d8\u4f53\u4e0a\u7684\u8868\u73b0\u3002", "result": "90%\u4ee5\u4e0a\u6a21\u578b\uff08\u5305\u62ec\u524d\u6cbf\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\uff09\u65e0\u6cd5\u751f\u6210\u57fa\u672c\u4eba\u5f62\u70b9\u5149\u6e90\u7ec4\u6216\u6d41\u7545\u751f\u7269\u8fd0\u52a8\u3002", "conclusion": "BioMotion Arena\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6027\u80fd\u53ef\u89c6\u5316\u57fa\u51c6\uff0c\u65e0\u9700\u4f9d\u8d56\u771f\u5b9e\u6570\u636e\u3002"}}
{"id": "2508.06051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06051", "abs": "https://arxiv.org/abs/2508.06051", "authors": ["Linhan Cao", "Wei Sun", "Weixia Zhang", "Xiangyang Zhu", "Jun Jia", "Kaiwei Zhang", "Dandan Zhu", "Guangtao Zhai", "Xiongkuo Min"], "title": "VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning", "comment": null, "summary": "Video quality assessment (VQA) aims to objectively quantify perceptual\nquality degradation in alignment with human visual perception. Despite recent\nadvances, existing VQA models still suffer from two critical limitations:\n\\textit{poor generalization to out-of-distribution (OOD) videos} and\n\\textit{limited explainability}, which restrict their applicability in\nreal-world scenarios. To address these challenges, we propose\n\\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large\nmultimodal models (LMMs) with reinforcement learning to jointly model video\nquality understanding and scoring, emulating human perceptual decision-making.\nSpecifically, we adopt group relative policy optimization (GRPO), a rule-guided\nreinforcement learning algorithm that enables reasoning over video quality\nunder score-level supervision, and introduce three VQA-specific rewards: (1) a\n\\textbf{bell-shaped regression reward} that increases rapidly as the prediction\nerror decreases and becomes progressively less sensitive near the ground truth;\n(2) a \\textbf{pairwise ranking reward} that guides the model to correctly\ndetermine the relative quality between video pairs; and (3) a \\textbf{temporal\nconsistency reward} that encourages the model to prefer temporally coherent\nvideos over their perturbed counterparts. Extensive experiments demonstrate\nthat VQAThinker achieves state-of-the-art performance on both in-domain and OOD\nVQA benchmarks, showing strong generalization for video quality scoring.\nFurthermore, evaluations on video quality understanding tasks validate its\nsuperiority in distortion attribution and quality description compared to\nexisting explainable VQA models and LMMs. These findings demonstrate that\nreinforcement learning offers an effective pathway toward building\ngeneralizable and explainable VQA models solely with score-level supervision.", "AI": {"tldr": "VQAThinker\u662f\u4e00\u4e2a\u57fa\u4e8e\u63a8\u7406\u7684\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7684\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709VQA\u6a21\u578b\u5728\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e09\u79cdVQA\u7279\u5b9a\u5956\u52b1\uff08\u949f\u5f62\u56de\u5f52\u3001\u6210\u5bf9\u6392\u5e8f\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff09\u6765\u6a21\u62df\u4eba\u7c7b\u611f\u77e5\u51b3\u7b56\u3002", "result": "\u5728\u57df\u5185\u548cOOD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u540c\u65f6\u5728\u8d28\u91cf\u7406\u89e3\u548c\u63cf\u8ff0\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u4e3a\u4ec5\u4f9d\u8d56\u5206\u6570\u76d1\u7763\u7684\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027VQA\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.06076", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06076", "abs": "https://arxiv.org/abs/2508.06076", "authors": ["Michael Wehrli", "Alicia Durrer", "Paul Friedrich", "Sidaty El Hadramy", "Edwin Li", "Luana Brahaj", "Carol C. Hasler", "Philippe C. Cattin"], "title": "Towards MR-Based Trochleoplasty Planning", "comment": "Accepted at MICCAI COLAS Workshop 2025. Code:\n  https://wehrlimi.github.io/sr-3d-planning/", "summary": "To treat Trochlear Dysplasia (TD), current approaches rely mainly on\nlow-resolution clinical Magnetic Resonance (MR) scans and surgical intuition.\nThe surgeries are planned based on surgeons experience, have limited adoption\nof minimally invasive techniques, and lead to inconsistent outcomes. We propose\na pipeline that generates super-resolved, patient-specific 3D pseudo-healthy\ntarget morphologies from conventional clinical MR scans. First, we compute an\nisotropic super-resolved MR volume using an Implicit Neural Representation\n(INR). Next, we segment femur, tibia, patella, and fibula with a multi-label\ncustom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to\ngenerate pseudo-healthy target morphologies of the trochlear region. In\ncontrast to prior work producing pseudo-healthy low-resolution 3D MR images,\nour approach enables the generation of sub-millimeter resolved 3D shapes\ncompatible for pre- and intraoperative use. These can serve as preoperative\nblueprints for reshaping the femoral groove while preserving the native patella\narticulation. Furthermore, and in contrast to other work, we do not require a\nCT for our pipeline - reducing the amount of radiation. We evaluated our\napproach on 25 TD patients and could show that our target morphologies\nsignificantly improve the sulcus angle (SA) and trochlear groove depth (TGD).\nThe code and interactive visualization are available at\nhttps://wehrlimi.github.io/sr-3d-planning/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u4f4e\u5206\u8fa8\u7387MR\u626b\u63cf\u751f\u6210\u9ad8\u5206\u8fa8\u73873D\u4f2a\u5065\u5eb7\u76ee\u6807\u5f62\u6001\u7684\u6d41\u7a0b\uff0c\u7528\u4e8e\u6cbb\u7597\u6ed1\u8f66\u53d1\u80b2\u4e0d\u826f\uff0c\u65e0\u9700CT\u626b\u63cf\uff0c\u51cf\u5c11\u8f90\u5c04\u3002", "motivation": "\u5f53\u524d\u6cbb\u7597\u6ed1\u8f66\u53d1\u80b2\u4e0d\u826f\u7684\u65b9\u6cd5\u4f9d\u8d56\u4f4e\u5206\u8fa8\u7387MR\u626b\u63cf\u548c\u5916\u79d1\u533b\u751f\u7ecf\u9a8c\uff0c\u624b\u672f\u6548\u679c\u4e0d\u4e00\u81f4\u4e14\u5fae\u521b\u6280\u672f\u5e94\u7528\u6709\u9650\u3002", "method": "\u6d41\u7a0b\u5305\u62ec\uff1a1\uff09\u4f7f\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u751f\u6210\u5404\u5411\u540c\u6027\u8d85\u5206\u8fa8\u7387MR\u4f53\u79ef\uff1b2\uff09\u591a\u6807\u7b7e\u7f51\u7edc\u5206\u5272\u9aa8\u9abc\uff1b3\uff09\u5c0f\u6ce2\u6269\u6563\u6a21\u578b\u751f\u6210\u4f2a\u5065\u5eb7\u76ee\u6807\u5f62\u6001\u3002", "result": "\u572825\u540d\u60a3\u8005\u4e2d\u9a8c\u8bc1\uff0c\u663e\u8457\u6539\u5584\u4e86\u6ed1\u8f66\u89d2\u5ea6\u548c\u6ed1\u8f66\u6c9f\u6df1\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u672f\u524d\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u5206\u8fa8\u73873D\u5f62\u6001\uff0c\u51cf\u5c11\u8f90\u5c04\u4e14\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2508.06055", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.06055", "abs": "https://arxiv.org/abs/2508.06055", "authors": ["Wonjung Park", "Suhyun Ahn", "Jinah Park"], "title": "LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing", "comment": null, "summary": "Lateral ventricle (LV) shape analysis holds promise as a biomarker for\nneurological diseases; however, challenges remain due to substantial shape\nvariability across individuals and segmentation difficulties arising from\nlimited MRI resolution. We introduce LV-Net, a novel framework for producing\nindividualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint\nLV-hippocampus template mesh. By incorporating anatomical relationships\nembedded within the joint template, LV-Net reduces boundary segmentation\nartifacts and improves reconstruction robustness. In addition, by classifying\nthe vertices of the template mesh based on their anatomical adjacency, our\nmethod enhances point correspondence across subjects, leading to more accurate\nLV shape statistics. We demonstrate that LV-Net achieves superior\nreconstruction accuracy, even in the presence of segmentation imperfections,\nand delivers more reliable shape descriptors across diverse datasets. Finally,\nwe apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that\nshow significantly associations with the disease relative to cognitively normal\ncontrols. The codes for LV shape modeling are available at\nhttps://github.com/PWonjung/LV_Shape_Modeling.", "AI": {"tldr": "LV-Net\u662f\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u53d8\u5f62\u8054\u5408LV-\u6d77\u9a6c\u6a21\u677f\u7f51\u683c\u4ece\u8111MRI\u751f\u6210\u4e2a\u6027\u53163D LV\u7f51\u683c\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u7cbe\u5ea6\u548c\u5f62\u72b6\u7edf\u8ba1\u51c6\u786e\u6027\u3002", "motivation": "\u4fa7\u8111\u5ba4\uff08LV\uff09\u5f62\u72b6\u5206\u6790\u4f5c\u4e3a\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u7684\u751f\u7269\u6807\u5fd7\u7269\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u4e2a\u4f53\u95f4\u5f62\u72b6\u5dee\u5f02\u5927\u548cMRI\u5206\u8fa8\u7387\u9650\u5236\u5bfc\u81f4\u7684\u5206\u5272\u56f0\u96be\uff0c\u4ecd\u5b58\u5728\u6311\u6218\u3002", "method": "LV-Net\u901a\u8fc7\u7ed3\u5408\u89e3\u5256\u5b66\u5173\u7cfb\u7684\u8054\u5408\u6a21\u677f\uff0c\u51cf\u5c11\u5206\u5272\u4f2a\u5f71\u5e76\u589e\u5f3a\u91cd\u5efa\u9c81\u68d2\u6027\uff1b\u901a\u8fc7\u5206\u7c7b\u6a21\u677f\u7f51\u683c\u9876\u70b9\u4ee5\u6539\u5584\u70b9\u5bf9\u5e94\u6027\u3002", "result": "LV-Net\u5728\u5206\u5272\u4e0d\u5b8c\u7f8e\u60c5\u51b5\u4e0b\u4ecd\u5b9e\u73b0\u9ad8\u91cd\u5efa\u7cbe\u5ea6\uff0c\u5e76\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u5206\u6790\u4e2d\u8bc6\u522b\u51fa\u4e0e\u75be\u75c5\u663e\u8457\u76f8\u5173\u7684LV\u5b50\u533a\u57df\u3002", "conclusion": "LV-Net\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u7684LV\u5f62\u72b6\u5206\u6790\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u75be\u75c5\u7814\u7a76\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.06107", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06107", "abs": "https://arxiv.org/abs/2508.06107", "authors": ["Shree Mitra", "Ritabrata Chakraborty", "Nilkanta Sahu"], "title": "Mask & Match: Learning to Recognize Handwritten Math with Self-Supervised Attention", "comment": null, "summary": "Recognizing handwritten mathematical expressions (HMER) is a challenging task\ndue to the inherent two-dimensional structure, varying symbol scales, and\ncomplex spatial relationships among symbols. In this paper, we present a\nself-supervised learning (SSL) framework for HMER that eliminates the need for\nexpensive labeled data. Our approach begins by pretraining an image encoder\nusing a combination of global and local contrastive loss, enabling the model to\nlearn both holistic and fine-grained representations. A key contribution of\nthis work is a novel self-supervised attention network, which is trained using\na progressive spatial masking strategy. This attention mechanism is designed to\nlearn semantically meaningful focus regions, such as operators, exponents, and\nnested mathematical notation, without requiring any supervision. The\nprogressive masking curriculum encourages the network to become increasingly\nrobust to missing or occluded visual information, ultimately improving\nstructural understanding. Our complete pipeline consists of (1) self-supervised\npretraining of the encoder, (2) self-supervised attention learning, and (3)\nsupervised fine-tuning with a transformer decoder to generate LATEX sequences.\nExtensive experiments on CROHME benchmarks demonstrate that our method\noutperforms existing SSL and fully supervised baselines, validating the\neffectiveness of our progressive attention mechanism in enhancing HMER\nperformance. Our codebase can be found here.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u624b\u5199\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\uff08HMER\uff09\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u5bf9\u6bd4\u635f\u5931\u9884\u8bad\u7ec3\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u5e76\u7ed3\u5408\u6e10\u8fdb\u7a7a\u95f4\u63a9\u7801\u7b56\u7565\u8bad\u7ec3\u81ea\u76d1\u7763\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u624b\u5199\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\u56e0\u4e8c\u7ef4\u7ed3\u6784\u3001\u7b26\u53f7\u5c3a\u5ea6\u53d8\u5316\u548c\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u6807\u6ce8\u6570\u636e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "1. \u4f7f\u7528\u5168\u5c40\u548c\u5c40\u90e8\u5bf9\u6bd4\u635f\u5931\u9884\u8bad\u7ec3\u56fe\u50cf\u7f16\u7801\u5668\uff1b2. \u901a\u8fc7\u6e10\u8fdb\u7a7a\u95f4\u63a9\u7801\u7b56\u7565\u8bad\u7ec3\u81ea\u76d1\u7763\u6ce8\u610f\u529b\u7f51\u7edc\uff1b3. \u7528Transformer\u89e3\u7801\u5668\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u751f\u6210LaTeX\u5e8f\u5217\u3002", "result": "\u5728CROHME\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u81ea\u76d1\u7763\u548c\u5168\u76d1\u7763\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86\u6e10\u8fdb\u6ce8\u610f\u529b\u673a\u5236\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u548c\u6e10\u8fdb\u6ce8\u610f\u529b\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86HMER\u6027\u80fd\uff0c\u51cf\u5c11\u4e86\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2508.06057", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06057", "abs": "https://arxiv.org/abs/2508.06057", "authors": ["Mojtaba Valipour", "Kelly Zheng", "James Lowman", "Spencer Szabados", "Mike Gartner", "Bobby Braswell"], "title": "AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?", "comment": "Accepted in IGARSS 2025!", "summary": "Artificial General Intelligence (AGI) is closer than ever to becoming a\nreality, sparking widespread enthusiasm in the research community to collect\nand work with various modalities, including text, image, video, and audio.\nDespite recent efforts, satellite spectral imagery, as an additional modality,\nhas yet to receive the attention it deserves. This area presents unique\nchallenges, but also holds great promise in advancing the capabilities of AGI\nin understanding the natural world. In this paper, we argue why Earth\nObservation data is useful for an intelligent model, and then we review\nexisting benchmarks and highlight their limitations in evaluating the\ngeneralization ability of foundation models in this domain. This paper\nemphasizes the need for a more comprehensive benchmark to evaluate earth\nobservation models. To facilitate this, we propose a comprehensive set of tasks\nthat a benchmark should encompass to effectively assess a model's ability to\nunderstand and interact with Earth observation data.", "AI": {"tldr": "\u8bba\u6587\u547c\u5401\u5173\u6ce8\u536b\u661f\u5149\u8c31\u56fe\u50cf\u4f5c\u4e3aAGI\u7684\u65b0\u6a21\u6001\uff0c\u63d0\u51fa\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\uff0c\u5e76\u5efa\u8bae\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u4efb\u52a1\u3002", "motivation": "\u536b\u661f\u5149\u8c31\u56fe\u50cf\u4f5c\u4e3aAGI\u7684\u65b0\u6a21\u6001\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u5176\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5206\u6790\u73b0\u6709\u57fa\u51c6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u5957\u5168\u9762\u7684\u8bc4\u4f30\u4efb\u52a1\u3002", "result": "\u5f3a\u8c03\u4e86\u5730\u7403\u89c2\u6d4b\u6570\u636e\u5bf9\u667a\u80fd\u6a21\u578b\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u57fa\u51c6\u7684\u4e0d\u8db3\u3002", "conclusion": "\u9700\u8981\u66f4\u5168\u9762\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u5730\u7403\u89c2\u6d4b\u6a21\u578b\uff0c\u4ee5\u63a8\u52a8AGI\u5728\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.06109", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06109", "abs": "https://arxiv.org/abs/2508.06109", "authors": ["Zhibo Zhu", "Renyu Huang", "Lei He"], "title": "FMCE-Net++: Feature Map Convergence Evaluation and Training", "comment": null, "summary": "Deep Neural Networks (DNNs) face interpretability challenges due to their\nopaque internal representations. While Feature Map Convergence Evaluation\n(FMCE) quantifies module-level convergence via Feature Map Convergence Scores\n(FMCS), it lacks experimental validation and closed-loop integration. To\naddress this limitation, we propose FMCE-Net++, a novel training framework that\nintegrates a pretrained, frozen FMCE-Net as an auxiliary head. This module\ngenerates FMCS predictions, which, combined with task labels, jointly supervise\nbackbone optimization through a Representation Auxiliary Loss. The RAL\ndynamically balances the primary classification loss and feature convergence\noptimization via a tunable \\Representation Abstraction Factor. Extensive\nexperiments conducted on MNIST, CIFAR-10, FashionMNIST, and CIFAR-100\ndemonstrate that FMCE-Net++ consistently enhances model performance without\narchitectural modifications or additional data. Key experimental outcomes\ninclude accuracy gains of $+1.16$ pp (ResNet-50/CIFAR-10) and $+1.08$ pp\n(ShuffleNet v2/CIFAR-100), validating that FMCE-Net++ can effectively elevate\nstate-of-the-art performance ceilings.", "AI": {"tldr": "FMCE-Net++\u662f\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u9884\u8bad\u7ec3\u7684FMCE-Net\u4f5c\u4e3a\u8f85\u52a9\u5934\uff0c\u52a8\u6001\u5e73\u8861\u5206\u7c7b\u635f\u5931\u548c\u7279\u5f81\u6536\u655b\u4f18\u5316\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u5185\u90e8\u8868\u793a\u4e0d\u900f\u660e\u7684\u95ee\u9898\uff0cFMCE\u65b9\u6cd5\u7f3a\u4e4f\u5b9e\u9a8c\u9a8c\u8bc1\u548c\u95ed\u73af\u96c6\u6210\u3002", "method": "\u63d0\u51faFMCE-Net++\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684FMCE-Net\u751f\u6210\u7279\u5f81\u56fe\u6536\u655b\u5206\u6570\uff08FMCS\uff09\uff0c\u901a\u8fc7\u8868\u793a\u8f85\u52a9\u635f\u5931\uff08RAL\uff09\u52a8\u6001\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cFMCE-Net++\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5982ResNet-50\u5728CIFAR-10\u4e0a\u51c6\u786e\u7387\u63d0\u53471.16\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "FMCE-Net++\u80fd\u6709\u6548\u63d0\u5347\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\u4e0a\u9650\uff0c\u65e0\u9700\u4fee\u6539\u67b6\u6784\u6216\u989d\u5916\u6570\u636e\u3002"}}
{"id": "2508.06058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06058", "abs": "https://arxiv.org/abs/2508.06058", "authors": ["Shiyang Zhou", "Haijin Zeng", "Yunfan Lu", "Yongyong Chen", "Jie Liu", "Jingyong Su"], "title": "Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention", "comment": null, "summary": "Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera\ncapture brightness changes as asynchronous \"events\" instead of frames, offering\nadvanced application on mobile photography. However, challenges arise from\ncombining a Quad Bayer Color Filter Array (CFA) sensor with event pixels\nlacking color information, resulting in aliasing and artifacts on the\ndemosaicing process before downstream application. Current methods struggle to\naddress these issues, especially on resource-limited mobile devices. In\nresponse, we introduce \\textbf{TSANet}, a lightweight \\textbf{T}wo-stage\nnetwork via \\textbf{S}tate space augmented cross-\\textbf{A}ttention, which can\nhandle event pixels inpainting and demosaicing separately, leveraging the\nbenefits of dividing complex tasks into manageable subtasks. Furthermore, we\nintroduce a lightweight Cross-Swin State Block that uniquely utilizes\npositional prior for demosaicing and enhances global dependencies through the\nstate space model with linear complexity. In summary, TSANet demonstrates\nexcellent demosaicing performance on both simulated and real data of HybridEVS\nwhile maintaining a lightweight model, averaging better results than the\nprevious state-of-the-art method DemosaicFormer across seven diverse datasets\nin both PSNR and SSIM, while respectively reducing parameter and computation\ncosts by $1.86\\times$ and $3.29\\times$. Our approach presents new possibilities\nfor efficient image demosaicing on mobile devices. Code is available in the\nsupplementary materials.", "AI": {"tldr": "TSANet\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u4e24\u9636\u6bb5\u7f51\u7edc\uff0c\u901a\u8fc7\u72b6\u6001\u7a7a\u95f4\u589e\u5f3a\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u5206\u522b\u5904\u7406\u4e8b\u4ef6\u50cf\u7d20\u4fee\u590d\u548c\u53bb\u9a6c\u8d5b\u514b\uff0c\u663e\u8457\u63d0\u5347\u4e86HybridEVS\u76f8\u673a\u7684\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "HybridEVS\u76f8\u673a\u7ed3\u5408Quad Bayer CFA\u4f20\u611f\u5668\u548c\u4e8b\u4ef6\u50cf\u7d20\u65f6\uff0c\u7f3a\u4e4f\u989c\u8272\u4fe1\u606f\u5bfc\u81f4\u53bb\u9a6c\u8d5b\u514b\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u4f2a\u5f71\u548c\u6df7\u53e0\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u8d44\u6e90\u6709\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u4e0a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "TSANet\u91c7\u7528\u4e24\u9636\u6bb5\u7f51\u7edc\u8bbe\u8ba1\uff0c\u5206\u522b\u5904\u7406\u4e8b\u4ef6\u50cf\u7d20\u4fee\u590d\u548c\u53bb\u9a6c\u8d5b\u514b\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u7684Cross-Swin State Block\uff0c\u5229\u7528\u4f4d\u7f6e\u5148\u9a8c\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u589e\u5f3a\u5168\u5c40\u4f9d\u8d56\u3002", "result": "TSANet\u5728\u6a21\u62df\u548c\u771f\u5b9eHybridEVS\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u8272\uff0cPSNR\u548cSSIM\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5DemosaicFormer\uff0c\u540c\u65f6\u53c2\u6570\u548c\u8ba1\u7b97\u6210\u672c\u5206\u522b\u964d\u4f4e\u4e861.86\u500d\u548c3.29\u500d\u3002", "conclusion": "TSANet\u4e3a\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u56fe\u50cf\u53bb\u9a6c\u8d5b\u514b\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.06136", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06136", "abs": "https://arxiv.org/abs/2508.06136", "authors": ["YoungChan Choi", "HengFei Wang", "YiHua Cheng", "Boeun Kim", "Hyung Jin Chang", "YoungGeun Choi", "Sang-Il Choi"], "title": "Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation", "comment": "9 pages, 5 figures, ACM Multimeida 2025 accepted", "summary": "We propose a novel 3D gaze redirection framework that leverages an explicit\n3D eyeball structure. Existing gaze redirection methods are typically based on\nneural radiance fields, which employ implicit neural representations via volume\nrendering. Unlike these NeRF-based approaches, where the rotation and\ntranslation of 3D representations are not explicitly modeled, we introduce a\ndedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian\nSplatting (3DGS). Our method generates photorealistic images that faithfully\nreproduce the desired gaze direction by explicitly rotating and translating the\n3D eyeball structure. In addition, we propose an adaptive deformation module\nthat enables the replication of subtle muscle movements around the eyes.\nThrough experiments conducted on the ETH-XGaze dataset, we demonstrate that our\nframework is capable of generating diverse novel gaze images, achieving\nsuperior image quality and gaze estimation accuracy compared to previous\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u773c\u7403\u7ed3\u6784\u7684\u89c6\u7ebf\u91cd\u5b9a\u5411\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u773c\u7403\u65cb\u8f6c\u548c\u5e73\u79fb\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u53d8\u5f62\u6a21\u5757\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u51c6\u786e\u7684\u89c6\u7ebf\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u57fa\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\uff0c\u7f3a\u4e4f\u5bf9\u773c\u7403\u65cb\u8f6c\u548c\u5e73\u79fb\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u5bfc\u81f4\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u53d7\u9650\u3002", "method": "\u91c7\u75283D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u663e\u5f0f\u8868\u793a\u773c\u7403\u7ed3\u6784\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u53d8\u5f62\u6a21\u5757\u6a21\u62df\u773c\u90e8\u808c\u8089\u7ec6\u5fae\u8fd0\u52a8\u3002", "result": "\u5728ETH-XGaze\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u9ad8\u4e14\u89c6\u7ebf\u4f30\u8ba1\u51c6\u786e\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u663e\u5f0f3D\u773c\u7403\u7ed3\u6784\u7ed3\u5408\u81ea\u9002\u5e94\u53d8\u5f62\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u7ebf\u91cd\u5b9a\u5411\u7684\u751f\u6210\u6548\u679c\u3002"}}
{"id": "2508.06063", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06063", "abs": "https://arxiv.org/abs/2508.06063", "authors": ["Chao Hao", "Zitong Yu", "Xin Liu", "Yuhao Wang", "Weicheng Xie", "Jingang Shi", "Huanjing Yue", "Jingyu Yang"], "title": "Distribution-Specific Learning for Joint Salient and Camouflaged Object Detection", "comment": null, "summary": "Salient object detection (SOD) and camouflaged object detection (COD) are two\nclosely related but distinct computer vision tasks. Although both are\nclass-agnostic segmentation tasks that map from RGB space to binary space, the\nformer aims to identify the most salient objects in the image, while the latter\nfocuses on detecting perfectly camouflaged objects that blend into the\nbackground in the image. These two tasks exhibit strong contradictory\nattributes. Previous works have mostly believed that joint learning of these\ntwo tasks would confuse the network, reducing its performance on both tasks.\nHowever, here we present an opposite perspective: with the correct approach to\nlearning, the network can simultaneously possess the capability to find both\nsalient and camouflaged objects, allowing both tasks to benefit from joint\nlearning. We propose SCJoint, a joint learning scheme for SOD and COD tasks,\nassuming that the decoding processes of SOD and COD have different distribution\ncharacteristics. The key to our method is to learn the respective means and\nvariances of the decoding processes for both tasks by inserting a minimal\namount of task-specific learnable parameters within a fully shared network\nstructure, thereby decoupling the contradictory attributes of the two tasks at\na minimal cost. Furthermore, we propose a saliency-based sampling strategy\n(SBSS) to sample the training set of the SOD task to balance the training set\nsizes of the two tasks. In addition, SBSS improves the training set quality and\nshortens the training time. Based on the proposed SCJoint and SBSS, we train a\npowerful generalist network, named JoNet, which has the ability to\nsimultaneously capture both ``salient\" and ``camouflaged\". Extensive\nexperiments demonstrate the competitive performance and effectiveness of our\nproposed method. The code is available at https://github.com/linuxsino/JoNet.", "AI": {"tldr": "SCJoint\u662f\u4e00\u79cd\u8054\u5408\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u540c\u65f6\u5904\u7406\u663e\u8457\u76ee\u6807\u68c0\u6d4b\uff08SOD\uff09\u548c\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\uff08COD\uff09\u4efb\u52a1\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u7684\u53c2\u6570\u548c\u5171\u4eab\u7f51\u7edc\u7ed3\u6784\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3aSOD\u548cCOD\u4efb\u52a1\u8054\u5408\u5b66\u4e60\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u4f46\u4f5c\u8005\u63d0\u51fa\u76f8\u53cd\u89c2\u70b9\uff0c\u8ba4\u4e3a\u6b63\u786e\u7684\u65b9\u6cd5\u53ef\u4ee5\u540c\u65f6\u63d0\u5347\u4e24\u8005\u6027\u80fd\u3002", "method": "\u63d0\u51faSCJoint\u65b9\u6848\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u7684\u5b66\u4e60\u53c2\u6570\u548c\u5171\u4eab\u7f51\u7edc\u7ed3\u6784\uff0c\u4ee5\u53ca\u57fa\u4e8e\u663e\u8457\u6027\u7684\u91c7\u6837\u7b56\u7565\uff08SBSS\uff09\u6765\u5e73\u8861\u8bad\u7ec3\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eJoNet\u7f51\u7edc\u5728SOD\u548cCOD\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "SCJoint\u548cSBSS\u7684\u7ed3\u5408\u80fd\u591f\u9ad8\u6548\u5730\u8054\u5408\u5b66\u4e60SOD\u548cCOD\u4efb\u52a1\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2508.06169", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06169", "abs": "https://arxiv.org/abs/2508.06169", "authors": ["Wenpeng Xing", "Jie Chen", "Zaifeng Yang", "Changting Lin", "Jianfeng Dong", "Chaochao Chen", "Xun Zhou", "Meng Han"], "title": "UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting", "comment": null, "summary": "Underwater 3D scene reconstruction faces severe challenges from light\nabsorption, scattering, and turbidity, which degrade geometry and color\nfidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF\nextensions such as SeaThru-NeRF incorporate physics-based models, their MLP\nreliance limits efficiency and spatial resolution in hazy environments. We\nintroduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for\nrobust underwater reconstruction. Key innovations include: (1) a plug-and-play\nlearnable underwater image formation module using voxel-based regression for\nspatially varying attenuation and backscatter; and (2) a Physics-Aware\nUncertainty Pruning (PAUP) branch that adaptively removes noisy floating\nGaussians via uncertainty scoring, ensuring artifact-free geometry. The\npipeline operates in training and rendering stages. During training, noisy\nGaussians are optimized end-to-end with underwater parameters, guided by PAUP\npruning and scattering modeling. In rendering, refined Gaussians produce clean\nUnattenuated Radiance Images (URIs) free from media effects, while learned\nphysics enable realistic Underwater Images (UWIs) with accurate light\ntransport. Experiments on SeaThru-NeRF and UWBundle datasets show superior\nperformance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on\nSeaThru-NeRF, with ~65% reduction in floating artifacts.", "AI": {"tldr": "UW-3DGS\u662f\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u65b0\u578b\u6c34\u4e0b3D\u573a\u666f\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u7269\u7406\u6a21\u578b\u548c\u81ea\u9002\u5e94\u4fee\u526a\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982NeRF\u53ca\u5176\u6269\u5c55\uff09\u5728\u6c34\u4e0b\u73af\u5883\u4e2d\u56e0\u5149\u7ebf\u5438\u6536\u3001\u6563\u5c04\u548c\u6d51\u6d4a\u95ee\u9898\u5bfc\u81f4\u51e0\u4f55\u548c\u989c\u8272\u4fdd\u771f\u5ea6\u4e0b\u964d\uff0c\u4e14\u6548\u7387\u8f83\u4f4e\u3002", "method": "\u63d0\u51faUW-3DGS\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u5b66\u4e60\u7684\u6c34\u4e0b\u6210\u50cf\u6a21\u5757\u548c\u7269\u7406\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u4fee\u526a\uff08PAUP\uff09\u5206\u652f\uff0c\u4f18\u5316\u9ad8\u65af\u5206\u5e03\u5e76\u53bb\u9664\u566a\u58f0\u3002", "result": "\u5728SeaThru-NeRF\u548cUWBundle\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cPSNR\u8fbe27.604\uff0cSSIM\u4e3a0.868\uff0cLPIPS\u4e3a0.104\uff0c\u6d6e\u6e38\u4f2a\u5f71\u51cf\u5c11\u7ea665%\u3002", "conclusion": "UW-3DGS\u901a\u8fc7\u7269\u7406\u5efa\u6a21\u548c\u81ea\u9002\u5e94\u4fee\u526a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b3D\u91cd\u5efa\u7684\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.06170", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06170", "abs": "https://arxiv.org/abs/2508.06170", "authors": ["Ojonugwa Oluwafemi Ejiga Peter", "Akingbola Oluwapemiisin", "Amalahu Chetachi", "Adeniran Opeyemi", "Fahmi Khalifa", "Md Mahmudur Rahman"], "title": "Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation", "comment": null, "summary": "Colonoscopy is a vital tool for the early diagnosis of colorectal cancer,\nwhich is one of the main causes of cancer-related mortality globally; hence, it\nis deemed an essential technique for the prevention and early detection of\ncolorectal cancer. The research introduces a unique multidirectional\narchitectural framework to automate polyp detection within colonoscopy images\nwhile helping resolve limited healthcare dataset sizes and annotation\ncomplexities. The research implements a comprehensive system that delivers\nsynthetic data generation through Stable Diffusion enhancements together with\ndetection and segmentation algorithms. This detection approach combines Faster\nR-CNN for initial object localization while the Segment Anything Model (SAM)\nrefines the segmentation masks. The faster R-CNN detection algorithm achieved a\nrecall of 93.08% combined with a precision of 88.97% and an F1 score of\n90.98%.SAM is then used to generate the image mask. The research evaluated five\nstate-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet,\nand MANet using ResNet34 as a base model. The results demonstrate the superior\nperformance of FPN with the highest scores of PSNR (7.205893) and SSIM\n(0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced\nperformance in IoU (64.20%) and Dice score (77.53%).", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u65b9\u5411\u67b6\u6784\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408Faster R-CNN\u548cSAM\u6a21\u578b\uff0c\u81ea\u52a8\u5316\u7ed3\u80a0\u955c\u56fe\u50cf\u4e2d\u7684\u606f\u8089\u68c0\u6d4b\uff0c\u5e76\u5229\u7528\u5408\u6210\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u6570\u636e\u96c6\u548c\u6807\u6ce8\u95ee\u9898\u3002", "motivation": "\u7ed3\u80a0\u955c\u68c0\u67e5\u662f\u7ed3\u76f4\u80a0\u764c\u65e9\u671f\u8bca\u65ad\u7684\u5173\u952e\u5de5\u5177\uff0c\u4f46\u6570\u636e\u96c6\u6709\u9650\u4e14\u6807\u6ce8\u590d\u6742\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408Faster R-CNN\u8fdb\u884c\u521d\u59cb\u5b9a\u4f4d\uff0cSAM\u7ec6\u5316\u5206\u5272\u63a9\u7801\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e94\u79cd\u5206\u5272\u6a21\u578b\uff08U-Net\u3001PSPNet\u3001FPN\u3001LinkNet\u3001MANet\uff09\u3002", "result": "Faster R-CNN\u53ec\u56de\u738793.08%\uff0c\u7cbe\u786e\u738788.97%\uff1bFPN\u5728PSNR\u548cSSIM\u4e0a\u8868\u73b0\u6700\u4f73\uff0cUNet\u5728\u53ec\u56de\u7387\u4e0a\u9886\u5148\u3002", "conclusion": "\u591a\u65b9\u5411\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u606f\u8089\u68c0\u6d4b\u6027\u80fd\uff0cFPN\u548cUNet\u5728\u4e0d\u540c\u6307\u6807\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2508.06202", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06202", "abs": "https://arxiv.org/abs/2508.06202", "authors": ["Chang Che", "Ziqi Wang", "Pengwan Yang", "Qi Wang", "Hui Ma", "Zenglin Shi"], "title": "LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning", "comment": null, "summary": "Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language\nModels (MLLMs) to incrementally learn new tasks over time. However, this\nprocess is challenged by catastrophic forgetting, where performance on\npreviously learned tasks deteriorates as the model adapts to new ones. A common\napproach to mitigate forgetting is architecture expansion, which introduces\ntask-specific modules to prevent interference. Yet, existing methods often\nexpand entire layers for each task, leading to significant parameter overhead\nand poor scalability. To overcome these issues, we introduce LoRA in LoRA\n(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in\nMLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,\napplies an additional low-rank decomposition to matrix B to minimize\ntask-specific parameters, and incorporates a cosine-regularized stability loss\nto preserve consistency in shared representations over time. Extensive\nexperiments on a diverse CVIT benchmark show that LiLoRA consistently achieves\nsuperior performance in sequential task learning while significantly improving\nparameter efficiency compared to existing approaches.", "AI": {"tldr": "LiLoRA\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u67b6\u6784\u6269\u5c55\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u89c6\u89c9\u6307\u4ee4\u8c03\u6574\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u901a\u8fc7\u5171\u4eabLoRA\u77e9\u9635\u548c\u4f4e\u79e9\u5206\u89e3\u51cf\u5c11\u53c2\u6570\u5f00\u9500\u3002", "motivation": "\u89e3\u51b3\u6301\u7eed\u89c6\u89c9\u6307\u4ee4\u8c03\u6574\u4e2d\u707e\u96be\u6027\u9057\u5fd8\u548c\u53c2\u6570\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u5171\u4eabLoRA\u77e9\u9635A\uff0c\u5bf9\u77e9\u9635B\u8fdb\u884c\u4f4e\u79e9\u5206\u89e3\uff0c\u5e76\u5f15\u5165\u4f59\u5f26\u6b63\u5219\u5316\u7a33\u5b9a\u6027\u635f\u5931\u3002", "result": "\u5728\u591a\u6837\u5316CVIT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53c2\u6570\u6548\u7387\u3002", "conclusion": "LiLoRA\u5728\u6301\u7eed\u4efb\u52a1\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u8d8a\u4e14\u9ad8\u6548\u3002"}}
{"id": "2508.06080", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06080", "abs": "https://arxiv.org/abs/2508.06080", "authors": ["Bin Xia", "Jiyang Liu", "Yuechen Zhang", "Bohao Peng", "Ruihang Chu", "Yitong Wang", "Xinglong Wu", "Bei Yu", "Jiaya Jia"], "title": "DreamVE: Unified Instruction-based Image and Video Editing", "comment": null, "summary": "Instruction-based editing holds vast potential due to its simple and\nefficient interactive editing format. However, instruction-based editing,\nparticularly for video, has been constrained by limited training data,\nhindering its practical application. To this end, we introduce DreamVE, a\nunified model for instruction-based image and video editing. Specifically, We\npropose a two-stage training strategy: first image editing, then video editing.\nThis offers two main benefits: (1) Image data scales more easily, and models\nare more efficient to train, providing useful priors for faster and better\nvideo editing training. (2) Unifying image and video generation is natural and\naligns with current trends. Moreover, we present comprehensive training data\nsynthesis pipelines, including collage-based and generative model-based data\nsynthesis. The collage-based data synthesis combines foreground objects and\nbackgrounds to generate diverse editing data, such as object manipulation,\nbackground changes, and text modifications. It can easily generate billions of\naccurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE\non extensive collage-based data to achieve strong performance in key editing\ntypes and enhance generalization and transfer capabilities. However,\ncollage-based data lacks some attribute editing cases, leading to a relative\ndrop in performance. In contrast, the generative model-based pipeline, despite\nbeing hard to scale up, offers flexibility in handling attribute editing cases.\nTherefore, we use generative model-based data to further fine-tune DreamVE.\nBesides, we design an efficient and powerful editing framework for DreamVE. We\nbuild on the SOTA T2V model and use a token concatenation with early drop\napproach to inject source image guidance, ensuring strong consistency and\neditability. The codes and models will be released.", "AI": {"tldr": "DreamVE\u662f\u4e00\u4e2a\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u548c\u89c6\u9891\u7f16\u8f91\u7edf\u4e00\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u548c\u591a\u6837\u5316\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u6307\u4ee4\u7f16\u8f91\u6f5c\u529b\u5927\u4f46\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u89c6\u9891\u7f16\u8f91\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u5148\u56fe\u50cf\u540e\u89c6\u9891\uff09\uff0c\u7ed3\u5408\u62fc\u8d34\u548c\u751f\u6210\u6a21\u578b\u6570\u636e\u5408\u6210\uff0c\u8bbe\u8ba1\u9ad8\u6548\u7f16\u8f91\u6846\u67b6\u3002", "result": "DreamVE\u5728\u5173\u952e\u7f16\u8f91\u7c7b\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "conclusion": "DreamVE\u4e3a\u6307\u4ee4\u7f16\u8f91\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5f00\u6e90\u3002"}}
{"id": "2508.06259", "categories": ["cs.CV", "cs.AI", "I.2.10"], "pdf": "https://arxiv.org/pdf/2508.06259", "abs": "https://arxiv.org/abs/2508.06259", "authors": ["Zhangquan Chen", "Ruihui Zhao", "Chuwei Luo", "Mingze Sun", "Xinlei Yu", "Yangyang Kang", "Ruqi Huang"], "title": "SIFThinker: Spatially-Aware Image Focus for Visual Reasoning", "comment": "15 pages, 13 figures", "summary": "Current multimodal large language models (MLLMs) still face significant\nchallenges in complex visual tasks (e.g., spatial understanding, fine-grained\nperception). Prior methods have tried to incorporate visual reasoning, however,\nthey fail to leverage attention correction with spatial cues to iteratively\nrefine their focus on prompt-relevant regions. In this paper, we introduce\nSIFThinker, a spatially-aware \"think-with-images\" framework that mimics human\nvisual perception. Specifically, SIFThinker enables attention correcting and\nimage region focusing by interleaving depth-enhanced bounding boxes and natural\nlanguage. Our contributions are twofold: First, we introduce a\nreverse-expansion-forward-inference strategy that facilitates the generation of\ninterleaved image-text chains of thought for process-level supervision, which\nin turn leads to the construction of the SIF-50K dataset. Besides, we propose\nGRPO-SIF, a reinforced training paradigm that integrates depth-informed visual\ngrounding into a unified reasoning pipeline, teaching the model to dynamically\ncorrect and focus on prompt-relevant regions. Extensive experiments demonstrate\nthat SIFThinker outperforms state-of-the-art methods in spatial understanding\nand fine-grained visual perception, while maintaining strong general\ncapabilities, highlighting the effectiveness of our method.", "AI": {"tldr": "SIFThinker\u662f\u4e00\u4e2a\u7a7a\u95f4\u611f\u77e5\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u589e\u5f3a\u7684\u8fb9\u754c\u6846\u548c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\uff0c\u63d0\u5347\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u590d\u6742\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u7a7a\u95f4\u7406\u89e3\u548c\u7ec6\u7c92\u5ea6\u611f\u77e5\uff09\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u5bf9\u6ce8\u610f\u529b\u6821\u6b63\u548c\u533a\u57df\u805a\u7126\u7684\u6709\u6548\u5229\u7528\u3002", "method": "\u63d0\u51faSIFThinker\u6846\u67b6\uff0c\u7ed3\u5408\u53cd\u5411\u6269\u5c55\u524d\u5411\u63a8\u7406\u7b56\u7565\u548cGRPO-SIF\u8bad\u7ec3\u8303\u5f0f\uff0c\u52a8\u6001\u6821\u6b63\u6ce8\u610f\u529b\u5e76\u805a\u7126\u76f8\u5173\u533a\u57df\u3002", "result": "\u5728\u7a7a\u95f4\u7406\u89e3\u548c\u7ec6\u7c92\u5ea6\u611f\u77e5\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u80fd\u529b\u3002", "conclusion": "SIFThinker\u901a\u8fc7\u7a7a\u95f4\u611f\u77e5\u548c\u52a8\u6001\u6ce8\u610f\u529b\u6821\u6b63\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2508.06082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06082", "abs": "https://arxiv.org/abs/2508.06082", "authors": ["Yanxiao Sun", "Jiafu Wu", "Yun Cao", "Chengming Xu", "Yabiao Wang", "Weijian Cao", "Donghao Luo", "Chengjie Wang", "Yanwei Fu"], "title": "SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment", "comment": null, "summary": "Diffusion-based or flow-based models have achieved significant progress in\nvideo synthesis but require multiple iterative sampling steps, which incurs\nsubstantial computational overhead. While many distillation methods that are\nsolely based on trajectory-preserving or distribution-matching have been\ndeveloped to accelerate video generation models, these approaches often suffer\nfrom performance breakdown or increased artifacts under few-step settings. To\naddress these limitations, we propose \\textbf{\\emph{SwiftVideo}}, a unified and\nstable distillation framework that combines the advantages of\ntrajectory-preserving and distribution-matching strategies. Our approach\nintroduces continuous-time consistency distillation to ensure precise\npreservation of ODE trajectories. Subsequently, we propose a dual-perspective\nalignment that includes distribution alignment between synthetic and real data\nalong with trajectory alignment across different inference steps. Our method\nmaintains high-quality video generation while substantially reducing the number\nof inference steps. Quantitative evaluations on the OpenVid-1M benchmark\ndemonstrate that our method significantly outperforms existing approaches in\nfew-step video generation.", "AI": {"tldr": "SwiftVideo\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u84b8\u998f\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u8f68\u8ff9\u4fdd\u6301\u548c\u5206\u5e03\u5339\u914d\u7b56\u7565\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u89c6\u9891\u751f\u6210\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u5c11\u6b65\u8bbe\u7f6e\u4e0b\u6027\u80fd\u4e0b\u964d\u6216\u4ea7\u751f\u66f4\u591a\u4f2a\u5f71\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5b9a\u7684\u84b8\u998f\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u8fde\u7eed\u65f6\u95f4\u4e00\u81f4\u6027\u84b8\u998f\u548c\u53cc\u89c6\u89d2\u5bf9\u9f50\uff08\u5206\u5e03\u5bf9\u9f50\u548c\u8f68\u8ff9\u5bf9\u9f50\uff09\u6765\u4f18\u5316\u84b8\u998f\u8fc7\u7a0b\u3002", "result": "\u5728OpenVid-1M\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSwiftVideo\u5728\u5c11\u6b65\u89c6\u9891\u751f\u6210\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SwiftVideo\u901a\u8fc7\u7ed3\u5408\u4e24\u79cd\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2508.06318", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06318", "abs": "https://arxiv.org/abs/2508.06318", "authors": ["Giacomo D'Amicantonio", "Snehashis Majhi", "Quan Kong", "Lorenzo Garattoni", "Gianpiero Francesca", "Fran\u00e7ois Bremond", "Egor Bondarev"], "title": "Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection", "comment": null, "summary": "Video Anomaly Detection (VAD) is a challenging task due to the variability of\nanomalous events and the limited availability of labeled data. Under the\nWeakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided\nduring training, while predictions are made at the frame level. Although\nstate-of-the-art models perform well on simple anomalies (e.g., explosions),\nthey struggle with complex real-world events (e.g., shoplifting). This\ndifficulty stems from two key issues: (1) the inability of current models to\naddress the diversity of anomaly types, as they process all categories with a\nshared model, overlooking category-specific features; and (2) the weak\nsupervision signal, which lacks precise temporal information, limiting the\nability to capture nuanced anomalous patterns blended with normal events. To\naddress these challenges, we propose Gaussian Splatting-guided Mixture of\nExperts (GS-MoE), a novel framework that employs a set of expert models, each\nspecialized in capturing specific anomaly types. These experts are guided by a\ntemporal Gaussian splatting loss, enabling the model to leverage temporal\nconsistency and enhance weak supervision. The Gaussian splatting approach\nencourages a more precise and comprehensive representation of anomalies by\nfocusing on temporal segments most likely to contain abnormal events. The\npredictions from these specialized experts are integrated through a\nmixture-of-experts mechanism to model complex relationships across diverse\nanomaly patterns. Our approach achieves state-of-the-art performance, with a\n91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on\nXD-Violence and MSAD datasets. By leveraging category-specific expertise and\ntemporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.", "AI": {"tldr": "GS-MoE\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u6a21\u578b\u548c\u65f6\u5e8f\u9ad8\u65af\u6e85\u5c04\u635f\u5931\u63d0\u5347\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u3002", "motivation": "\u73b0\u6709\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u5f02\u5e38\u4e8b\u4ef6\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u6a21\u578b\u5171\u4eab\u7ed3\u6784\u548c\u5f31\u76d1\u7763\u4fe1\u53f7\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faGS-MoE\u6846\u67b6\uff0c\u7ed3\u5408\u4e13\u5bb6\u6a21\u578b\u548c\u65f6\u5e8f\u9ad8\u65af\u6e85\u5c04\u635f\u5931\uff0c\u4e13\u6ce8\u4e8e\u7279\u5b9a\u5f02\u5e38\u7c7b\u578b\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u3002", "result": "\u5728UCF-Crime\u3001XD-Violence\u548cMSAD\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523091.58% AUC\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GS-MoE\u901a\u8fc7\u7c7b\u522b\u7279\u5b9a\u4e13\u5bb6\u548c\u65f6\u5e8f\u6307\u5bfc\uff0c\u4e3a\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2508.06084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06084", "abs": "https://arxiv.org/abs/2508.06084", "authors": ["Weichen Zhang", "Zhui Zhu", "Ningbo Li", "Kebin Liu", "Yunhao Liu"], "title": "AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance", "comment": null, "summary": "Vision-language models (VLMs) have achieved impressive performance on\nmultimodal reasoning tasks such as visual question answering (VQA), but their\ninference cost remains a significant challenge due to the large number of\nvision tokens processed during the prefill stage. Existing pruning methods\noften rely on directly using the attention patterns or static text prompt\nguidance, failing to exploit the dynamic internal signals generated during\ninference. To address these issues, we propose AdaptInfer, a plug-and-play\nframework for adaptive vision token pruning in VLMs. First, we introduce a\nfine-grained, dynamic text-guided pruning mechanism that reuses layer-wise\ntext-to-text attention maps to construct soft priors over text-token\nimportance, allowing more informed scoring of vision tokens at each stage.\nSecond, we perform an offline analysis of cross-modal attention shifts and\nidentify consistent inflection locations in inference, which inspire us to\npropose a more principled and efficient pruning schedule. Our method is\nlightweight and plug-and-play, also generalizable across multi-modal tasks.\nExperimental results have verified the effectiveness of the proposed method.\nFor example, it reduces CUDA latency by 61.3\\% while maintaining an average\naccuracy of 92.9\\% on vanilla LLaVA-1.5-7B. Under the same token budget,\nAdaptInfer surpasses SOTA in accuracy.", "AI": {"tldr": "AdaptInfer\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u81ea\u9002\u5e94\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6587\u672c\u5f15\u5bfc\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5206\u6790\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u63a8\u7406\u8fc7\u7a0b\u4e2d\u751f\u6210\u7684\u52a8\u6001\u5185\u90e8\u4fe1\u53f7\uff0c\u5bfc\u81f4\u6548\u7387\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u6587\u672c\u5f15\u5bfc\u526a\u679d\u673a\u5236\u548c\u57fa\u4e8e\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u53d8\u5316\u7684\u526a\u679d\u8ba1\u5212\u3002", "result": "\u5728LLaVA-1.5-7B\u4e0a\uff0cCUDA\u5ef6\u8fdf\u964d\u4f4e61.3%\uff0c\u5e73\u5747\u7cbe\u5ea6\u4fdd\u630192.9%\u3002", "conclusion": "AdaptInfer\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.06357", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06357", "abs": "https://arxiv.org/abs/2508.06357", "authors": ["Aman Bhatta", "Maria Dhakal", "Michael C. King", "Kevin W. Bowyer"], "title": "Are you In or Out (of gallery)? Wisdom from the Same-Identity Crowd", "comment": null, "summary": "A central problem in one-to-many facial identification is that the person in\nthe probe image may or may not have enrolled image(s) in the gallery; that is,\nmay be In-gallery or Out-of-gallery. Past approaches to detect when a rank-one\nresult is Out-of-gallery have mostly focused on finding a suitable threshold on\nthe similarity score. We take a new approach, using the additional enrolled\nimages of the identity with the rank-one result to predict if the rank-one\nresult is In-gallery / Out-of-gallery. Given a gallery of identities and\nimages, we generate In-gallery and Out-of-gallery training data by extracting\nthe ranks of additional enrolled images corresponding to the rank-one identity.\nWe then train a classifier to utilize this feature vector to predict whether a\nrank-one result is In-gallery or Out-of-gallery. Using two different datasets\nand four different matchers, we present experimental results showing that our\napproach is viable for mugshot quality probe images, and also, importantly, for\nprobes degraded by blur, reduced resolution, atmospheric turbulence and\nsunglasses. We also analyze results across demographic groups, and show that\nIn-gallery / Out-of-gallery classification accuracy is similar across\ndemographics. Our approach has the potential to provide an objective estimate\nof whether a one-to-many facial identification is Out-of-gallery, and thereby\nto reduce false positive identifications, wrongful arrests, and wasted\ninvestigative time. Interestingly, comparing the results of older deep\nCNN-based face matchers with newer ones suggests that the effectiveness of our\nOut-of-gallery detection approach emerges only with matchers trained using\nadvanced margin-based loss functions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6392\u540d\u7b2c\u4e00\u8eab\u4efd\u7684\u989d\u5916\u6ce8\u518c\u56fe\u50cf\u6765\u9884\u6d4b\u5176\u662f\u5426\u4e3a\u56fe\u5e93\u5185\u6216\u56fe\u5e93\u5916\uff0c\u4ee5\u51cf\u5c11\u8bef\u8bc6\u522b\u548c\u8c03\u67e5\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u4e00\u5bf9\u9762\u591a\u8bc6\u522b\u4e2d\u56fe\u5e93\u5916\u6837\u672c\u8bef\u8bc6\u522b\u7684\u95ee\u9898\uff0c\u51cf\u5c11\u8bef\u6355\u548c\u65e0\u6548\u8c03\u67e5\u3002", "method": "\u5229\u7528\u6392\u540d\u7b2c\u4e00\u8eab\u4efd\u7684\u989d\u5916\u6ce8\u518c\u56fe\u50cf\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u8bad\u7ec3\u5206\u7c7b\u5668\u9884\u6d4b\u56fe\u5e93\u5185\u5916\u72b6\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u56fe\u50cf\u8d28\u91cf\uff0c\u4e14\u5728\u4eba\u53e3\u7edf\u8ba1\u7ec4\u95f4\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u8bef\u8bc6\u522b\uff0c\u4e14\u4ec5\u5728\u4f7f\u7528\u9ad8\u7ea7\u635f\u5931\u51fd\u6570\u8bad\u7ec3\u7684\u5339\u914d\u5668\u4e2d\u6709\u6548\u3002"}}
{"id": "2508.06092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06092", "abs": "https://arxiv.org/abs/2508.06092", "authors": ["Yachun Mi", "Yu Li", "Yanting Li", "Shixin Sun", "Chen Hui", "Tong Zhang", "Yuanyuan Liu", "Chenyue Song", "Shaohui Liu"], "title": "Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation", "comment": null, "summary": "Accurate and efficient Video Quality Assessment (VQA) has long been a key\nresearch challenge. Current mainstream VQA methods typically improve\nperformance by pretraining on large-scale classification datasets (e.g.,\nImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this\nstrategy presents two significant challenges: (1) merely transferring semantic\nknowledge learned from pretraining is insufficient for VQA, as video quality\ndepends on multiple factors (e.g., semantics, distortion, motion, aesthetics);\n(2) pretraining on large-scale datasets demands enormous computational\nresources, often dozens or even hundreds of times greater than training\ndirectly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown\nremarkable generalization capabilities across a wide range of visual tasks, and\nhave begun to demonstrate promising potential in quality assessment. In this\nwork, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP\nenhances both visual and textual representations through a Shared Cross-Modal\nAdapter (SCMA), which contains only a minimal number of trainable parameters\nand is the only component that requires training. This design significantly\nreduces computational cost. In addition, we introduce a set of five learnable\nquality-level prompts to guide the VLMs in perceiving subtle quality\nvariations, thereby further enhancing the model's sensitivity to video quality.\nFurthermore, we investigate the impact of different frame sampling strategies\non VQA performance, and find that frame-difference-based sampling leads to\nbetter generalization performance across datasets. Extensive experiments\ndemonstrate that Q-CLIP exhibits excellent performance on several VQA datasets.", "AI": {"tldr": "Q-CLIP\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\uff08VQA\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u8de8\u6a21\u6001\u9002\u914d\u5668\uff08SCMA\uff09\u548c\u8d28\u91cf\u7ea7\u63d0\u793a\u63d0\u5347\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5f53\u524dVQA\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u4f46\u5b58\u5728\u8bed\u4e49\u77e5\u8bc6\u4e0d\u8db3\u548c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u7684\u95ee\u9898\uff0cVLMs\u5728\u8d28\u91cf\u8bc4\u4f30\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\u3002", "method": "\u63d0\u51faQ-CLIP\u6846\u67b6\uff0c\u5229\u7528SCMA\u589e\u5f3a\u89c6\u89c9\u548c\u6587\u672c\u8868\u793a\uff0c\u5f15\u5165\u8d28\u91cf\u7ea7\u63d0\u793a\u63d0\u5347\u6a21\u578b\u654f\u611f\u6027\uff0c\u5e76\u7814\u7a76\u5e27\u91c7\u6837\u7b56\u7565\u7684\u5f71\u54cd\u3002", "result": "Q-CLIP\u5728\u591a\u4e2aVQA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "Q-CLIP\u4e3aVQA\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86VLMs\u5728\u8be5\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.06407", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.06407", "abs": "https://arxiv.org/abs/2508.06407", "authors": ["Ch Muhammad Awais", "Marco Reggiannini", "Davide Moroni", "Oktay Karakus"], "title": "A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery", "comment": null, "summary": "High-resolution imagery plays a critical role in improving the performance of\nvisual recognition tasks such as classification, detection, and segmentation.\nIn many domains, including remote sensing and surveillance, low-resolution\nimages can limit the accuracy of automated analysis. To address this,\nsuper-resolution (SR) techniques have been widely adopted to attempt to\nreconstruct high-resolution images from low-resolution inputs. Related\ntraditional approaches focus solely on enhancing image quality based on\npixel-level metrics, leaving the relationship between super-resolved image\nfidelity and downstream classification performance largely underexplored. This\nraises a key question: can integrating classification objectives directly into\nthe super-resolution process further improve classification accuracy? In this\npaper, we try to respond to this question by investigating the relationship\nbetween super-resolution and classification through the deployment of a\nspecialised algorithmic strategy. We propose a novel methodology that increases\nthe resolution of synthetic aperture radar imagery by optimising loss functions\nthat account for both image quality and classification performance. Our\napproach improves image quality, as measured by scientifically ascertained\nimage quality indicators, while also enhancing classification accuracy.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5c06\u5206\u7c7b\u76ee\u6807\u76f4\u63a5\u878d\u5165\u8d85\u5206\u8fa8\u7387\u8fc7\u7a0b\u662f\u5426\u80fd\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u56fe\u50cf\u8d28\u91cf\u548c\u5206\u7c7b\u6027\u80fd\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u9650\u5236\u4e86\u81ea\u52a8\u5316\u5206\u6790\u7684\u51c6\u786e\u6027\uff0c\u4f20\u7edf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4ec5\u5173\u6ce8\u50cf\u7d20\u7ea7\u6307\u6807\uff0c\u672a\u5145\u5206\u63a2\u7d22\u8d85\u5206\u8fa8\u7387\u56fe\u50cf\u4fdd\u771f\u5ea6\u4e0e\u4e0b\u6e38\u5206\u7c7b\u6027\u80fd\u7684\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u540c\u65f6\u8003\u8651\u56fe\u50cf\u8d28\u91cf\u548c\u5206\u7c7b\u6027\u80fd\u7684\u635f\u5931\u51fd\u6570\uff0c\u63d0\u9ad8\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u56fe\u50cf\u7684\u5206\u8fa8\u7387\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u79d1\u5b66\u9a8c\u8bc1\u7684\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u4e0a\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u5206\u7c7b\u76ee\u6807\u878d\u5165\u8d85\u5206\u8fa8\u7387\u8fc7\u7a0b\u53ef\u4ee5\u540c\u65f6\u6539\u5584\u56fe\u50cf\u8d28\u91cf\u548c\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2508.06093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06093", "abs": "https://arxiv.org/abs/2508.06093", "authors": ["Chen Zhu", "Buzhen Huang", "Zijing Wu", "Binghui Zuo", "Yangang Wang"], "title": "E-React: Towards Emotionally Controlled Synthesis of Human Reactions", "comment": null, "summary": "Emotion serves as an essential component in daily human interactions.\nExisting human motion generation frameworks do not consider the impact of\nemotions, which reduces naturalness and limits their application in interactive\ntasks, such as human reaction synthesis. In this work, we introduce a novel\ntask: generating diverse reaction motions in response to different emotional\ncues. However, learning emotion representation from limited motion data and\nincorporating it into a motion generation framework remains a challenging\nproblem. To address the above obstacles, we introduce a semi-supervised emotion\nprior in an actor-reactor diffusion model to facilitate emotion-driven reaction\nsynthesis. Specifically, based on the observation that motion clips within a\nshort sequence tend to share the same emotion, we first devise a\nsemi-supervised learning framework to train an emotion prior. With this prior,\nwe further train an actor-reactor diffusion model to generate reactions by\nconsidering both spatial interaction and emotional response. Finally, given a\nmotion sequence of an actor, our approach can generate realistic reactions\nunder various emotional conditions. Experimental results demonstrate that our\nmodel outperforms existing reaction generation methods. The code and data will\nbe made publicly available at https://ereact.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u60c5\u611f\u9a71\u52a8\u7684\u53cd\u5e94\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u534a\u76d1\u7763\u60c5\u611f\u5148\u9a8c\u548c\u6269\u6563\u6a21\u578b\u63d0\u5347\u751f\u6210\u7684\u81ea\u7136\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u52a8\u4f5c\u751f\u6210\u6846\u67b6\u672a\u8003\u8651\u60c5\u611f\u5f71\u54cd\uff0c\u5bfc\u81f4\u751f\u6210\u52a8\u4f5c\u4e0d\u591f\u81ea\u7136\uff0c\u9650\u5236\u4e86\u5728\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u534a\u76d1\u7763\u5b66\u4e60\u8bad\u7ec3\u60c5\u611f\u5148\u9a8c\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u751f\u6210\u53cd\u5e94\u52a8\u4f5c\uff0c\u8003\u8651\u7a7a\u95f4\u4ea4\u4e92\u548c\u60c5\u611f\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u53cd\u5e94\u52a8\u4f5c\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u591a\u6837\u4e14\u81ea\u7136\u7684\u53cd\u5e94\u52a8\u4f5c\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u4efb\u52a1\u3002"}}
{"id": "2508.06429", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06429", "abs": "https://arxiv.org/abs/2508.06429", "authors": ["Guido Manni", "Clemente Lauretti", "Loredana Zollo", "Paolo Soda"], "title": "SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation", "comment": null, "summary": "Deep learning has revolutionized medical imaging, but its effectiveness is\nseverely limited by insufficient labeled training data. This paper introduces a\nnovel GAN-based semi-supervised learning framework specifically designed for\nlow labeled-data regimes, evaluated across settings with 5 to 50 labeled\nsamples per class. Our approach integrates three specialized neural networks --\na generator for class-conditioned image translation, a discriminator for\nauthenticity assessment and classification, and a dedicated classifier --\nwithin a three-phase training framework. The method alternates between\nsupervised training on limited labeled data and unsupervised learning that\nleverages abundant unlabeled images through image-to-image translation rather\nthan generation from noise. We employ ensemble-based pseudo-labeling that\ncombines confidence-weighted predictions from the discriminator and classifier\nwith temporal consistency through exponential moving averaging, enabling\nreliable label estimation for unlabeled data. Comprehensive evaluation across\neleven MedMNIST datasets demonstrates that our approach achieves statistically\nsignificant improvements over six state-of-the-art GAN-based semi-supervised\nmethods, with particularly strong performance in the extreme 5-shot setting\nwhere the scarcity of labeled data is most challenging. The framework maintains\nits superiority across all evaluated settings (5, 10, 20, and 50 shots per\nclass). Our approach offers a practical solution for medical imaging\napplications where annotation costs are prohibitive, enabling robust\nclassification performance even with minimal labeled data. Code is available at\nhttps://github.com/GuidoManni/SPARSE.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGAN\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u5b66\u5f71\u50cf\u4e2d\u6807\u7b7e\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u4e2d\u6807\u7b7e\u6570\u636e\u4e0d\u8db3\u5bfc\u81f4\u6df1\u5ea6\u5b66\u4e60\u6548\u679c\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u751f\u6210\u5668\u3001\u5224\u522b\u5668\u548c\u5206\u7c7b\u5668\u7684\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528\u56fe\u50cf\u7ffb\u8bd1\u548c\u96c6\u6210\u4f2a\u6807\u7b7e\u6280\u672f\u3002", "result": "\u572811\u4e2aMedMNIST\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u57285-shot\u6781\u7aef\u60c5\u51b5\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u4e3a\u533b\u5b66\u5f71\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u6807\u7b7e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06101", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06101", "abs": "https://arxiv.org/abs/2508.06101", "authors": ["Yachun Mi", "Xingyang He", "Shixin Sun", "Yu Li", "Yanting Li", "Zhixuan Li", "Jian Jin", "Chen Hui", "Shaohui Liu"], "title": "UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization", "comment": null, "summary": "In the digital age, advanced image editing tools pose a serious threat to the\nintegrity of visual content, making image forgery detection and localization a\nkey research focus. Most existing Image Manipulation Localization (IML) methods\nrely on discriminative learning and require large, high-quality annotated\ndatasets. However, current datasets lack sufficient scale and diversity,\nlimiting model performance in real-world scenarios. To overcome this, recent\nstudies have explored Constrained IML (CIML), which generates pixel-level\nannotations through algorithmic supervision. However, existing CIML approaches\noften depend on complex multi-stage pipelines, making the annotation process\ninefficient. In this work, we propose a novel generative framework based on\ndiffusion models, named UGD-IML, which for the first time unifies both IML and\nCIML tasks within a single framework. By learning the underlying data\ndistribution, generative diffusion models inherently reduce the reliance on\nlarge-scale labeled datasets, allowing our approach to perform effectively even\nunder limited data conditions. In addition, by leveraging a class embedding\nmechanism and a parameter-sharing design, our model seamlessly switches between\nIML and CIML modes without extra components or training overhead. Furthermore,\nthe end-to-end design enables our model to avoid cumbersome steps in the data\nannotation process. Extensive experimental results on multiple datasets\ndemonstrate that UGD-IML outperforms the SOTA methods by an average of 9.66 and\n4.36 in terms of F1 metrics for IML and CIML tasks, respectively. Moreover, the\nproposed method also excels in uncertainty estimation, visualization and\nrobustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6846\u67b6UGD-IML\uff0c\u9996\u6b21\u5c06IML\u548cCIML\u4efb\u52a1\u7edf\u4e00\u5728\u4e00\u4e2a\u6846\u67b6\u4e2d\uff0c\u51cf\u5c11\u4e86\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6570\u5b57\u65f6\u4ee3\u4e2d\u9ad8\u7ea7\u56fe\u50cf\u7f16\u8f91\u5de5\u5177\u5a01\u80c1\u89c6\u89c9\u5185\u5bb9\u7684\u771f\u5b9e\u6027\uff0c\u73b0\u6709IML\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u6570\u636e\u96c6\u7f3a\u4e4f\u89c4\u6a21\u548c\u591a\u6837\u6027\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faUGD-IML\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u5b66\u4e60\u6570\u636e\u5206\u5e03\uff0c\u901a\u8fc7\u7c7b\u522b\u5d4c\u5165\u673a\u5236\u548c\u53c2\u6570\u5171\u4eab\u8bbe\u8ba1\uff0c\u5b9e\u73b0IML\u548cCIML\u4efb\u52a1\u7684\u65e0\u7f1d\u5207\u6362\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cUGD-IML\u5728IML\u548cCIML\u4efb\u52a1\u4e2d\u7684F1\u6307\u6807\u5206\u522b\u5e73\u5747\u4f18\u4e8eSOTA\u65b9\u6cd59.66\u548c4.36\u3002", "conclusion": "UGD-IML\u6846\u67b6\u5728\u6027\u80fd\u3001\u4e0d\u786e\u5b9a\u4f30\u8ba1\u3001\u53ef\u89c6\u5316\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u56fe\u50cf\u7be1\u6539\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06434", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06434", "abs": "https://arxiv.org/abs/2508.06434", "authors": ["Shengzhu Yang", "Jiawei Du", "Shuai Lu", "Weihang Zhang", "Ningli Wang", "Huiqi Li"], "title": "CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment", "comment": null, "summary": "Large-scale natural image-text datasets, especially those automatically\ncollected from the web, often suffer from loose semantic alignment due to weak\nsupervision, while medical datasets tend to have high cross-modal correlation\nbut low content diversity. These properties pose a common challenge for\ncontrastive language-image pretraining (CLIP): they hinder the model's ability\nto learn robust and generalizable representations. In this work, we propose\nCLIPin, a unified non-contrastive plug-in that can be seamlessly integrated\ninto CLIP-style architectures to improve multimodal semantic alignment,\nproviding stronger supervision and enhancing alignment robustness. Furthermore,\ntwo shared pre-projectors are designed for image and text modalities\nrespectively to facilitate the integration of contrastive and non-contrastive\nlearning in a parameter-compromise manner. Extensive experiments on diverse\ndownstream tasks demonstrate the effectiveness and generality of CLIPin as a\nplug-and-play component compatible with various contrastive frameworks. Code is\navailable at https://github.com/T6Yang/CLIPin.", "AI": {"tldr": "CLIPin\u662f\u4e00\u79cd\u975e\u5bf9\u6bd4\u6027\u63d2\u4ef6\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230CLIP\u67b6\u6784\u4e2d\uff0c\u63d0\u5347\u591a\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u81ea\u7136\u56fe\u50cf-\u6587\u672c\u6570\u636e\u96c6\u8bed\u4e49\u5bf9\u9f50\u677e\u6563\u548c\u533b\u5b66\u6570\u636e\u96c6\u591a\u6837\u6027\u4f4e\u7684\u95ee\u9898\uff0c\u4ee5\u63d0\u5347CLIP\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faCLIPin\u63d2\u4ef6\uff0c\u8bbe\u8ba1\u5171\u4eab\u9884\u6295\u5f71\u5668\uff0c\u7ed3\u5408\u5bf9\u6bd4\u548c\u975e\u5bf9\u6bd4\u5b66\u4e60\u3002", "result": "\u5728\u591a\u6837\u5316\u4e0b\u6e38\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86CLIPin\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "CLIPin\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u7ec4\u4ef6\uff0c\u517c\u5bb9\u591a\u79cd\u5bf9\u6bd4\u6846\u67b6\u3002"}}
{"id": "2508.06104", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06104", "abs": "https://arxiv.org/abs/2508.06104", "authors": ["Gui Zou", "Chaofan Gan", "Chern Hong Lim", "Supavadee Aramvith", "Weiyao Lin"], "title": "MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment", "comment": "ICMEW 2025", "summary": "With the increasing availability of 2D and 3D data, significant advancements\nhave been made in the field of cross-modal retrieval. Nevertheless, the\nexistence of imperfect annotations presents considerable challenges, demanding\nrobust solutions for 2D-3D cross-modal retrieval in the presence of noisy label\nconditions. Existing methods generally address the issue of noise by dividing\nsamples independently within each modality, making them susceptible to\noverfitting on corrupted labels. To address these issues, we propose a robust\n2D-3D \\textbf{M}ulti-level cross-modal adaptive \\textbf{C}orrection and\n\\textbf{A}lignment framework (MCA). Specifically, we introduce a Multimodal\nJoint label Correction (MJC) mechanism that leverages multimodal historical\nself-predictions to jointly model the modality prediction consistency, enabling\nreliable label refinement. Additionally, we propose a Multi-level Adaptive\nAlignment (MAA) strategy to effectively enhance cross-modal feature semantics\nand discrimination across different levels. Extensive experiments demonstrate\nthe superiority of our method, MCA, which achieves state-of-the-art performance\non both conventional and realistic noisy 3D benchmarks, highlighting its\ngenerality and effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMCA\u7684\u9c81\u68d22D-3D\u8de8\u6a21\u6001\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ea7\u81ea\u9002\u5e94\u6821\u6b63\u548c\u5bf9\u9f50\u89e3\u51b3\u566a\u58f0\u6807\u7b7e\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u566a\u58f0\u6807\u7b7e\u65f6\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u4e14\u7f3a\u4e4f\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u5efa\u6a21\u3002", "method": "MCA\u6846\u67b6\u5305\u542b\u591a\u6a21\u6001\u8054\u5408\u6807\u7b7e\u6821\u6b63\uff08MJC\uff09\u548c\u591a\u7ea7\u81ea\u9002\u5e94\u5bf9\u9f50\uff08MAA\uff09\u7b56\u7565\u3002", "result": "\u5728\u4f20\u7edf\u548c\u566a\u58f03D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MCA\u5c55\u793a\u4e86\u5728\u566a\u58f0\u6807\u7b7e\u6761\u4ef6\u4e0b\u7684\u901a\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2508.06453", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06453", "abs": "https://arxiv.org/abs/2508.06453", "authors": ["Ruida Cheng", "Tejas Sudharshan Mathai", "Pritam Mukherjee", "Benjamin Hou", "Qingqing Zhu", "Zhiyong Lu", "Matthew McAuliffe", "Ronald M. Summers"], "title": "Text Embedded Swin-UMamba for DeepLesion Segmentation", "comment": null, "summary": "Segmentation of lesions on CT enables automatic measurement for clinical\nassessment of chronic diseases (e.g., lymphoma). Integrating large language\nmodels (LLMs) into the lesion segmentation workflow offers the potential to\ncombine imaging features with descriptions of lesion characteristics from the\nradiology reports. In this study, we investigate the feasibility of integrating\ntext into the Swin-UMamba architecture for the task of lesion segmentation. The\npublicly available ULS23 DeepLesion dataset was used along with short-form\ndescriptions of the findings from the reports. On the test dataset, a high Dice\nScore of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for\nlesion segmentation. The proposed Text-Swin-UMamba model outperformed prior\napproaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <\n0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by\n1.74% and 0.22%, respectively. The dataset and code can be accessed at\nhttps://github.com/ruida/LLM-Swin-UMamba", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0eSwin-UMamba\u67b6\u6784\u7ed3\u5408\u7528\u4e8eCT\u75c5\u7076\u5206\u5272\u7684\u53ef\u884c\u6027\uff0c\u7ed3\u679c\u663e\u793a\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408\u5f71\u50cf\u7279\u5f81\u548c\u653e\u5c04\u62a5\u544a\u4e2d\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u63d0\u5347\u6162\u6027\u75be\u75c5\uff08\u5982\u6dcb\u5df4\u7624\uff09\u75c5\u7076\u5206\u5272\u7684\u81ea\u52a8\u5316\u6d4b\u91cf\u6548\u679c\u3002", "method": "\u4f7f\u7528\u516c\u5f00\u7684ULS23 DeepLesion\u6570\u636e\u96c6\u548c\u62a5\u544a\u4e2d\u7684\u7b80\u77ed\u63cf\u8ff0\uff0c\u5c06\u6587\u672c\u6574\u5408\u5230Swin-UMamba\u67b6\u6784\u4e2d\u8fdb\u884c\u75c5\u7076\u5206\u5272\u3002", "result": "\u6d4b\u8bd5\u6570\u636e\u4e0a\u83b7\u5f9782%\u7684Dice\u5206\u6570\u548c6.58\u50cf\u7d20\u7684Hausdorff\u8ddd\u79bb\uff0c\u6027\u80fd\u4f18\u4e8eLLM\u9a71\u52a8\u7684LanGuideMedSeg\uff08\u63d0\u534737%\uff09\u53ca\u7eaf\u56fe\u50cf\u6a21\u578bxLSTM-UNet\u548cnnUNet\u3002", "conclusion": "Text-Swin-UMamba\u6a21\u578b\u5728\u75c5\u7076\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u6587\u672c\u4e0e\u5f71\u50cf\u7ed3\u5408\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.06485", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06485", "abs": "https://arxiv.org/abs/2508.06485", "authors": ["Sofiane Bouaziz", "Adel Hafiane", "Raphael Canals", "Rachid Nedjai"], "title": "WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion", "comment": "Submitted to IEEE Transactions on Geoscience and Remote Sensing\n  (TGRS)", "summary": "Urbanization, climate change, and agricultural stress are increasing the\ndemand for precise and timely environmental monitoring. Land Surface\nTemperature (LST) is a key variable in this context and is retrieved from\nremote sensing satellites. However, these systems face a trade-off between\nspatial and temporal resolution. While spatio-temporal fusion methods offer\npromising solutions, few have addressed the estimation of daily LST at 10 m\nresolution. In this study, we present WGAST, a Weakly-Supervised Generative\nNetwork for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra\nMODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning\nframework designed for this task. It adopts a conditional generative\nadversarial architecture, with a generator composed of four stages: feature\nextraction, fusion, LST reconstruction, and noise suppression. The first stage\nemploys a set of encoders to extract multi-level latent representations from\nthe inputs, which are then fused in the second stage using cosine similarity,\nnormalization, and temporal attention mechanisms. The third stage decodes the\nfused features into high-resolution LST, followed by a Gaussian filter to\nsuppress high-frequency noise. Training follows a weakly supervised strategy\nbased on physical averaging principles and reinforced by a PatchGAN\ndiscriminator. Experiments demonstrate that WGAST outperforms existing methods\nin both quantitative and qualitative evaluations. Compared to the\nbest-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves\nSSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and\neffectively captures fine-scale thermal patterns, as validated against 33\nground-based sensors. The code is available at\nhttps://github.com/Sofianebouaziz1/WGAST.git.", "AI": {"tldr": "WGAST\u662f\u4e00\u79cd\u5f31\u76d1\u7763\u751f\u6210\u7f51\u7edc\uff0c\u7528\u4e8e\u901a\u8fc7\u65f6\u7a7a\u878d\u5408Terra MODIS\u3001Landsat 8\u548cSentinel-2\u6570\u636e\uff0c\u5b9e\u73b0\u6bcf\u65e510\u7c73\u5206\u8fa8\u7387\u7684\u5730\u8868\u6e29\u5ea6\uff08LST\uff09\u4f30\u8ba1\u3002", "motivation": "\u57ce\u5e02\u5316\u3001\u6c14\u5019\u53d8\u5316\u548c\u519c\u4e1a\u538b\u529b\u589e\u52a0\u4e86\u5bf9\u7cbe\u786e\u53ca\u65f6\u73af\u5883\u76d1\u6d4b\u7684\u9700\u6c42\uff0c\u800c\u73b0\u6709\u9065\u611f\u7cfb\u7edf\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "WGAST\u91c7\u7528\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u67b6\u6784\uff0c\u5305\u62ec\u7279\u5f81\u63d0\u53d6\u3001\u878d\u5408\u3001LST\u91cd\u5efa\u548c\u566a\u58f0\u6291\u5236\u56db\u4e2a\u9636\u6bb5\uff0c\u5e76\u7ed3\u5408\u5f31\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\u3002", "result": "WGAST\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u964d\u4f4eRMSE 17.18%\uff0c\u63d0\u9ad8SSIM 11.00%\uff0c\u5e76\u80fd\u6709\u6548\u6355\u6349\u7ec6\u5c3a\u5ea6\u70ed\u6a21\u5f0f\u3002", "conclusion": "WGAST\u662f\u9996\u4e2a\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u9ad8\u6548\u4f30\u8ba1\u6bcf\u65e510\u7c73\u5206\u8fa8\u7387\u7684LST\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.06113", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06113", "abs": "https://arxiv.org/abs/2508.06113", "authors": ["Jian Wang", "Chaokang Jiang", "Haitao Xu"], "title": "GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving", "comment": "7 pages, 4 figures", "summary": "Diffusion-based models are redefining the state-of-the-art in end-to-end\nautonomous driving, yet their performance is increasingly hampered by a\nreliance on transformer-based fusion. These architectures face fundamental\nlimitations: quadratic computational complexity restricts the use of\nhigh-resolution features, and a lack of spatial priors prevents them from\neffectively modeling the inherent structure of Bird's Eye View (BEV)\nrepresentations. This paper introduces GMF-Drive (Gated Mamba Fusion for\nDriving), an end-to-end framework that overcomes these challenges through two\nprincipled innovations. First, we supersede the information-limited\nhistogram-based LiDAR representation with a geometrically-augmented pillar\nformat encoding shape descriptors and statistical features, preserving critical\n3D geometric details. Second, we propose a novel hierarchical gated mamba\nfusion (GM-Fusion) architecture that substitutes an expensive transformer with\na highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM\nleverages directional sequencing and adaptive fusion mechanisms to capture\nlong-range dependencies with linear complexity, while explicitly respecting the\nunique spatial properties of the driving scene. Extensive experiments on the\nchallenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new\nstate-of-the-art performance, significantly outperforming DiffusionDrive.\nComprehensive ablation studies validate the efficacy of each component,\ndemonstrating that task-specific SSMs can surpass a general-purpose transformer\nin both performance and efficiency for autonomous driving.", "AI": {"tldr": "GMF-Drive\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u95e8\u63a7Mamba\u878d\u5408\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u589e\u5f3a\u7684LiDAR\u8868\u793a\u548c\u9ad8\u6548\u7684\u7a7a\u95f4\u611f\u77e5\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u4f9d\u8d56Transformer\u878d\u5408\uff0c\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u548c\u7f3a\u4e4f\u7a7a\u95f4\u5148\u9a8c\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u7684\u4f7f\u7528\u548cBEV\u8868\u793a\u7684\u6709\u6548\u5efa\u6a21\u3002", "method": "1. \u4f7f\u7528\u51e0\u4f55\u589e\u5f3a\u7684\u67f1\u72b6LiDAR\u8868\u793a\u66ff\u4ee3\u76f4\u65b9\u56fe\u8868\u793a\uff1b2. \u63d0\u51fa\u5206\u5c42\u95e8\u63a7Mamba\u878d\u5408\u67b6\u6784\uff08GM-Fusion\uff09\uff0c\u7528\u9ad8\u6548\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u66ff\u4ee3Transformer\u3002", "result": "\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGMF-Drive\u5b9e\u73b0\u4e86\u65b0\u7684SOTA\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8eDiffusionDrive\u3002", "conclusion": "\u4efb\u52a1\u7279\u5b9a\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u901a\u7528Transformer\u3002"}}
{"id": "2508.06115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06115", "abs": "https://arxiv.org/abs/2508.06115", "authors": ["Weichen Zhang", "Kebin Liu", "Fan Dang", "Zhui Zhu", "Xikai Sun", "Yunhao Liu"], "title": "SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation", "comment": null, "summary": "Semantic segmentation in open-vocabulary scenarios presents significant\nchallenges due to the wide range and granularity of semantic categories.\nExisting weakly-supervised methods often rely on category-specific supervision\nand ill-suited feature construction methods for contrastive learning, leading\nto semantic misalignment and poor performance. In this work, we propose a novel\nweakly-supervised approach, SynSeg, to address the challenges. SynSeg performs\nMulti-Category Contrastive Learning (MCCL) as a stronger training signal with a\nnew feature reconstruction framework named Feature Synergy Structure (FSS).\nSpecifically, MCCL strategy robustly combines both intra- and inter-category\nalignment and separation in order to make the model learn the knowledge of\ncorrelations from different categories within the same image. Moreover, FSS\nreconstructs discriminative features for contrastive learning through prior\nfusion and semantic-activation-map enhancement, effectively avoiding the\nforeground bias introduced by the visual encoder. In general, SynSeg\neffectively improves the abilities in semantic localization and discrimination\nunder weak supervision. Extensive experiments on benchmarks demonstrate that\nour method outperforms state-of-the-art (SOTA) performance. For instance,\nSynSeg achieves higher accuracy than SOTA baselines by 4.5\\% on VOC, 8.9\\% on\nContext, 2.6\\% on Object and 2.0\\% on City.", "AI": {"tldr": "SynSeg\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f31\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7c7b\u522b\u5bf9\u6bd4\u5b66\u4e60\u548c\u7279\u5f81\u534f\u540c\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c47\u573a\u666f\u4e0b\u7684\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u5f00\u653e\u8bcd\u6c47\u573a\u666f\u4e0b\u7684\u8bed\u4e49\u5206\u5272\u9762\u4e34\u8bed\u4e49\u7c7b\u522b\u5e7f\u6cdb\u548c\u7ec6\u7c92\u5ea6\u7684\u6311\u6218\uff0c\u73b0\u6709\u5f31\u76d1\u7763\u65b9\u6cd5\u56e0\u4f9d\u8d56\u7c7b\u522b\u7279\u5b9a\u76d1\u7763\u548c\u4e0d\u9002\u5408\u7684\u7279\u5f81\u6784\u5efa\u65b9\u6cd5\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "SynSeg\u91c7\u7528\u591a\u7c7b\u522b\u5bf9\u6bd4\u5b66\u4e60\uff08MCCL\uff09\u548c\u7279\u5f81\u534f\u540c\u7ed3\u6784\uff08FSS\uff09\uff0c\u7ed3\u5408\u7c7b\u522b\u5185\u548c\u7c7b\u522b\u95f4\u7684\u5bf9\u9f50\u4e0e\u5206\u79bb\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u91cd\u6784\u907f\u514d\u89c6\u89c9\u7f16\u7801\u5668\u5f15\u5165\u7684\u524d\u666f\u504f\u5dee\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSynSeg\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u4f8b\u5982\u5728VOC\u4e0a\u51c6\u786e\u7387\u63d0\u53474.5%\uff0cContext\u4e0a\u63d0\u53478.9%\u3002", "conclusion": "SynSeg\u5728\u5f31\u76d1\u7763\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u5b9a\u4f4d\u548c\u533a\u5206\u80fd\u529b\uff0c\u4e3a\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06122", "categories": ["cs.CV", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2508.06122", "abs": "https://arxiv.org/abs/2508.06122", "authors": ["Ting-Shuo Yo", "Shih-Hao Su", "Chien-Ming Wu", "Wei-Ting Chen", "Jung-Lien Chu", "Chiao-Wei Chang", "Hung-Chi Kuo"], "title": "Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events", "comment": "37 pages, 6 figures, 3 tables", "summary": "This study applied representation learning algorithms to satellite images and\nevaluated the learned latent spaces with classifications of various weather\nevents. The algorithms investigated include the classical linear\ntransformation, i.e., principal component analysis (PCA), state-of-the-art deep\nlearning method, i.e., convolutional autoencoder (CAE), and a residual network\npre-trained with large image datasets (PT). The experiment results indicated\nthat the latent space learned by CAE consistently showed higher threat scores\nfor all classification tasks. The classifications with PCA yielded high hit\nrates but also high false-alarm rates. In addition, the PT performed\nexceptionally well at recognizing tropical cyclones but was inferior in other\ntasks. Further experiments suggested that representations learned from\nhigher-resolution datasets are superior in all classification tasks for\ndeep-learning algorithms, i.e., CAE and PT. We also found that smaller latent\nspace sizes had minor impact on the classification task's hit rate. Still, a\nlatent space dimension smaller than 128 caused a significantly higher false\nalarm rate. Though the CAE can learn latent spaces effectively and efficiently,\nthe interpretation of the learned representation lacks direct connections to\nphysical attributions. Therefore, developing a physics-informed version of CAE\ncan be a promising outlook for the current work.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5e94\u7528\u8868\u793a\u5b66\u4e60\u7b97\u6cd5\u5904\u7406\u536b\u661f\u56fe\u50cf\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540c\u5929\u6c14\u4e8b\u4ef6\u5206\u7c7b\u7684\u6f5c\u5728\u7a7a\u95f4\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u4e0d\u540c\u8868\u793a\u5b66\u4e60\u7b97\u6cd5\u5728\u536b\u661f\u56fe\u50cf\u5929\u6c14\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u6f5c\u5728\u7a7a\u95f4\u5bf9\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u4e86\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u3001\u5377\u79ef\u81ea\u7f16\u7801\u5668\uff08CAE\uff09\u548c\u9884\u8bad\u7ec3\u6b8b\u5dee\u7f51\u7edc\uff08PT\uff09\u4e09\u79cd\u7b97\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u548c\u6f5c\u5728\u7a7a\u95f4\u5c3a\u5bf8\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "CAE\u5728\u6240\u6709\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0cPCA\u547d\u4e2d\u7387\u9ad8\u4f46\u8bef\u62a5\u7387\u4e5f\u9ad8\uff0cPT\u5728\u70ed\u5e26\u6c14\u65cb\u8bc6\u522b\u4e2d\u8868\u73b0\u7a81\u51fa\u4f46\u5176\u4ed6\u4efb\u52a1\u8f83\u5dee\u3002\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u96c6\u5bf9\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u66f4\u6709\u5229\uff0c\u6f5c\u5728\u7a7a\u95f4\u5c3a\u5bf8\u5c0f\u4e8e128\u4f1a\u5bfc\u81f4\u8bef\u62a5\u7387\u663e\u8457\u4e0a\u5347\u3002", "conclusion": "CAE\u80fd\u9ad8\u6548\u5b66\u4e60\u6f5c\u5728\u7a7a\u95f4\uff0c\u4f46\u7f3a\u4e4f\u7269\u7406\u53ef\u89e3\u91ca\u6027\uff0c\u672a\u6765\u53ef\u5f00\u53d1\u7269\u7406\u4fe1\u606f\u589e\u5f3a\u7684CAE\u7248\u672c\u3002"}}
{"id": "2508.06125", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06125", "abs": "https://arxiv.org/abs/2508.06125", "authors": ["Lin Zhang", "Xianfang Zeng", "Kangcong Li", "Gang Yu", "Tao Chen"], "title": "SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning", "comment": "ICCV 2025", "summary": "We propose SC-Captioner, a reinforcement learning framework that enables the\nself-correcting capability of image caption models. Our crucial technique lies\nin the design of the reward function to incentivize accurate caption\ncorrections. Specifically, the predicted and reference captions are decomposed\ninto object, attribute, and relation sets using scene-graph parsing algorithms.\nWe calculate the set difference between sets of initial and self-corrected\ncaptions to identify added and removed elements. These elements are matched\nagainst the reference sets to calculate correctness bonuses for accurate\nrefinements and mistake punishments for wrong additions and removals, thereby\nforming the final reward. For image caption quality assessment, we propose a\nset of metrics refined from CAPTURE that alleviate its incomplete precision\nevaluation and inefficient relation matching problems. Furthermore, we collect\na fine-grained annotated image caption dataset, RefinedCaps, consisting of 6.5K\ndiverse images from COCO dataset. Experiments show that applying SC-Captioner\non large visual-language models can generate better image captions across\nvarious scenarios, significantly outperforming the direct preference\noptimization training strategy.", "AI": {"tldr": "SC-Captioner\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u63d0\u5347\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\u7684\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\u7f3a\u4e4f\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u63cf\u8ff0\u53ef\u80fd\u4e0d\u51c6\u786e\u3002SC-Captioner\u65e8\u5728\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u573a\u666f\u56fe\u89e3\u6790\u7b97\u6cd5\u5206\u89e3\u9884\u6d4b\u548c\u53c2\u8003\u63cf\u8ff0\u4e3a\u5bf9\u8c61\u3001\u5c5e\u6027\u548c\u5173\u7cfb\u96c6\u5408\uff0c\u901a\u8fc7\u96c6\u5408\u5dee\u5f02\u8ba1\u7b97\u5956\u52b1\u51fd\u6570\uff0c\u6fc0\u52b1\u51c6\u786e\u7ea0\u6b63\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSC-Captioner\u80fd\u751f\u6210\u66f4\u51c6\u786e\u7684\u56fe\u50cf\u63cf\u8ff0\uff0c\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7b56\u7565\u3002", "conclusion": "SC-Captioner\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u548c\u7cbe\u7ec6\u7684\u5956\u52b1\u8bbe\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\u7684\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\u548c\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2508.06127", "categories": ["cs.CV", "I.4.9"], "pdf": "https://arxiv.org/pdf/2508.06127", "abs": "https://arxiv.org/abs/2508.06127", "authors": ["Yi Qin", "Rui Wang", "Tao Huang", "Tong Xiao", "Liping Jing"], "title": "SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures", "comment": "8 pages,recived by ICCV2025", "summary": "While the Segment Anything Model (SAM) transforms interactive segmentation\nwith zero-shot abilities, its inherent vulnerabilities present a single-point\nrisk, potentially leading to the failure of numerous downstream applications.\nProactively evaluating these transferable vulnerabilities is thus imperative.\nPrior adversarial attacks on SAM often present limited transferability due to\ninsufficient exploration of common weakness across domains. To address this, we\npropose Vertex-Refining Simplicial Complex Attack (VeSCA), a novel method that\nleverages only the encoder of SAM for generating transferable adversarial\nexamples. Specifically, it achieves this by explicitly characterizing the\nshared vulnerable regions between SAM and downstream models through a\nparametric simplicial complex. Our goal is to identify such complexes within\nadversarially potent regions by iterative vertex-wise refinement. A lightweight\ndomain re-adaptation strategy is introduced to bridge domain divergence using\nminimal reference data during the initialization of simplicial complex.\nUltimately, VeSCA generates consistently transferable adversarial examples\nthrough random simplicial complex sampling. Extensive experiments demonstrate\nthat VeSCA achieves performance improved by 12.7% compared to state-of-the-art\nmethods across three downstream model categories across five domain-specific\ndatasets. Our findings further highlight the downstream model risks posed by\nSAM's vulnerabilities and emphasize the urgency of developing more robust\nfoundation models.", "AI": {"tldr": "VeSCA\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528SAM\u7684\u7f16\u7801\u5668\u751f\u6210\u53ef\u8f6c\u79fb\u7684\u5bf9\u6297\u6837\u672c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u8bc4\u4f30SAM\u7684\u53ef\u8f6c\u79fb\u6f0f\u6d1e\u5bf9\u4e0b\u6e38\u5e94\u7528\u7684\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "VeSCA\u901a\u8fc7\u53c2\u6570\u5316\u5355\u7eaf\u590d\u5f62\u548c\u9876\u70b9\u7ec6\u5316\u8fed\u4ee3\uff0c\u8bc6\u522b\u5171\u4eab\u8106\u5f31\u533a\u57df\u3002", "result": "VeSCA\u5728\u4e94\u4e2a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u63d0\u534712.7%\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86SAM\u6f0f\u6d1e\u5bf9\u4e0b\u6e38\u6a21\u578b\u7684\u98ce\u9669\uff0c\u547c\u5401\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2508.06139", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06139", "abs": "https://arxiv.org/abs/2508.06139", "authors": ["Shaohua Pan", "Xinyu Yi", "Yan Zhou", "Weihua Jian", "Yuan Zhang", "Pengfei Wan", "Feng Xu"], "title": "DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera", "comment": null, "summary": "Combining sparse IMUs and a monocular camera is a new promising setting to\nperform real-time human motion capture. This paper proposes a diffusion-based\nsolution to learn human motion priors and fuse the two modalities of signals\ntogether seamlessly in a unified framework. By delicately considering the\ncharacteristics of the two signals, the sequential visual information is\nconsidered as a whole and transformed into a condition embedding, while the\ninertial measurement is concatenated with the noisy body pose frame by frame to\nconstruct a sequential input for the diffusion model. Firstly, we observe that\nthe visual information may be unavailable in some frames due to occlusions or\nsubjects moving out of the camera view. Thus incorporating the sequential\nvisual features as a whole to get a single feature embedding is robust to the\noccasional degenerations of visual information in those frames. On the other\nhand, the IMU measurements are robust to occlusions and always stable when\nsignal transmission has no problem. So incorporating them frame-wisely could\nbetter explore the temporal information for the system. Experiments have\ndemonstrated the effectiveness of the system design and its state-of-the-art\nperformance in pose estimation compared with the previous works. Our codes are\navailable for research at https://shaohua-pan.github.io/diffcap-page.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u7a00\u758fIMU\u548c\u5355\u76ee\u6444\u50cf\u5934\u8fdb\u884c\u5b9e\u65f6\u4eba\u4f53\u8fd0\u52a8\u6355\u6349\uff0c\u901a\u8fc7\u878d\u5408\u4e24\u79cd\u4fe1\u53f7\u6a21\u6001\u5e76\u8003\u8651\u5176\u7279\u6027\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u4e14\u9ad8\u6027\u80fd\u7684\u59ff\u6001\u4f30\u8ba1\u3002", "motivation": "\u7a00\u758fIMU\u548c\u5355\u76ee\u6444\u50cf\u5934\u7ed3\u5408\u662f\u4e00\u79cd\u65b0\u5174\u7684\u5b9e\u65f6\u4eba\u4f53\u8fd0\u52a8\u6355\u6349\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u4fe1\u606f\u7f3a\u5931\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5b66\u4e60\u8fd0\u52a8\u5148\u9a8c\u5e76\u878d\u5408\u4e24\u79cd\u4fe1\u53f7\u6a21\u6001\uff0c\u63d0\u5347\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "method": "\u5c06\u89c6\u89c9\u4fe1\u606f\u6574\u4f53\u8f6c\u5316\u4e3a\u6761\u4ef6\u5d4c\u5165\uff0c\u540c\u65f6\u5c06IMU\u6d4b\u91cf\u4e0e\u566a\u58f0\u59ff\u6001\u9010\u5e27\u62fc\u63a5\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u8f93\u5165\u3002\u8fd9\u79cd\u8bbe\u8ba1\u5206\u522b\u5229\u7528\u4e86\u89c6\u89c9\u4fe1\u606f\u7684\u5168\u5c40\u9c81\u68d2\u6027\u548cIMU\u4fe1\u53f7\u7684\u65f6\u5e8f\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u6269\u6563\u6a21\u578b\u6846\u67b6\u6709\u6548\u878d\u5408\u4e86IMU\u548c\u89c6\u89c9\u4fe1\u53f7\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u4fe1\u606f\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u8fbe\u5230\u4e86\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2508.06142", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06142", "abs": "https://arxiv.org/abs/2508.06142", "authors": ["Hanqing Wang", "Yuan Tian", "Mingyu Liu", "Zhenhao Zhang", "Xiangyang Zhu"], "title": "SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models", "comment": null, "summary": "In the rapidly evolving landscape of Multimodal Large Language Models\n(MLLMs), the safety concerns of their outputs have earned significant\nattention. Although numerous datasets have been proposed, they may become\noutdated with MLLM advancements and are susceptible to data contamination\nissues. To address these problems, we propose \\textbf{SDEval}, the\n\\textit{first} safety dynamic evaluation framework to controllably adjust the\ndistribution and complexity of safety benchmarks. Specifically, SDEval mainly\nadopts three dynamic strategies: text, image, and text-image dynamics to\ngenerate new samples from original benchmarks. We first explore the individual\neffects of text and image dynamics on model safety. Then, we find that\ninjecting text dynamics into images can further impact safety, and conversely,\ninjecting image dynamics into text also leads to safety risks. SDEval is\ngeneral enough to be applied to various existing safety and even capability\nbenchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and\ncapability benchmarks, MMBench and MMVet, show that SDEval significantly\ninfluences safety evaluation, mitigates data contamination, and exposes safety\nlimitations of MLLMs. Code is available at https://github.com/hq-King/SDEval", "AI": {"tldr": "SDEval\u662f\u4e00\u4e2a\u52a8\u6001\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u8c03\u6574\u5b89\u5168\u57fa\u51c6\u7684\u5206\u5e03\u548c\u590d\u6742\u6027\u6765\u5e94\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u53ef\u80fd\u8fc7\u65f6\u4e14\u6613\u53d7\u6570\u636e\u6c61\u67d3\u5f71\u54cd\uff0c\u9700\u8981\u52a8\u6001\u8bc4\u4f30\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6587\u672c\u3001\u56fe\u50cf\u548c\u6587\u672c-\u56fe\u50cf\u52a8\u6001\u7b56\u7565\u751f\u6210\u65b0\u6837\u672c\uff0c\u5e76\u63a2\u7d22\u5176\u5bf9\u6a21\u578b\u5b89\u5168\u6027\u7684\u5f71\u54cd\u3002", "result": "SDEval\u663e\u8457\u5f71\u54cd\u5b89\u5168\u8bc4\u4f30\uff0c\u51cf\u8f7b\u6570\u636e\u6c61\u67d3\uff0c\u5e76\u66b4\u9732MLLMs\u7684\u5b89\u5168\u5c40\u9650\u6027\u3002", "conclusion": "SDEval\u662f\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u73b0\u6709\u5b89\u5168\u548c\u80fd\u529b\u57fa\u51c6\uff0c\u6709\u6548\u63d0\u5347\u8bc4\u4f30\u8d28\u91cf\u3002"}}
{"id": "2508.06146", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06146", "abs": "https://arxiv.org/abs/2508.06146", "authors": ["Yuchen Guan", "Chong Sun", "Canmiao Fu", "Zhipeng Huang", "Chun Yuan", "Chen Li"], "title": "Text-guided Visual Prompt DINO for Generic Segmentation", "comment": null, "summary": "Recent advancements in multimodal vision models have highlighted limitations\nin late-stage feature fusion and suboptimal query selection for hybrid prompts\nopen-world segmentation, alongside constraints from caption-derived\nvocabularies. To address these challenges, we propose Prompt-DINO, a\ntext-guided visual Prompt DINO framework featuring three key innovations.\nFirst, we introduce an early fusion mechanism that unifies text/visual prompts\nand backbone features at the initial encoding stage, enabling deeper\ncross-modal interactions to resolve semantic ambiguities. Second, we design\norder-aligned query selection for DETR-based architectures, explicitly\noptimizing the structural alignment between text and visual queries during\ndecoding to enhance semantic-spatial consistency. Third, we develop a\ngenerative data engine powered by the Recognize Anything via Prompting (RAP)\nmodel, which synthesizes 0.5B diverse training instances through a dual-path\ncross-verification pipeline, reducing label noise by 80.5% compared to\nconventional approaches. Extensive experiments demonstrate that Prompt-DINO\nachieves state-of-the-art performance on open-world detection benchmarks while\nsignificantly expanding semantic coverage beyond fixed-vocabulary constraints.\nOur work establishes a new paradigm for scalable multimodal detection and data\ngeneration in open-world scenarios. Data&Code are available at\nhttps://github.com/WeChatCV/WeVisionOne.", "AI": {"tldr": "Prompt-DINO \u662f\u4e00\u79cd\u6587\u672c\u5f15\u5bfc\u7684\u89c6\u89c9\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u65e9\u671f\u878d\u5408\u673a\u5236\u3001\u987a\u5e8f\u5bf9\u9f50\u67e5\u8be2\u9009\u62e9\u548c\u751f\u6210\u6570\u636e\u5f15\u64ce\uff0c\u89e3\u51b3\u4e86\u5f00\u653e\u4e16\u754c\u5206\u5272\u4e2d\u7684\u7279\u5f81\u878d\u5408\u548c\u67e5\u8be2\u9009\u62e9\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u89c6\u89c9\u6a21\u578b\u5728\u540e\u671f\u7279\u5f81\u878d\u5408\u548c\u6df7\u5408\u63d0\u793a\u67e5\u8be2\u9009\u62e9\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e14\u53d7\u9650\u4e8e\u5b57\u5e55\u8bcd\u6c47\u3002", "method": "\u63d0\u51fa\u65e9\u671f\u878d\u5408\u673a\u5236\u3001\u987a\u5e8f\u5bf9\u9f50\u67e5\u8be2\u9009\u62e9\u548c\u751f\u6210\u6570\u636e\u5f15\u64ce\u3002", "result": "\u5728\u5f00\u653e\u4e16\u754c\u68c0\u6d4b\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8bed\u4e49\u8986\u76d6\u8303\u56f4\u663e\u8457\u6269\u5927\u3002", "conclusion": "Prompt-DINO \u4e3a\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u53ef\u6269\u5c55\u591a\u6a21\u6001\u68c0\u6d4b\u548c\u6570\u636e\u751f\u6210\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.06147", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06147", "abs": "https://arxiv.org/abs/2508.06147", "authors": ["Xuanyu Liu", "Bonan An"], "title": "DSConv: Dynamic Splitting Convolution for Pansharpening", "comment": null, "summary": "Aiming to obtain a high-resolution image, pansharpening involves the fusion\nof a multi-spectral image (MS) and a panchromatic image (PAN), the low-level\nvision task remaining significant and challenging in contemporary research.\nMost existing approaches rely predominantly on standard convolutions, few\nmaking the effort to adaptive convolutions, which are effective owing to the\ninter-pixel correlations of remote sensing images. In this paper, we propose a\nnovel strategy for dynamically splitting convolution kernels in conjunction\nwith attention, selecting positions of interest, and splitting the original\nconvolution kernel into multiple smaller kernels, named DSConv. The proposed\nDSConv more effectively extracts features of different positions within the\nreceptive field, enhancing the network's generalization, optimization, and\nfeature representation capabilities. Furthermore, we innovate and enrich\nconcepts of dynamic splitting convolution and provide a novel network\narchitecture for pansharpening capable of achieving the tasks more efficiently,\nbuilding upon this methodology. Adequate fair experiments illustrate the\neffectiveness and the state-of-the-art performance attained by\nDSConv.Comprehensive and rigorous discussions proved the superiority and\noptimal usage conditions of DSConv.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDSConv\u7684\u52a8\u6001\u5377\u79ef\u6838\u5206\u5272\u65b9\u6cd5\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u56fe\u50cf\u878d\u5408\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u6807\u51c6\u5377\u79ef\uff0c\u800c\u81ea\u9002\u5e94\u5377\u79ef\u80fd\u66f4\u597d\u5730\u5229\u7528\u9065\u611f\u56fe\u50cf\u7684\u50cf\u7d20\u76f8\u5173\u6027\u3002", "method": "\u52a8\u6001\u5206\u5272\u5377\u79ef\u6838\u5e76\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u9009\u62e9\u611f\u5174\u8da3\u4f4d\u7f6e\uff0c\u5c06\u539f\u59cb\u5377\u79ef\u6838\u5206\u5272\u4e3a\u591a\u4e2a\u5c0f\u6838\u3002", "result": "DSConv\u80fd\u66f4\u6709\u6548\u5730\u63d0\u53d6\u7279\u5f81\uff0c\u63d0\u5347\u7f51\u7edc\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5148\u8fdb\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "DSConv\u5728\u56fe\u50cf\u878d\u5408\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u6700\u4f73\u4f7f\u7528\u6761\u4ef6\u3002"}}
{"id": "2508.06152", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06152", "abs": "https://arxiv.org/abs/2508.06152", "authors": ["Kaiyuan Jiang", "Ruoxi Sun", "Ying Cao", "Yuqi Xu", "Xinran Zhang", "Junyan Guo", "ChengSheng Deng"], "title": "VISTAR:A User-Centric and Role-Driven Benchmark for Text-to-Image Evaluation", "comment": "17 pages,8 figures", "summary": "We present VISTAR, a user-centric, multi-dimensional benchmark for\ntext-to-image (T2I) evaluation that addresses the limitations of existing\nmetrics. VISTAR introduces a two-tier hybrid paradigm: it employs\ndeterministic, scriptable metrics for physically quantifiable attributes (e.g.,\ntext rendering, lighting) and a novel Hierarchical Weighted P/N Questioning\n(HWPQ) scheme that uses constrained vision-language models to assess abstract\nsemantics (e.g., style fusion, cultural fidelity). Grounded in a Delphi study\nwith 120 experts, we defined seven user roles and nine evaluation angles to\nconstruct the benchmark, which comprises 2,845 prompts validated by over 15,000\nhuman pairwise comparisons. Our metrics achieve high human alignment (>75%),\nwith the HWPQ scheme reaching 85.9% accuracy on abstract semantics,\nsignificantly outperforming VQA baselines. Comprehensive evaluation of\nstate-of-the-art models reveals no universal champion, as role-weighted scores\nreorder rankings and provide actionable guidance for domain-specific\ndeployment. All resources are publicly released to foster reproducible T2I\nassessment.", "AI": {"tldr": "VISTAR\u662f\u4e00\u4e2a\u7528\u6237\u4e2d\u5fc3\u7684\u591a\u7ef4\u5ea6\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u8bc4\u4f30\u57fa\u51c6\uff0c\u7ed3\u5408\u786e\u5b9a\u6027\u6307\u6807\u548c\u65b0\u578bHWPQ\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709T2I\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u5c40\u9650\u6027\uff0cVISTAR\u65e8\u5728\u63d0\u4f9b\u66f4\u5168\u9762\u3001\u7528\u6237\u5bfc\u5411\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6df7\u5408\u8303\u5f0f\uff1a\u786e\u5b9a\u6027\u6307\u6807\u91cf\u5316\u7269\u7406\u5c5e\u6027\uff0cHWPQ\u65b9\u6848\u8bc4\u4f30\u62bd\u8c61\u8bed\u4e49\u3002\u57fa\u4e8e\u4e13\u5bb6\u7814\u7a76\u5b9a\u4e49\u7528\u6237\u89d2\u8272\u548c\u8bc4\u4f30\u89d2\u5ea6\u3002", "result": "VISTAR\u6307\u6807\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\uff08>75%\uff09\uff0cHWPQ\u5728\u62bd\u8c61\u8bed\u4e49\u4e0a\u8fbe\u523085.9%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "VISTAR\u4e3aT2I\u8bc4\u4f30\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u652f\u6301\u9488\u5bf9\u6027\u90e8\u7f72\u3002"}}
{"id": "2508.06157", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06157", "abs": "https://arxiv.org/abs/2508.06157", "authors": ["Xiaoxiao Yang", "Meiliang Liu", "Yunfang Xu", "Zijin Li", "Zhengye Si", "Xinyue Yang", "Zhiwen Zhao"], "title": "An Interpretable Multi-Plane Fusion Framework With Kolmogorov-Arnold Network Guided Attention Enhancement for Alzheimer's Disease Diagnosis", "comment": null, "summary": "Alzheimer's disease (AD) is a progressive neurodegenerative disorder that\nseverely impairs cognitive function and quality of life. Timely intervention in\nAD relies heavily on early and precise diagnosis, which remains challenging due\nto the complex and subtle structural changes in the brain. Most existing deep\nlearning methods focus only on a single plane of structural magnetic resonance\nimaging (sMRI) and struggle to accurately capture the complex and nonlinear\nrelationships among pathological regions of the brain, thus limiting their\nability to precisely identify atrophic features. To overcome these limitations,\nwe propose an innovative framework, MPF-KANSC, which integrates multi-plane\nfusion (MPF) for combining features from the coronal, sagittal, and axial\nplanes, and a Kolmogorov-Arnold Network-guided spatial-channel attention\nmechanism (KANSC) to more effectively learn and represent sMRI atrophy\nfeatures. Specifically, the proposed model enables parallel feature extraction\nfrom multiple anatomical planes, thus capturing more comprehensive structural\ninformation. The KANSC attention mechanism further leverages a more flexible\nand accurate nonlinear function approximation technique, facilitating precise\nidentification and localization of disease-related abnormalities. Experiments\non the ADNI dataset confirm that the proposed MPF-KANSC achieves superior\nperformance in AD diagnosis. Moreover, our findings provide new evidence of\nright-lateralized asymmetry in subcortical structural changes during AD\nprogression, highlighting the model's promising interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMPF-KANSC\u7684\u521b\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5e73\u9762\u878d\u5408\u548cKANSC\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u7684\u65e9\u671f\u8bca\u65ad\u56e0\u8111\u90e8\u7ed3\u6784\u53d8\u5316\u7684\u590d\u6742\u6027\u548c\u7ec6\u5fae\u6027\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u6355\u6349\u75c5\u7406\u533a\u57df\u7684\u975e\u7ebf\u6027\u5173\u7cfb\u3002", "method": "MPF-KANSC\u6846\u67b6\u7ed3\u5408\u4e86\u591a\u5e73\u9762\u878d\u5408\uff08MPF\uff09\u548cKolmogorov-Arnold\u7f51\u7edc\u5f15\u5bfc\u7684\u7a7a\u95f4\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\uff08KANSC\uff09\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u5b66\u4e60\u548c\u8868\u793asMRI\u840e\u7f29\u7279\u5f81\u3002", "result": "\u5728ADNI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMPF-KANSC\u5728AD\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86AD\u8fdb\u5c55\u4e2d\u53f3\u504f\u4fa7\u5316\u7684\u76ae\u5c42\u4e0b\u7ed3\u6784\u53d8\u5316\u3002", "conclusion": "MPF-KANSC\u4e0d\u4ec5\u63d0\u9ad8\u4e86AD\u8bca\u65ad\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u63d0\u4f9b\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3aAD\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc1\u636e\u3002"}}
{"id": "2508.06160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06160", "abs": "https://arxiv.org/abs/2508.06160", "authors": ["Zhenbang Du", "Yonggan Fu", "Lifu Wang", "Jiayi Qian", "Xiao Luo", "Yingyan", "Lin"], "title": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment", "comment": "Accepted by ICCV 2025", "summary": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff.", "AI": {"tldr": "PostDiff\u6846\u67b6\u901a\u8fc7\u6df7\u5408\u5206\u8fa8\u7387\u53bb\u566a\u548c\u6a21\u5757\u7ea7\u7f13\u5b58\u7b56\u7565\uff0c\u5728\u65e0\u9700\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u6548\u7387\u4e0e\u4fdd\u771f\u5ea6\u3002", "motivation": "\u7814\u7a76\u5728\u8d44\u6e90\u6709\u9650\u5e73\u53f0\u4e0a\u90e8\u7f72\u6269\u6563\u6a21\u578b\u65f6\uff0c\u51cf\u5c11\u53bb\u566a\u6b65\u9aa4\u4e0e\u964d\u4f4e\u6bcf\u6b65\u8ba1\u7b97\u6210\u672c\u54ea\u79cd\u66f4\u6709\u6548\u3002", "method": "\u63d0\u51faPostDiff\u6846\u67b6\uff0c\u5305\u62ec\u6df7\u5408\u5206\u8fa8\u7387\u53bb\u566a\u548c\u6a21\u5757\u7ea7\u7f13\u5b58\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u964d\u4f4e\u6bcf\u6b65\u8ba1\u7b97\u6210\u672c\u6bd4\u51cf\u5c11\u53bb\u566a\u6b65\u9aa4\u66f4\u80fd\u63d0\u5347\u6548\u7387\u4e0e\u4fdd\u771f\u5ea6\u3002", "conclusion": "PostDiff\u663e\u8457\u4f18\u5316\u4e86\u6269\u6563\u6a21\u578b\u7684\u6548\u7387\u4e0e\u4fdd\u771f\u5ea6\u6743\u8861\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.06177", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06177", "abs": "https://arxiv.org/abs/2508.06177", "authors": ["Dominik Br\u00e4mer", "Diana Kleingarn", "Oliver Urbann"], "title": "Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor", "comment": "Accepted at 28th RoboCup International Symposium, Salvador, Brasil", "summary": "Accurate localization represents a fundamental challenge in\n  robotic navigation. Traditional methodologies, such as Lidar or QR-code based\nsystems, suffer from inherent scalability and adaptability con straints,\nparticularly in complex environments. In this work, we propose\n  an innovative localization framework that harnesses flooring characteris tics\nby employing graph-based representations and Graph Convolutional\n  Networks (GCNs). Our method uses graphs to represent floor features,\n  which helps localize the robot more accurately (0.64cm error) and more\n  efficiently than comparing individual image features. Additionally, this\n  approach successfully addresses the kidnapped robot problem in every\n  frame without requiring complex filtering processes. These advancements\n  open up new possibilities for robotic navigation in diverse environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u7684\u673a\u5668\u4eba\u5b9a\u4f4d\u6846\u67b6\uff0c\u5229\u7528\u5730\u677f\u7279\u5f81\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\uff080.64cm\u8bef\u5dee\uff09\u548c\u9ad8\u6548\u5b9a\u4f4d\uff0c\u5e76\u89e3\u51b3\u4e86\u7ed1\u67b6\u673a\u5668\u4eba\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5b9a\u4f4d\u65b9\u6cd5\uff08\u5982\u6fc0\u5149\u96f7\u8fbe\u6216\u4e8c\u7ef4\u7801\uff09\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u56fe\u8868\u793a\u5730\u677f\u7279\u5f81\uff0c\u5e76\u5229\u7528\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u8fdb\u884c\u5b9a\u4f4d\uff0c\u907f\u514d\u4e86\u590d\u6742\u7684\u6ee4\u6ce2\u8fc7\u7a0b\u3002", "result": "\u5b9e\u73b0\u4e860.64cm\u7684\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\uff0c\u5e76\u5728\u6bcf\u4e00\u5e27\u4e2d\u6210\u529f\u89e3\u51b3\u4e86\u7ed1\u67b6\u673a\u5668\u4eba\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u5bfc\u822a\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.06189", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06189", "abs": "https://arxiv.org/abs/2508.06189", "authors": ["Cheng Liu", "Daou Zhang", "Tingxu Liu", "Yuhan Wang", "Jinyang Chen", "Yuexuan Li", "Xinying Xiao", "Chenbo Xin", "Ziru Wang", "Weichao Wu"], "title": "MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration", "comment": null, "summary": "With the acceleration of urbanization, criminal behavior in public scenes\nposes an increasingly serious threat to social security. Traditional anomaly\ndetection methods based on feature recognition struggle to capture high-level\nbehavioral semantics from historical information, while generative approaches\nbased on Large Language Models (LLMs) often fail to meet real-time\nrequirements. To address these challenges, we propose MA-CBP, a criminal\nbehavior prediction framework based on multi-agent asynchronous collaboration.\nThis framework transforms real-time video streams into frame-level semantic\ndescriptions, constructs causally consistent historical summaries, and fuses\nadjacent image frames to perform joint reasoning over long- and short-term\ncontexts. The resulting behavioral decisions include key elements such as event\nsubjects, locations, and causes, enabling early warning of potential criminal\nactivity. In addition, we construct a high-quality criminal behavior dataset\nthat provides multi-scale language supervision, including frame-level,\nsummary-level, and event-level semantic annotations. Experimental results\ndemonstrate that our method achieves superior performance on multiple datasets\nand offers a promising solution for risk warning in urban public safety\nscenarios.", "AI": {"tldr": "MA-CBP\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f02\u6b65\u534f\u4f5c\u9884\u6d4b\u72af\u7f6a\u884c\u4e3a\uff0c\u7ed3\u5408\u5b9e\u65f6\u89c6\u9891\u6d41\u548c\u5386\u53f2\u4fe1\u606f\uff0c\u5b9e\u73b0\u9ad8\u6548\u9884\u8b66\u3002", "motivation": "\u57ce\u5e02\u5316\u52a0\u901f\u5bfc\u81f4\u516c\u5171\u573a\u666f\u72af\u7f6a\u5a01\u80c1\u589e\u52a0\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u9ad8\u7ea7\u8bed\u4e49\u6216\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\u3002", "method": "\u5c06\u89c6\u9891\u6d41\u8f6c\u4e3a\u8bed\u4e49\u63cf\u8ff0\uff0c\u6784\u5efa\u56e0\u679c\u4e00\u81f4\u7684\u5386\u53f2\u6458\u8981\uff0c\u878d\u5408\u957f\u77ed\u4e0a\u4e0b\u6587\u8fdb\u884c\u8054\u5408\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u72af\u7f6a\u884c\u4e3a\u6570\u636e\u96c6\u3002", "conclusion": "MA-CBP\u4e3a\u57ce\u5e02\u516c\u5171\u5b89\u5168\u98ce\u9669\u9884\u8b66\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06191", "categories": ["cs.CV", "68T45, 92C55", "I.4.6; I.5.4; J.3"], "pdf": "https://arxiv.org/pdf/2508.06191", "abs": "https://arxiv.org/abs/2508.06191", "authors": ["Ruixiang Tang", "Jianglong Qin", "Mingda Zhang", "Yan Song", "Yi Wu", "Wei Wu"], "title": "A Semantic Segmentation Algorithm for Pleural Effusion Based on DBIF-AUNet", "comment": "12 pages, 6 figures, 2 tables", "summary": "Pleural effusion semantic segmentation can significantly enhance the accuracy\nand timeliness of clinical diagnosis and treatment by precisely identifying\ndisease severity and lesion areas. Currently, semantic segmentation of pleural\neffusion CT images faces multiple challenges. These include similar gray levels\nbetween effusion and surrounding tissues, blurred edges, and variable\nmorphology. Existing methods often struggle with diverse image variations and\ncomplex edges, primarily because direct feature concatenation causes semantic\ngaps. To address these challenges, we propose the Dual-Branch Interactive\nFusion Attention model (DBIF-AUNet). This model constructs a densely nested\nskip-connection network and innovatively refines the Dual-Domain Feature\nDisentanglement module (DDFD). The DDFD module orthogonally decouples the\nfunctions of dual-domain modules to achieve multi-scale feature complementarity\nand enhance characteristics at different levels. Concurrently, we design a\nBranch Interaction Attention Fusion module (BIAF) that works synergistically\nwith the DDFD. This module dynamically weights and fuses global, local, and\nfrequency band features, thereby improving segmentation robustness.\nFurthermore, we implement a nested deep supervision mechanism with hierarchical\nadaptive hybrid loss to effectively address class imbalance. Through validation\non 1,622 pleural effusion CT images from Southwest Hospital, DBIF-AUNet\nachieved IoU and Dice scores of 80.1% and 89.0% respectively. These results\noutperform state-of-the-art medical image segmentation models U-Net++ and\nSwin-UNet by 5.7%/2.7% and 2.2%/1.5% respectively, demonstrating significant\noptimization in segmentation accuracy for complex pleural effusion CT images.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDBIF-AUNet\u7684\u53cc\u5206\u652f\u4ea4\u4e92\u878d\u5408\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8\u80f8\u8154\u79ef\u6db2CT\u56fe\u50cf\u7684\u8bed\u4e49\u5206\u5272\u7cbe\u5ea6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u8fb9\u7f18\u548c\u591a\u53d8\u5f62\u6001\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u80f8\u8154\u79ef\u6db2CT\u56fe\u50cf\u7684\u8bed\u4e49\u5206\u5272\u9762\u4e34\u7070\u5ea6\u76f8\u4f3c\u3001\u8fb9\u7f18\u6a21\u7cca\u548c\u5f62\u6001\u591a\u53d8\u7b49\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u76f4\u63a5\u7279\u5f81\u62fc\u63a5\u5bfc\u81f4\u8bed\u4e49\u9e3f\u6c9f\uff0c\u96be\u4ee5\u5e94\u5bf9\u591a\u6837\u56fe\u50cf\u53d8\u5316\u3002", "method": "\u6a21\u578b\u91c7\u7528\u5bc6\u96c6\u5d4c\u5957\u8df3\u8dc3\u8fde\u63a5\u7f51\u7edc\uff0c\u521b\u65b0\u6027\u5730\u8bbe\u8ba1\u4e86\u53cc\u57df\u7279\u5f81\u89e3\u8026\u6a21\u5757\uff08DDFD\uff09\u548c\u5206\u652f\u4ea4\u4e92\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\uff08BIAF\uff09\uff0c\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u4e92\u8865\u548c\u52a8\u6001\u52a0\u6743\u878d\u5408\u3002", "result": "\u57281622\u5f20CT\u56fe\u50cf\u4e0a\u9a8c\u8bc1\uff0cIoU\u548cDice\u5206\u6570\u5206\u522b\u8fbe\u523080.1%\u548c89.0%\uff0c\u4f18\u4e8eU-Net++\u548cSwin-UNet\u3002", "conclusion": "DBIF-AUNet\u663e\u8457\u4f18\u5316\u4e86\u590d\u6742\u80f8\u8154\u79ef\u6db2CT\u56fe\u50cf\u7684\u5206\u5272\u7cbe\u5ea6\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u5de5\u5177\u3002"}}
{"id": "2508.06203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06203", "abs": "https://arxiv.org/abs/2508.06203", "authors": ["Zhaopeng Gu", "Bingke Zhu", "Guibo Zhu", "Yingying Chen", "Wei Ge", "Ming Tang", "Jinqiao Wang"], "title": "AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual Anomaly Detection", "comment": null, "summary": "Anomaly detection is a critical task across numerous domains and modalities,\nyet existing methods are often highly specialized, limiting their\ngeneralizability. These specialized models, tailored for specific anomaly types\nlike textural defects or logical errors, typically exhibit limited performance\nwhen deployed outside their designated contexts. To overcome this limitation,\nwe propose AnomalyMoE, a novel and universal anomaly detection framework based\non a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose the\ncomplex anomaly detection problem into three distinct semantic hierarchies:\nlocal structural anomalies, component-level semantic anomalies, and global\nlogical anomalies. AnomalyMoE correspondingly employs three dedicated expert\nnetworks at the patch, component, and global levels, and is specialized in\nreconstructing features and identifying deviations at its designated semantic\nlevel. This hierarchical design allows a single model to concurrently\nunderstand and detect a wide spectrum of anomalies. Furthermore, we introduce\nan Expert Information Repulsion (EIR) module to promote expert diversity and an\nExpert Selection Balancing (ESB) module to ensure the comprehensive utilization\nof all experts. Experiments on 8 challenging datasets spanning industrial\nimaging, 3D point clouds, medical imaging, video surveillance, and logical\nanomaly detection demonstrate that AnomalyMoE establishes new state-of-the-art\nperformance, significantly outperforming specialized methods in their\nrespective domains.", "AI": {"tldr": "AnomalyMoE\u662f\u4e00\u79cd\u57fa\u4e8eMixture-of-Experts\u67b6\u6784\u7684\u901a\u7528\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u8bbe\u8ba1\u548c\u4e13\u5bb6\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u8fc7\u4e8e\u4e13\u4e1a\u5316\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u901a\u7528\u6846\u67b6\u3002", "method": "AnomalyMoE\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u4e09\u4e2a\u8bed\u4e49\u5c42\u6b21\uff0c\u5e76\u5206\u522b\u4f7f\u7528\u4e13\u5bb6\u7f51\u7edc\u5904\u7406\uff0c\u540c\u65f6\u5f15\u5165EIR\u548cESB\u6a21\u5757\u4f18\u5316\u4e13\u5bb6\u591a\u6837\u6027\u53ca\u5229\u7528\u7387\u3002", "result": "\u57288\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cAnomalyMoE\u663e\u8457\u4f18\u4e8e\u9886\u57df\u4e13\u7528\u65b9\u6cd5\uff0c\u8fbe\u5230\u65b0SOTA\u3002", "conclusion": "AnomalyMoE\u901a\u8fc7\u5206\u5c42\u8bbe\u8ba1\u548c\u4e13\u5bb6\u6a21\u5757\u5b9e\u73b0\u4e86\u901a\u7528\u4e14\u9ad8\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u3002"}}
{"id": "2508.06205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06205", "abs": "https://arxiv.org/abs/2508.06205", "authors": ["Ruiyan Wang", "Lin Zuo", "Zonghao Lin", "Qiang Wang", "Zhengxue Cheng", "Rong Xie", "Jun Ling", "Li Song"], "title": "PA-HOI: A Physics-Aware Human and Object Interaction Dataset", "comment": null, "summary": "The Human-Object Interaction (HOI) task explores the dynamic interactions\nbetween humans and objects in physical environments, providing essential\nbiomechanical and cognitive-behavioral foundations for fields such as robotics,\nvirtual reality, and human-computer interaction. However, existing HOI data\nsets focus on details of affordance, often neglecting the influence of physical\nproperties of objects on human long-term motion. To bridge this gap, we\nintroduce the PA-HOI Motion Capture dataset, which highlights the impact of\nobjects' physical attributes on human motion dynamics, including human posture,\nmoving velocity, and other motion characteristics. The dataset comprises 562\nmotion sequences of human-object interactions, with each sequence performed by\nsubjects of different genders interacting with 35 3D objects that vary in size,\nshape, and weight. This dataset stands out by significantly extending the scope\nof existing ones for understanding how the physical attributes of different\nobjects influence human posture, speed, motion scale, and interacting\nstrategies. We further demonstrate the applicability of the PA-HOI dataset by\nintegrating it with existing motion generation methods, validating its capacity\nto transfer realistic physical awareness.", "AI": {"tldr": "PA-HOI\u6570\u636e\u96c6\u586b\u8865\u4e86\u73b0\u6709HOI\u6570\u636e\u96c6\u4e2d\u7269\u4f53\u7269\u7406\u5c5e\u6027\u5bf9\u4eba\u7c7b\u957f\u671f\u8fd0\u52a8\u5f71\u54cd\u7684\u7a7a\u767d\uff0c\u5305\u542b562\u4e2a\u8fd0\u52a8\u5e8f\u5217\uff0c\u5c55\u793a\u4e86\u7269\u4f53\u5c3a\u5bf8\u3001\u5f62\u72b6\u548c\u91cd\u91cf\u5bf9\u4eba\u7c7b\u8fd0\u52a8\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709HOI\u6570\u636e\u96c6\u5173\u6ce8\u529f\u80fd\u7ec6\u8282\uff0c\u4f46\u5ffd\u7565\u4e86\u7269\u4f53\u7269\u7406\u5c5e\u6027\u5bf9\u4eba\u7c7b\u957f\u671f\u8fd0\u52a8\u7684\u5f71\u54cd\u3002", "method": "\u5f15\u5165PA-HOI\u8fd0\u52a8\u6355\u6349\u6570\u636e\u96c6\uff0c\u5305\u542b562\u4e2a\u8fd0\u52a8\u5e8f\u5217\uff0c\u6d89\u53ca\u4e0d\u540c\u6027\u522b\u53d7\u8bd5\u8005\u4e0e35\u4e2a3D\u7269\u4f53\u7684\u4ea4\u4e92\u3002", "result": "\u6570\u636e\u96c6\u6269\u5c55\u4e86\u73b0\u6709\u8303\u56f4\uff0c\u5c55\u793a\u4e86\u7269\u4f53\u7269\u7406\u5c5e\u6027\u5bf9\u4eba\u7c7b\u59ff\u52bf\u3001\u901f\u5ea6\u3001\u8fd0\u52a8\u89c4\u6a21\u548c\u4ea4\u4e92\u7b56\u7565\u7684\u5f71\u54cd\u3002", "conclusion": "PA-HOI\u6570\u636e\u96c6\u80fd\u591f\u4e0e\u73b0\u6709\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u7ed3\u5408\uff0c\u9a8c\u8bc1\u5176\u4f20\u9012\u771f\u5b9e\u7269\u7406\u611f\u77e5\u7684\u80fd\u529b\u3002"}}
{"id": "2508.06218", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06218", "abs": "https://arxiv.org/abs/2508.06218", "authors": ["Zhiyan Bo", "Laura C. Coates", "Bartlomiej W. Papiez"], "title": "Interpretable Rheumatoid Arthritis Scoring via Anatomy-aware Multiple Instance Learning", "comment": "Accepted by MICCAI AMAI Workshop 2025", "summary": "The Sharp/van der Heijde (SvdH) score has been widely used in clinical trials\nto quantify radiographic damage in Rheumatoid Arthritis (RA), but its\ncomplexity has limited its adoption in routine clinical practice. To address\nthe inefficiency of manual scoring, this work proposes a two-stage pipeline for\ninterpretable image-level SvdH score prediction using dual-hand radiographs.\nOur approach extracts disease-relevant image regions and integrates them using\nattention-based multiple instance learning to generate image-level features for\nprediction. We propose two region extraction schemes: 1) sampling image tiles\nmost likely to contain abnormalities, and 2) cropping patches containing\ndisease-relevant joints. With Scheme 2, our best individual score prediction\nmodel achieved a Pearson's correlation coefficient (PCC) of 0.943 and a root\nmean squared error (RMSE) of 15.73. Ensemble learning further boosted\nprediction accuracy, yielding a PCC of 0.945 and RMSE of 15.57, achieving\nstate-of-the-art performance that is comparable to that of experienced\nradiologists (PCC = 0.97, RMSE = 18.75). Finally, our pipeline effectively\nidentified and made decisions based on anatomical structures which clinicians\nconsider relevant to RA progression.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u624bX\u5149\u7247\u7684\u53ef\u89e3\u91ca\u56fe\u50cf\u7ea7SvdH\u8bc4\u5206\u9884\u6d4b\u7684\u4e24\u9636\u6bb5\u6d41\u7a0b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u5b9e\u4f8b\u5b66\u4e60\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u8fbe\u5230\u4e0e\u7ecf\u9a8c\u4e30\u5bcc\u7684\u653e\u5c04\u79d1\u533b\u751f\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "SvdH\u8bc4\u5206\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u56e0\u590d\u6742\u6027\u672a\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u624b\u52a8\u8bc4\u5206\u6548\u7387\u4f4e\uff0c\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a1\uff09\u63d0\u53d6\u75be\u75c5\u76f8\u5173\u56fe\u50cf\u533a\u57df\uff1b2\uff09\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u5b9e\u4f8b\u5b66\u4e60\u6574\u5408\u533a\u57df\u7279\u5f81\u8fdb\u884c\u9884\u6d4b\u3002\u63d0\u51fa\u4e24\u79cd\u533a\u57df\u63d0\u53d6\u65b9\u6848\uff1a\u91c7\u6837\u5f02\u5e38\u533a\u57df\u6216\u88c1\u526a\u5173\u8282\u533a\u57df\u3002", "result": "\u6700\u4f73\u6a21\u578bPCC\u4e3a0.943\uff0cRMSE\u4e3a15.73\uff1b\u96c6\u6210\u5b66\u4e60\u540ePCC\u63d0\u5347\u81f30.945\uff0cRMSE\u964d\u81f315.57\uff0c\u63a5\u8fd1\u653e\u5c04\u79d1\u533b\u751f\u6c34\u5e73\uff08PCC=0.97\uff0cRMSE=18.75\uff09\u3002", "conclusion": "\u8be5\u6d41\u7a0b\u80fd\u6709\u6548\u8bc6\u522b\u4e34\u5e8a\u76f8\u5173\u7684\u89e3\u5256\u7ed3\u6784\uff0c\u4e3aRA\u8fdb\u5c55\u63d0\u4f9b\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2508.06224", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06224", "abs": "https://arxiv.org/abs/2508.06224", "authors": ["Guoyu Zhou", "Jing Zhang", "Yi Yan", "Hui Zhang", "Li Zhuo"], "title": "TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images", "comment": "Submitted to GRSL", "summary": "Semantic segmentation of urban remote sensing images (URSIs) is crucial for\napplications such as urban planning and environmental monitoring. However,\ngeospatial objects often exhibit subtle texture differences and similar spatial\nstructures, which can easily lead to semantic ambiguity and misclassification.\nMoreover, challenges such as irregular object shapes, blurred boundaries, and\noverlapping spatial distributions of semantic objects contribute to complex and\ndiverse edge morphologies, further complicating accurate segmentation. To\ntackle these issues, we propose a texture-aware and edge-guided Transformer\n(TEFormer) that integrates texture awareness and edge-guidance mechanisms for\nsemantic segmentation of URSIs. In the encoder, a texture-aware module (TaM) is\ndesigned to capture fine-grained texture differences between visually similar\ncategories to enhance semantic discrimination. Then, an edge-guided tri-branch\ndecoder (Eg3Head) is constructed to preserve local edges and details for\nmultiscale context-awareness. Finally, an edge-guided feature fusion module\n(EgFFM) is to fuse contextual and detail information with edge information to\nrealize refined semantic segmentation. Extensive experiments show that TEFormer\nachieves mIoU of 88.57%, 81.46%, and 53.55% on the Potsdam, Vaihingen, and\nLoveDA datasets, respectively, shows the effectiveness in URSI semantic\nsegmentation.", "AI": {"tldr": "TEFormer\u662f\u4e00\u79cd\u7ed3\u5408\u7eb9\u7406\u611f\u77e5\u548c\u8fb9\u7f18\u5f15\u5bfc\u673a\u5236\u7684Transformer\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u57ce\u5e02\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u8bed\u4e49\u6a21\u7cca\u548c\u8fb9\u7f18\u590d\u6742\u6027\u95ee\u9898\u3002", "motivation": "\u57ce\u5e02\u9065\u611f\u56fe\u50cf\u4e2d\u5730\u7406\u5bf9\u8c61\u7684\u7ec6\u5fae\u7eb9\u7406\u5dee\u5f02\u548c\u76f8\u4f3c\u7a7a\u95f4\u7ed3\u6784\u6613\u5bfc\u81f4\u8bed\u4e49\u6a21\u7cca\u548c\u8bef\u5206\u7c7b\uff0c\u540c\u65f6\u4e0d\u89c4\u5219\u5f62\u72b6\u548c\u6a21\u7cca\u8fb9\u754c\u589e\u52a0\u4e86\u5206\u5272\u96be\u5ea6\u3002", "method": "\u63d0\u51faTEFormer\u6a21\u578b\uff0c\u5305\u542b\u7eb9\u7406\u611f\u77e5\u6a21\u5757\uff08TaM\uff09\u589e\u5f3a\u8bed\u4e49\u533a\u5206\uff0c\u8fb9\u7f18\u5f15\u5bfc\u4e09\u5206\u652f\u89e3\u7801\u5668\uff08Eg3Head\uff09\u4fdd\u7559\u5c40\u90e8\u8fb9\u7f18\u7ec6\u8282\uff0c\u4ee5\u53ca\u8fb9\u7f18\u5f15\u5bfc\u7279\u5f81\u878d\u5408\u6a21\u5757\uff08EgFFM\uff09\u5b9e\u73b0\u7cbe\u7ec6\u5206\u5272\u3002", "result": "\u5728Potsdam\u3001Vaihingen\u548cLoveDA\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523088.57%\u300181.46%\u548c53.55%\u7684mIoU\u3002", "conclusion": "TEFormer\u901a\u8fc7\u7eb9\u7406\u548c\u8fb9\u7f18\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u57ce\u5e02\u9065\u611f\u56fe\u50cf\u7684\u8bed\u4e49\u5206\u5272\u7cbe\u5ea6\u3002"}}
{"id": "2508.06227", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06227", "abs": "https://arxiv.org/abs/2508.06227", "authors": ["Md Sazidur Rahman", "David Cabecinhas", "Ricard Marxer"], "title": "Depth Jitter: Seeing through the Depth", "comment": null, "summary": "Depth information is essential in computer vision, particularly in underwater\nimaging, robotics, and autonomous navigation. However, conventional\naugmentation techniques overlook depth aware transformations, limiting model\nrobustness in real world depth variations. In this paper, we introduce\nDepth-Jitter, a novel depth-based augmentation technique that simulates natural\ndepth variations to improve generalization. Our approach applies adaptive depth\noffsetting, guided by depth variance thresholds, to generate synthetic depth\nperturbations while preserving structural integrity. We evaluate Depth-Jitter\non two benchmark datasets, FathomNet and UTDAC2020 demonstrating its impact on\nmodel stability under diverse depth conditions. Extensive experiments compare\nDepth-Jitter against traditional augmentation strategies such as ColorJitter,\nanalyzing performance across varying learning rates, encoders, and loss\nfunctions. While Depth-Jitter does not always outperform conventional methods\nin absolute performance, it consistently enhances model stability and\ngeneralization in depth-sensitive environments. These findings highlight the\npotential of depth-aware augmentation for real-world applications and provide a\nfoundation for further research into depth-based learning strategies. The\nproposed technique is publicly available to support advancements in depth-aware\naugmentation. The code is publicly available on\n\\href{https://github.com/mim-team/Depth-Jitter}{github}.", "AI": {"tldr": "Depth-Jitter\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u901a\u8fc7\u6a21\u62df\u81ea\u7136\u6df1\u5ea6\u53d8\u5316\u63d0\u5347\u6a21\u578b\u5728\u6df1\u5ea6\u654f\u611f\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u6280\u672f\u5ffd\u7565\u4e86\u6df1\u5ea6\u611f\u77e5\u53d8\u6362\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u6df1\u5ea6\u53d8\u5316\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "Depth-Jitter\u901a\u8fc7\u81ea\u9002\u5e94\u6df1\u5ea6\u504f\u79fb\uff08\u57fa\u4e8e\u6df1\u5ea6\u65b9\u5dee\u9608\u503c\uff09\u751f\u6210\u5408\u6210\u6df1\u5ea6\u6270\u52a8\uff0c\u540c\u65f6\u4fdd\u6301\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "result": "\u5728FathomNet\u548cUTDAC2020\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86Depth-Jitter\u5bf9\u6a21\u578b\u7a33\u5b9a\u6027\u7684\u63d0\u5347\uff0c\u5c24\u5176\u5728\u6df1\u5ea6\u654f\u611f\u73af\u5883\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "Depth-Jitter\u4e3a\u6df1\u5ea6\u611f\u77e5\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u6df1\u5ea6\u5b66\u4e60\u7b56\u7565\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.06228", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06228", "abs": "https://arxiv.org/abs/2508.06228", "authors": ["Daniel Feijoo", "Paula Garrido-Mellado", "Jaesung Rim", "Alvaro Garcia", "Marcos V. Conde"], "title": "Towards Unified Image Deblurring using a Mixture-of-Experts Decoder", "comment": "Preprint. Under review", "summary": "Image deblurring, removing blurring artifacts from images, is a fundamental\ntask in computational photography and low-level computer vision. Existing\napproaches focus on specialized solutions tailored to particular blur types,\nthus, these solutions lack generalization. This limitation in current methods\nimplies requiring multiple models to cover several blur types, which is not\npractical in many real scenarios. In this paper, we introduce the first\nall-in-one deblurring method capable of efficiently restoring images affected\nby diverse blur degradations, including global motion, local motion, blur in\nlow-light conditions, and defocus blur. We propose a mixture-of-experts (MoE)\ndecoding module, which dynamically routes image features based on the\nrecognized blur degradation, enabling precise and efficient restoration in an\nend-to-end manner. Our unified approach not only achieves performance\ncomparable to dedicated task-specific models, but also demonstrates remarkable\nrobustness and generalization capabilities on unseen blur degradation\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u56fe\u50cf\u53bb\u6a21\u7cca\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u591a\u79cd\u6a21\u7cca\u7c7b\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u6a21\u5757\u52a8\u6001\u8def\u7531\u7279\u5f81\uff0c\u5b9e\u73b0\u9ad8\u6548\u6062\u590d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9488\u5bf9\u7279\u5b9a\u6a21\u7cca\u7c7b\u578b\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u591a\u4e2a\u6a21\u578b\u5904\u7406\u4e0d\u540c\u6a21\u7cca\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u89e3\u7801\u6a21\u5757\uff0c\u6839\u636e\u6a21\u7cca\u7c7b\u578b\u52a8\u6001\u8def\u7531\u56fe\u50cf\u7279\u5f81\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u6062\u590d\u3002", "result": "\u7edf\u4e00\u65b9\u6cd5\u6027\u80fd\u4e0e\u4e13\u7528\u6a21\u578b\u76f8\u5f53\uff0c\u4e14\u5728\u672a\u89c1\u6a21\u7cca\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u5168\u573a\u666f\u53bb\u6a21\u7cca\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06248", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06248", "abs": "https://arxiv.org/abs/2508.06248", "authors": ["Andrii Yermakov", "Jan Cech", "Jiri Matas", "Mario Fritz"], "title": "Deepfake Detection that Generalizes Across Benchmarks", "comment": null, "summary": "The generalization of deepfake detectors to unseen manipulation techniques\nremains a challenge for practical deployment. Although many approaches adapt\nfoundation models by introducing significant architectural complexity, this\nwork demonstrates that robust generalization is achievable through a\nparameter-efficient adaptation of a pre-trained CLIP vision encoder. The\nproposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters\n(0.03% of the total) and enhances generalization by enforcing a hyperspherical\nfeature manifold using L2 normalization and latent space augmentations.\n  We conducted an extensive evaluation on 13 benchmark datasets spanning from\n2019 to 2025. The proposed method achieves state-of-the-art performance,\noutperforming more complex, recent approaches in average cross-dataset AUROC.\nOur analysis yields two primary findings for the field: 1) training on paired\nreal-fake data from the same source video is essential for mitigating shortcut\nlearning and improving generalization, and 2) detection difficulty on academic\ndatasets has not strictly increased over time, with models trained on older,\ndiverse datasets showing strong generalization capabilities.\n  This work delivers a computationally efficient and reproducible method,\nproving that state-of-the-art generalization is attainable by making targeted,\nminimal changes to a pre-trained CLIP model. The code will be made publicly\navailable upon acceptance.", "AI": {"tldr": "LNCLIP-DF\u901a\u8fc7\u5fae\u8c03CLIP\u89c6\u89c9\u7f16\u7801\u5668\u7684Layer Normalization\u53c2\u6570\uff08\u4ec50.03%\uff09\uff0c\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u8fc7\u7684\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7684\u9c81\u68d2\u6cdb\u5316\uff0c\u5e76\u572813\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u5bf9\u672a\u89c1\u8fc7\u7684\u4f2a\u9020\u6280\u672f\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u907f\u514d\u590d\u6742\u7684\u67b6\u6784\u6539\u52a8\u3002", "method": "\u4ec5\u5fae\u8c03Layer Normalization\u53c2\u6570\uff0c\u7ed3\u5408L2\u5f52\u4e00\u5316\u548c\u6f5c\u5728\u7a7a\u95f4\u589e\u5f3a\uff0c\u4f18\u5316\u7279\u5f81\u6d41\u5f62\u3002", "result": "\u572813\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f18\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u6700\u5c0f\u5316\u6539\u52a8\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u5b9e\u73b0\u6700\u4f73\u6cdb\u5316\u3002", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u6027\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u53ef\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u590d\u73b0\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2508.06256", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06256", "abs": "https://arxiv.org/abs/2508.06256", "authors": ["Bar\u0131\u015f B\u00fcy\u00fckta\u015f", "Jonas Klotz", "Beg\u00fcm Demir"], "title": "FedX: Explanation-Guided Pruning for Communication-Efficient Federated Learning in Remote Sensing", "comment": null, "summary": "Federated learning (FL) enables the collaborative training of deep neural\nnetworks across decentralized data archives (i.e., clients), where each client\nstores data locally and only shares model updates with a central server. This\nmakes FL a suitable learning paradigm for remote sensing (RS) image\nclassification tasks, where data centralization may be restricted due to legal\nand privacy constraints. However, a key challenge in applying FL to RS tasks is\nthe communication overhead caused by the frequent exchange of large model\nupdates between clients and the central server. To address this issue, in this\npaper we propose a novel strategy (denoted as FedX) that uses\nexplanation-guided pruning to reduce communication overhead by minimizing the\nsize of the transmitted models without compromising performance. FedX leverages\nbackpropagation-based explanation methods to estimate the task-specific\nimportance of model components and prunes the least relevant ones at the\ncentral server. The resulting sparse global model is then sent to clients,\nsubstantially reducing communication overhead. We evaluate FedX on multi-label\nscene classification using the BigEarthNet-S2 dataset and single-label scene\nclassification using the EuroSAT dataset. Experimental results show the success\nof FedX in significantly reducing the number of shared model parameters while\nenhancing the generalization capability of the global model, compared to both\nunpruned model and state-of-the-art pruning methods. The code of FedX will be\navailable at https://git.tu-berlin.de/rsim/FedX.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedX\u7684\u65b0\u7b56\u7565\uff0c\u901a\u8fc7\u89e3\u91ca\u5f15\u5bfc\u7684\u526a\u679d\u51cf\u5c11\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u9065\u611f\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u901a\u4fe1\u5f00\u9500\u5927\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "FedX\u5229\u7528\u53cd\u5411\u4f20\u64ad\u89e3\u91ca\u65b9\u6cd5\u8bc4\u4f30\u6a21\u578b\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\uff0c\u526a\u679d\u6700\u4e0d\u76f8\u5173\u7684\u90e8\u5206\u4ee5\u51cf\u5c11\u901a\u4fe1\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFedX\u663e\u8457\u51cf\u5c11\u4e86\u5171\u4eab\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u5168\u5c40\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "FedX\u662f\u4e00\u79cd\u6709\u6548\u7684\u901a\u4fe1\u4f18\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u3002"}}
{"id": "2508.06258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06258", "abs": "https://arxiv.org/abs/2508.06258", "authors": ["Byunghyun Ko", "Anning Tian", "Jeongkyu Lee"], "title": "XAG-Net: A Cross-Slice Attention and Skip Gating Network for 2.5D Femur MRI Segmentation", "comment": "Accepted at the 2025 International Conference on Artificial\n  Intelligence, Computer, Data Sciences and Applications (ACDSA). This is the\n  preprint version of the paper", "summary": "Accurate segmentation of femur structures from Magnetic Resonance Imaging\n(MRI) is critical for orthopedic diagnosis and surgical planning but remains\nchallenging due to the limitations of existing 2D and 3D deep learning-based\nsegmentation approaches. In this study, we propose XAG-Net, a novel 2.5D\nU-Net-based architecture that incorporates pixel-wise cross-slice attention\n(CSA) and skip attention gating (AG) mechanisms to enhance inter-slice\ncontextual modeling and intra-slice feature refinement. Unlike previous\nCSA-based models, XAG-Net applies pixel-wise softmax attention across adjacent\nslices at each spatial location for fine-grained inter-slice modeling.\nExtensive evaluations demonstrate that XAG-Net surpasses baseline 2D, 2.5D, and\n3D U-Net models in femur segmentation accuracy while maintaining computational\nefficiency. Ablation studies further validate the critical role of the CSA and\nAG modules, establishing XAG-Net as a promising framework for efficient and\naccurate femur MRI segmentation.", "AI": {"tldr": "XAG-Net\u662f\u4e00\u79cd\u65b0\u578b2.5D U-Net\u67b6\u6784\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u8de8\u5207\u7247\u6ce8\u610f\u529b\u548c\u8df3\u8dc3\u6ce8\u610f\u529b\u95e8\u63a7\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u80a1\u9aa8MRI\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u67092D\u548c3D\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u80a1\u9aa8MRI\u5206\u5272\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51faXAG-Net\uff0c\u7ed3\u5408\u50cf\u7d20\u7ea7\u8de8\u5207\u7247\u6ce8\u610f\u529b\uff08CSA\uff09\u548c\u8df3\u8dc3\u6ce8\u610f\u529b\u95e8\u63a7\uff08AG\uff09\u673a\u5236\uff0c\u4f18\u5316\u5207\u7247\u95f4\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u5207\u7247\u5185\u7279\u5f81\u7ec6\u5316\u3002", "result": "XAG-Net\u5728\u80a1\u9aa8\u5206\u5272\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u57fa\u7ebf2D\u30012.5D\u548c3D U-Net\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "XAG-Net\u901a\u8fc7CSA\u548cAG\u6a21\u5757\u7684\u534f\u540c\u4f5c\u7528\uff0c\u6210\u4e3a\u9ad8\u6548\u51c6\u786e\u7684\u80a1\u9aa8MRI\u5206\u5272\u6846\u67b6\u3002"}}
{"id": "2508.06317", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06317", "abs": "https://arxiv.org/abs/2508.06317", "authors": ["Jian Hu", "Zixu Cheng", "Shaogang Gong", "Isabel Guan", "Jianye Hao", "Jun Wang", "Kun Shao"], "title": "Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding", "comment": null, "summary": "Video Temporal Grounding (TG) aims to temporally locate video segments\nmatching a natural language description (a query) in a long video. While\nVision-Language Models (VLMs) are effective at holistic semantic matching, they\noften struggle with fine-grained temporal localisation. Recently, Group\nRelative Policy Optimisation (GRPO) reformulates the inference process as a\nreinforcement learning task, enabling fine-grained grounding and achieving\nstrong in-domain performance. However, GRPO relies on labelled data, making it\nunsuitable in unlabelled domains. Moreover, because videos are large and\nexpensive to store and process, performing full-scale adaptation introduces\nprohibitive latency and computational overhead, making it impractical for\nreal-time deployment. To overcome both problems, we introduce a Data-Efficient\nUnlabelled Cross-domain Temporal Grounding method, from which a model is first\ntrained on a labelled source domain, then adapted to a target domain using only\na small number of unlabelled videos from the target domain. This approach\neliminates the need for target annotation and keeps both computational and\nstorage overhead low enough to run in real time. Specifically, we introduce.\nUncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain\nknowledge transfer in learning video temporal grounding without target labels.\nURPA generates multiple candidate predictions using GRPO rollouts, averages\nthem to form a pseudo label, and estimates confidence from the variance across\nthese rollouts. This confidence then weights the training rewards, guiding the\nmodel to focus on reliable supervision. Experiments on three datasets across\nsix cross-domain settings show that URPA generalises well using only a few\nunlabelled target videos. Codes will be released once published.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u65e0\u6807\u6ce8\u8de8\u57df\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u65b9\u6cd5URPA\uff0c\u901a\u8fc7\u5c11\u91cf\u76ee\u6807\u57df\u65e0\u6807\u6ce8\u89c6\u9891\u5b9e\u73b0\u8de8\u57df\u77e5\u8bc6\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5GRPO\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u4e14\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u96be\u4ee5\u5728\u65e0\u6807\u6ce8\u57df\u548c\u5b9e\u65f6\u573a\u666f\u4e2d\u5e94\u7528\u3002", "method": "\u4f7f\u7528GRPO\u751f\u6210\u591a\u4e2a\u5019\u9009\u9884\u6d4b\uff0c\u901a\u8fc7\u5e73\u5747\u548c\u65b9\u5dee\u4f30\u8ba1\u7f6e\u4fe1\u5ea6\uff0c\u52a0\u6743\u8bad\u7ec3\u5956\u52b1\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u548c\u516d\u79cd\u8de8\u57df\u8bbe\u7f6e\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "URPA\u5728\u5c11\u91cf\u65e0\u6807\u6ce8\u76ee\u6807\u89c6\u9891\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u8de8\u57df\u65f6\u95f4\u5b9a\u4f4d\u3002"}}
{"id": "2508.06327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06327", "abs": "https://arxiv.org/abs/2508.06327", "authors": ["Xin Ci Wong", "Duygu Sarikaya", "Kieran Zucker", "Marc De Kamps", "Nishant Ravikumar"], "title": "Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?", "comment": "ICONIP 2025", "summary": "Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain\nshift due to variations in imaging devices and acquisition protocols. This\nchallenge limits the deployment of trained AI models in real-world scenarios,\nwhere performance degrades on unseen domains. Traditional solutions involve\nincreasing the size of the dataset through ad-hoc image augmentation or\nadditional online training/transfer learning, which have several limitations.\nSynthetic data offers a promising alternative, but anatomical/structural\nconsistency constraints limit the effectiveness of generative models in\ncreating image-label pairs. To address this, we propose a diffusion model (DM)\ntrained on a source domain that generates synthetic cardiac MR images that\nresemble a given reference. The synthetic data maintains spatial and structural\nfidelity, ensuring similarity to the source domain and compatibility with the\nsegmentation mask. We assess the utility of our generative approach in\nmulti-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and\nvanilla U-Net segmentation networks. We explore domain generalisation, where,\ndomain-invariant segmentation models are trained on synthetic source domain\ndata, and domain adaptation, where, we shift target domain data towards the\nsource domain using the DM. Both strategies significantly improved segmentation\nperformance on data from an unseen target domain, in terms of surface-based\nmetrics (Welch's t-test, p < 0.01), compared to training segmentation models on\nreal data alone. The proposed method ameliorates the need for transfer learning\nor online training to address domain shift challenges in cardiac MR image\nanalysis, especially useful in data-scarce settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5fc3\u810fMR\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u591a\u4e2d\u5fc3\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u5fc3\u810fMR\u56fe\u50cf\u56e0\u8bbe\u5907\u548c\u534f\u8bae\u5dee\u5f02\u5bfc\u81f4\u57df\u504f\u79fb\uff0c\u5f71\u54cdAI\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002\u4f20\u7edf\u65b9\u6cd5\u5982\u6570\u636e\u589e\u5f3a\u6216\u8fc1\u79fb\u5b66\u4e60\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u4e0e\u53c2\u8003\u56fe\u50cf\u76f8\u4f3c\u7684\u5408\u6210\u5fc3\u810fMR\u56fe\u50cf\uff0c\u4fdd\u6301\u7a7a\u95f4\u548c\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u7528\u4e8e\u57df\u6cdb\u5316\u548c\u57df\u9002\u5e94\u3002", "result": "\u5728\u672a\u89c1\u8fc7\u7684\u76ee\u6807\u57df\u4e0a\uff0c\u5206\u5272\u6027\u80fd\u663e\u8457\u63d0\u5347\uff08Welch's t-test, p < 0.01\uff09\uff0c\u4f18\u4e8e\u4ec5\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u57df\u504f\u79fb\u95ee\u9898\uff0c\u51cf\u5c11\u4e86\u5bf9\u8fc1\u79fb\u5b66\u4e60\u6216\u5728\u7ebf\u8bad\u7ec3\u7684\u4f9d\u8d56\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u573a\u666f\u3002"}}
{"id": "2508.06335", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06335", "abs": "https://arxiv.org/abs/2508.06335", "authors": ["Patrick Takenaka", "Johannes Maucher", "Marco F. Huber"], "title": "ViPro-2: Unsupervised State Estimation via Integrated Dynamics for Guiding Video Prediction", "comment": "Published in 2025 International Joint Conference on Neural Networks\n  (IJCNN)", "summary": "Predicting future video frames is a challenging task with many downstream\napplications. Previous work has shown that procedural knowledge enables deep\nmodels for complex dynamical settings, however their model ViPro assumed a\ngiven ground truth initial symbolic state. We show that this approach led to\nthe model learning a shortcut that does not actually connect the observed\nenvironment with the predicted symbolic state, resulting in the inability to\nestimate states given an observation if previous states are noisy. In this\nwork, we add several improvements to ViPro that enables the model to correctly\ninfer states from observations without providing a full ground truth state in\nthe beginning. We show that this is possible in an unsupervised manner, and\nextend the original Orbits dataset with a 3D variant to close the gap to real\nworld scenarios.", "AI": {"tldr": "\u6539\u8fdbViPro\u6a21\u578b\u4ee5\u65e0\u76d1\u7763\u65b9\u5f0f\u4ece\u89c2\u6d4b\u4e2d\u6b63\u786e\u63a8\u65ad\u72b6\u6001\uff0c\u65e0\u9700\u521d\u59cb\u771f\u5b9e\u72b6\u6001\uff0c\u5e76\u6269\u5c55\u6570\u636e\u96c6\u4ee5\u9002\u5e94\u73b0\u5b9e\u573a\u666f\u3002", "motivation": "\u89e3\u51b3ViPro\u6a21\u578b\u56e0\u4f9d\u8d56\u521d\u59cb\u771f\u5b9e\u72b6\u6001\u800c\u65e0\u6cd5\u5728\u566a\u58f0\u73af\u5883\u4e0b\u51c6\u786e\u63a8\u65ad\u72b6\u6001\u7684\u95ee\u9898\u3002", "method": "\u5728ViPro\u57fa\u7840\u4e0a\u6dfb\u52a0\u6539\u8fdb\uff0c\u4f7f\u5176\u80fd\u65e0\u76d1\u7763\u5730\u4ece\u89c2\u6d4b\u4e2d\u63a8\u65ad\u72b6\u6001\uff0c\u5e76\u6269\u5c553D\u6570\u636e\u96c6\u3002", "result": "\u6a21\u578b\u80fd\u591f\u5728\u4e0d\u63d0\u4f9b\u521d\u59cb\u771f\u5b9e\u72b6\u6001\u7684\u60c5\u51b5\u4e0b\u6b63\u786e\u63a8\u65ad\u72b6\u6001\uff0c\u4e14\u9002\u7528\u4e8e\u66f4\u590d\u6742\u76843D\u573a\u666f\u3002", "conclusion": "\u6539\u8fdb\u540e\u7684ViPro\u6a21\u578b\u5728\u65e0\u76d1\u7763\u72b6\u6001\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u9002\u7528\u4e8e\u66f4\u63a5\u8fd1\u73b0\u5b9e\u7684\u573a\u666f\u3002"}}
{"id": "2508.06342", "categories": ["cs.CV", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.06342", "abs": "https://arxiv.org/abs/2508.06342", "authors": ["Kieran Elrod", "Katherine Flanigan", "Mario Berg\u00e9s"], "title": "Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities", "comment": null, "summary": "Designing socially active streets has long been a goal of urban planning, yet\nexisting quantitative research largely measures pedestrian volume rather than\nthe quality of social interactions. We hypothesize that street view imagery --\nan inexpensive data source with global coverage -- contains latent social\ninformation that can be extracted and interpreted through established social\nscience theory. As a proof of concept, we analyzed 2,998 street view images\nfrom 15 cities using a multimodal large language model guided by Mehta's\ntaxonomy of passive, fleeting, and enduring sociability -- one illustrative\nexample of a theory grounded in urban design that could be substituted or\ncomplemented by other sociological frameworks. We then used linear regression\nmodels, controlling for factors like weather, time of day, and pedestrian\ncounts, to test whether the inferred sociability measures correlate with\ncity-level place attachment scores from the World Values Survey and with\nenvironmental predictors (e.g., green, sky, and water view indices) derived\nfrom individual street view images. Results aligned with long-standing urban\nplanning theory: the sky view index was associated with all three sociability\ntypes, the green view index predicted enduring sociability, and place\nattachment was positively associated with fleeting sociability. These results\nprovide preliminary evidence that street view images can be used to infer\nrelationships between specific types of social interactions and built\nenvironment variables. Further research could establish street view imagery as\na scalable, privacy-preserving tool for studying urban sociability, enabling\ncross-cultural theory testing and evidence-based design of socially vibrant\ncities.", "AI": {"tldr": "\u5229\u7528\u8857\u666f\u56fe\u50cf\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5206\u6790\u57ce\u5e02\u793e\u4ea4\u6027\uff0c\u9a8c\u8bc1\u4e86\u8857\u666f\u6570\u636e\u53ef\u7528\u4e8e\u7814\u7a76\u793e\u4ea4\u4e92\u52a8\u4e0e\u5efa\u7b51\u73af\u5883\u7684\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u884c\u4eba\u6570\u91cf\u800c\u975e\u793e\u4ea4\u4e92\u52a8\u8d28\u91cf\uff0c\u8857\u666f\u56fe\u50cf\u4f5c\u4e3a\u4f4e\u6210\u672c\u3001\u5168\u7403\u8986\u76d6\u7684\u6570\u636e\u6e90\u53ef\u80fd\u8574\u542b\u6f5c\u5728\u793e\u4ea4\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5206\u67902998\u5f20\u8857\u666f\u56fe\u50cf\uff0c\u7ed3\u5408Mehta\u7684\u793e\u4ea4\u6027\u5206\u7c7b\u7406\u8bba\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u63a7\u5236\u5176\u4ed6\u53d8\u91cf\u3002", "result": "\u5929\u7a7a\u89c6\u91ce\u6307\u6570\u4e0e\u4e09\u79cd\u793e\u4ea4\u6027\u76f8\u5173\uff0c\u7eff\u8272\u89c6\u91ce\u6307\u6570\u9884\u6d4b\u6301\u4e45\u793e\u4ea4\u6027\uff0c\u5730\u65b9\u4f9d\u604b\u4e0e\u77ed\u6682\u793e\u4ea4\u6027\u6b63\u76f8\u5173\u3002", "conclusion": "\u8857\u666f\u56fe\u50cf\u53ef\u4f5c\u4e3a\u7814\u7a76\u57ce\u5e02\u793e\u4ea4\u6027\u7684\u5de5\u5177\uff0c\u652f\u6301\u8de8\u6587\u5316\u7406\u8bba\u9a8c\u8bc1\u548c\u57fa\u4e8e\u8bc1\u636e\u7684\u57ce\u5e02\u8bbe\u8ba1\u3002"}}
{"id": "2508.06350", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06350", "abs": "https://arxiv.org/abs/2508.06350", "authors": ["Yingxian Chen", "Jiahui Liu", "Ruifan Di", "Yanwei Li", "Chirui Chang", "Shizhen Zhao", "Wilton W. T. Fok", "Xiaojuan Qi", "Yik-Chung Wu"], "title": "Aligning Effective Tokens with Video Anomaly in Large Language Models", "comment": null, "summary": "Understanding abnormal events in videos is a vital and challenging task that\nhas garnered significant attention in a wide range of applications. Although\ncurrent video understanding Multi-modal Large Language Models (MLLMs) are\ncapable of analyzing general videos, they often struggle to handle anomalies\ndue to the spatial and temporal sparsity of abnormal events, where the\nredundant information always leads to suboptimal outcomes. To address these\nchallenges, exploiting the representation and generalization capabilities of\nVison Language Models (VLMs) and Large Language Models (LLMs), we propose\nVA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in\nvarious videos. Our approach efficiently aligns effective tokens between visual\nencoders and LLMs through two key proposed modules: Spatial Effective Token\nSelection (SETS) and Temporal Effective Token Generation (TETG). These modules\nenable our model to effectively capture and analyze both spatial and temporal\ninformation associated with abnormal events, resulting in more accurate\nresponses and interactions. Furthermore, we construct an instruction-following\ndataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a\ncross-domain evaluation benchmark based on XD-Violence dataset. Our proposed\nmethod outperforms existing state-of-the-art methods on various benchmarks.", "AI": {"tldr": "VA-GPT\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u89c6\u9891\u4e2d\u5f02\u5e38\u4e8b\u4ef6\u7684\u603b\u7ed3\u548c\u5b9a\u4f4d\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u65f6\u95f4\u6709\u6548\u4ee4\u724c\u9009\u62e9\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7406\u89e3\u6a21\u578b\u5728\u5904\u7406\u5f02\u5e38\u4e8b\u4ef6\u65f6\u56e0\u4fe1\u606f\u5197\u4f59\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faSETS\u548cTETG\u6a21\u5757\uff0c\u4f18\u5316\u89c6\u89c9\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\uff0c\u5e76\u6784\u5efa\u4e13\u7528\u6570\u636e\u96c6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "VA-GPT\u901a\u8fc7\u6709\u6548\u4ee4\u724c\u9009\u62e9\u548c\u8de8\u57df\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u4e8b\u4ef6\u5206\u6790\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.06351", "categories": ["cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.06351", "abs": "https://arxiv.org/abs/2508.06351", "authors": ["Olakunle S. Abawonse", "G\u00fcnay Do\u011fan"], "title": "An Implemention of Two-Phase Image Segmentation using the Split Bregman Method", "comment": "15 pages", "summary": "In this paper, we describe an implementation of the two-phase image\nsegmentation algorithm proposed by Goldstein, Bresson, Osher in\n\\cite{gold:bre}. This algorithm partitions the domain of a given 2d image into\nforeground and background regions, and each pixel of the image is assigned\nmembership to one of these two regions. The underlying assumption for the\nsegmentation model is that the pixel values of the input image can be\nsummarized by two distinct average values, and that the region boundaries are\nsmooth. Accordingly, the model is defined as an energy in which the variable is\na region membership function to assign pixels to either region, originally\nproposed by Chan and Vese in \\cite{chan:vese}. This energy is the sum of image\ndata terms in the regions and a length penalty for region boundaries.\nGoldstein, Bresson, Osher modify the energy of Chan-Vese in \\cite{gold:bre} so\nthat their new energy can be minimized efficiently using the split Bregman\nmethod to produce an equivalent two-phase segmentation. We provide a detailed\nimplementation of this method \\cite{gold:bre}, and document its performance\nwith several images over a range of algorithm parameters.", "AI": {"tldr": "\u672c\u6587\u5b9e\u73b0\u4e86Goldstein\u7b49\u4eba\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u56fe\u50cf\u5206\u5272\u7b97\u6cd5\uff0c\u6539\u8fdb\u4e86Chan-Vese\u80fd\u91cf\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5206\u88c2Bregman\u65b9\u6cd5\u9ad8\u6548\u4f18\u5316\u3002", "motivation": "\u56fe\u50cf\u5206\u5272\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u91cd\u8981\u4efb\u52a1\uff0c\u9700\u8981\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u7b97\u6cd5\u3002Goldstein\u7b49\u4eba\u7684\u6539\u8fdb\u6a21\u578b\u65e8\u5728\u63d0\u5347\u5206\u5272\u6548\u7387\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5206\u5272\u7b97\u6cd5\uff0c\u57fa\u4e8eChan-Vese\u80fd\u91cf\u6a21\u578b\uff0c\u5f15\u5165\u5206\u88c2Bregman\u65b9\u6cd5\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u901a\u8fc7\u591a\u7ec4\u56fe\u50cf\u548c\u53c2\u6570\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u548c\u6027\u80fd\u3002", "conclusion": "\u8be5\u5b9e\u73b0\u5c55\u793a\u4e86\u6539\u8fdb\u6a21\u578b\u5728\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.06382", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06382", "abs": "https://arxiv.org/abs/2508.06382", "authors": ["Xiangyu Wu", "Feng Yu", "Yang Yang", "Jianfeng Lu"], "title": "Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning", "comment": "Accepted for publication at ACMMM 2025", "summary": "The integration of prompt tuning with multimodal learning has shown\nsignificant generalization abilities for various downstream tasks. Despite\nadvancements, existing methods heavily depend on massive modality-specific\nlabeled data (e.g., video, audio, and image), or are customized for a single\nmodality. In this study, we present Text as Any-Modality by Consistent Prompt\nTuning (TaAM-CPT), a scalable approach for constructing a general\nrepresentation model toward unlimited modalities using solely text data.\nTaAM-CPT comprises modality prompt pools, text construction, and\nmodality-aligned text encoders from pre-trained models, which allows for\nextending new modalities by simply adding prompt pools and modality-aligned\ntext encoders. To harmonize the learning across different modalities, TaAM-CPT\ndesigns intra- and inter-modal learning objectives, which can capture category\ndetails within modalities while maintaining semantic consistency across\ndifferent modalities. Benefiting from its scalable architecture and pre-trained\nmodels, TaAM-CPT can be seamlessly extended to accommodate unlimited\nmodalities. Remarkably, without any modality-specific labeled data, TaAM-CPT\nachieves leading results on diverse datasets spanning various modalities,\nincluding video classification, image classification, and audio classification.\nThe code is available at https://github.com/Jinx630/TaAM-CPT.", "AI": {"tldr": "TaAM-CPT\u662f\u4e00\u79cd\u901a\u8fc7\u4e00\u81f4\u63d0\u793a\u8c03\u4f18\u6784\u5efa\u901a\u7528\u8868\u793a\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u6587\u672c\u6570\u636e\u5373\u53ef\u6269\u5c55\u5230\u65e0\u9650\u6a21\u6001\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6a21\u6001\u7279\u5b9a\u6807\u8bb0\u6570\u636e\u6216\u4ec5\u9002\u7528\u4e8e\u5355\u4e00\u6a21\u6001\uff0cTaAM-CPT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "TaAM-CPT\u5305\u62ec\u6a21\u6001\u63d0\u793a\u6c60\u3001\u6587\u672c\u6784\u5efa\u548c\u6a21\u6001\u5bf9\u9f50\u6587\u672c\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u6dfb\u52a0\u63d0\u793a\u6c60\u548c\u7f16\u7801\u5668\u6269\u5c55\u65b0\u6a21\u6001\u3002", "result": "TaAM-CPT\u5728\u89c6\u9891\u3001\u56fe\u50cf\u548c\u97f3\u9891\u5206\u7c7b\u7b49\u591a\u4e2a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u9886\u5148\u7ed3\u679c\uff0c\u65e0\u9700\u6a21\u6001\u7279\u5b9a\u6807\u8bb0\u6570\u636e\u3002", "conclusion": "TaAM-CPT\u5c55\u793a\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u901a\u7528\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06392", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06392", "abs": "https://arxiv.org/abs/2508.06392", "authors": ["Wenbin Teng", "Gonglin Chen", "Haiwei Chen", "Yajie Zhao"], "title": "FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation", "comment": null, "summary": "Recent progress in 3D reconstruction has enabled realistic 3D models from\ndense image captures, yet challenges persist with sparse views, often leading\nto artifacts in unseen areas. Recent works leverage Video Diffusion Models\n(VDMs) to generate dense observations, filling the gaps when only sparse views\nare available for 3D reconstruction tasks. A significant limitation of these\nmethods is their slow sampling speed when using VDMs. In this paper, we present\nFVGen, a novel framework that addresses this challenge by enabling fast novel\nview synthesis using VDMs in as few as four sampling steps. We propose a novel\nvideo diffusion model distillation method that distills a multi-step denoising\nteacher model into a few-step denoising student model using Generative\nAdversarial Networks (GANs) and softened reverse KL-divergence minimization.\nExtensive experiments on real-world datasets show that, compared to previous\nworks, our framework generates the same number of novel views with similar (or\neven better) visual quality while reducing sampling time by more than 90%.\nFVGen significantly improves time efficiency for downstream reconstruction\ntasks, particularly when working with sparse input views (more than 2) where\npre-trained VDMs need to be run multiple times to achieve better spatial\ncoverage.", "AI": {"tldr": "FVGen\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDMs\uff09\u5728\u4ec5\u56db\u6b65\u91c7\u6837\u4e2d\u5b9e\u73b0\u5feb\u901f\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u56fe\u4e0b3D\u91cd\u5efa\u7684\u6162\u91c7\u6837\u95ee\u9898\u3002", "motivation": "\u7a00\u758f\u89c6\u56fe\u57283D\u91cd\u5efa\u4e2d\u4f1a\u5bfc\u81f4\u672a\u89c2\u5bdf\u533a\u57df\u7684\u4f2a\u5f71\uff0c\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528VDMs\u751f\u6210\u5bc6\u96c6\u89c2\u5bdf\u4f46\u91c7\u6837\u901f\u5ea6\u6162\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7GAN\u548c\u8f6f\u5316\u53cd\u5411KL\u6563\u5ea6\u6700\u5c0f\u5316\uff0c\u5c06\u591a\u6b65\u53bb\u566a\u6559\u5e08\u6a21\u578b\u84b8\u998f\u4e3a\u5c11\u6b65\u53bb\u566a\u5b66\u751f\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFVGen\u5728\u4fdd\u6301\uff08\u6216\u66f4\u597d\uff09\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5c06\u91c7\u6837\u65f6\u95f4\u51cf\u5c11\u4e8690%\u4ee5\u4e0a\u3002", "conclusion": "FVGen\u663e\u8457\u63d0\u9ad8\u4e86\u7a00\u758f\u8f93\u5165\u89c6\u56fe\u4e0b\u4e0b\u6e38\u91cd\u5efa\u4efb\u52a1\u7684\u65f6\u95f4\u6548\u7387\u3002"}}
{"id": "2508.06420", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06420", "abs": "https://arxiv.org/abs/2508.06420", "authors": ["Ch Muhammad Awais", "Marco Reggiannini", "Davide Moroni", "Oktay Karakus"], "title": "Feature-Space Oversampling for Addressing Class Imbalance in SAR Ship Classification", "comment": "Accepted and presented at IGARSS", "summary": "SAR ship classification faces the challenge of long-tailed datasets, which\ncomplicates the classification of underrepresented classes. Oversampling\nmethods have proven effective in addressing class imbalance in optical data. In\nthis paper, we evaluated the effect of oversampling in the feature space for\nSAR ship classification. We propose two novel algorithms inspired by the\nMajor-to-minor (M2m) method M2m$_f$, M2m$_u$. The algorithms are tested on two\npublic datasets, OpenSARShip (6 classes) and FuSARShip (9 classes), using three\nstate-of-the-art models as feature extractors: ViT, VGG16, and ResNet50.\nAdditionally, we also analyzed the impact of oversampling methods on different\nclass sizes. The results demonstrated the effectiveness of our novel methods\nover the original M2m and baselines, with an average F1-score increase of 8.82%\nfor FuSARShip and 4.44% for OpenSARShip.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u7b97\u6cd5M2m$_f$\u548cM2m$_u$\uff0c\u7528\u4e8e\u89e3\u51b3SAR\u8239\u8236\u5206\u7c7b\u4e2d\u7684\u957f\u5c3e\u6570\u636e\u96c6\u95ee\u9898\uff0c\u901a\u8fc7\u7279\u5f81\u7a7a\u95f4\u8fc7\u91c7\u6837\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "SAR\u8239\u8236\u5206\u7c7b\u9762\u4e34\u957f\u5c3e\u6570\u636e\u96c6\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5c11\u6570\u7c7b\u522b\u7684\u5206\u7c7b\u6548\u679c\u4e0d\u4f73\u3002\u5149\u5b66\u6570\u636e\u4e2d\u7684\u8fc7\u91c7\u6837\u65b9\u6cd5\u5df2\u88ab\u8bc1\u660e\u6709\u6548\uff0c\u4f46SAR\u6570\u636e\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8eMajor-to-minor (M2m)\u65b9\u6cd5\u7684\u7b97\u6cd5M2m$_f$\u548cM2m$_u$\uff0c\u5e76\u5728OpenSARShip\u548cFuSARShip\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u4f7f\u7528ViT\u3001VGG16\u548cResNet50\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\u3002", "result": "\u65b0\u65b9\u6cd5\u5728FuSARShip\u548cOpenSARShip\u6570\u636e\u96c6\u4e0a\u7684F1-score\u5206\u522b\u5e73\u5747\u63d0\u9ad8\u4e868.82%\u548c4.44%\uff0c\u4f18\u4e8e\u539f\u59cbM2m\u548c\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7279\u5f81\u7a7a\u95f4\u8fc7\u91c7\u6837\u65b9\u6cd5\u5728SAR\u8239\u8236\u5206\u7c7b\u4e2d\u6709\u6548\uff0c\u65b0\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6570\u7c7b\u522b\u7684\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2508.06430", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06430", "abs": "https://arxiv.org/abs/2508.06430", "authors": ["Om Patil", "Jinesh Modi", "Suryabha Mukhopadhyay", "Meghaditya Giri", "Chhavi Malhotra"], "title": "MotionSwap", "comment": "8 pages, 7 figures, 5 tables. This is a student research submission\n  from BITS Pilani, Hyderabad Campus. Our implementation enhances SimSwap with\n  attention modules and dynamic training strategies", "summary": "Face swapping technology has gained significant attention in both academic\nresearch and commercial applications. This paper presents our implementation\nand enhancement of SimSwap, an efficient framework for high fidelity face\nswapping. We introduce several improvements to the original model, including\nthe integration of self and cross-attention mechanisms in the generator\narchitecture, dynamic loss weighting, and cosine annealing learning rate\nscheduling. These enhancements lead to significant improvements in identity\npreservation, attribute consistency, and overall visual quality.\n  Our experimental results, spanning 400,000 training iterations, demonstrate\nprogressive improvements in generator and discriminator performance. The\nenhanced model achieves better identity similarity, lower FID scores, and\nvisibly superior qualitative results compared to the baseline. Ablation studies\nconfirm the importance of each architectural and training improvement. We\nconclude by identifying key future directions, such as integrating StyleGAN3,\nimproving lip synchronization, incorporating 3D facial modeling, and\nintroducing temporal consistency for video-based applications.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5bf9SimSwap\u6846\u67b6\u7684\u6539\u8fdb\uff0c\u5305\u62ec\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u3001\u52a8\u6001\u635f\u5931\u6743\u91cd\u548c\u4f59\u5f26\u9000\u706b\u5b66\u4e60\u7387\u8c03\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8eab\u4efd\u4fdd\u7559\u3001\u5c5e\u6027\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u63d0\u5347\u4eba\u8138\u4ea4\u6362\u6280\u672f\u7684\u9ad8\u4fdd\u771f\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u89e3\u51b3\u8eab\u4efd\u4fdd\u7559\u548c\u5c5e\u6027\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u5728\u751f\u6210\u5668\u67b6\u6784\u4e2d\u96c6\u6210\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u91c7\u7528\u52a8\u6001\u635f\u5931\u6743\u91cd\u548c\u4f59\u5f26\u9000\u706b\u5b66\u4e60\u7387\u8c03\u5ea6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6539\u8fdb\u540e\u7684\u6a21\u578b\u5728\u8eab\u4efd\u76f8\u4f3c\u6027\u3001FID\u5206\u6570\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u672a\u6765\u65b9\u5411\u5305\u62ec\u96c6\u6210StyleGAN3\u3001\u6539\u8fdb\u5507\u540c\u6b65\u3001\u5f15\u51653D\u9762\u90e8\u5efa\u6a21\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.06452", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06452", "abs": "https://arxiv.org/abs/2508.06452", "authors": ["Mattia Litrico", "Mario Valerio Giuffrida", "Sebastiano Battiato", "Devis Tuia"], "title": "TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation", "comment": null, "summary": "Recent unsupervised domain adaptation (UDA) methods have shown great success\nin addressing classical domain shifts (e.g., synthetic-to-real), but they still\nsuffer under complex shifts (e.g. geographical shift), where both the\nbackground and object appearances differ significantly across domains. Prior\nworks showed that the language modality can help in the adaptation process,\nexhibiting more robustness to such complex shifts. In this paper, we introduce\nTRUST, a novel UDA approach that exploits the robustness of the language\nmodality to guide the adaptation of a vision model. TRUST generates\npseudo-labels for target samples from their captions and introduces a novel\nuncertainty estimation strategy that uses normalised CLIP similarity scores to\nestimate the uncertainty of the generated pseudo-labels. Such estimated\nuncertainty is then used to reweight the classification loss, mitigating the\nadverse effects of wrong pseudo-labels obtained from low-quality captions. To\nfurther increase the robustness of the vision model, we propose a multimodal\nsoft-contrastive learning loss that aligns the vision and language feature\nspaces, by leveraging captions to guide the contrastive training of the vision\nmodel on target images. In our contrastive loss, each pair of images acts as\nboth a positive and a negative pair and their feature representations are\nattracted and repulsed with a strength proportional to the similarity of their\ncaptions. This solution avoids the need for hardly determining positive and\nnegative pairs, which is critical in the UDA setting. Our approach outperforms\nprevious methods, setting the new state-of-the-art on classical (DomainNet) and\ncomplex (GeoNet) domain shifts. The code will be available upon acceptance.", "AI": {"tldr": "TRUST\u662f\u4e00\u79cd\u65b0\u578b\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u6001\u7684\u9c81\u68d2\u6027\u6307\u5bfc\u89c6\u89c9\u6a21\u578b\u7684\u9002\u5e94\uff0c\u901a\u8fc7\u751f\u6210\u4f2a\u6807\u7b7e\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u57df\u504f\u79fb\uff08\u5982\u5730\u7406\u504f\u79fb\uff09\u4e2d\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u6001\u7684\u9c81\u68d2\u6027\u63d0\u5347\u89c6\u89c9\u6a21\u578b\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "TRUST\u901a\u8fc7\u751f\u6210\u4f2a\u6807\u7b7e\u3001\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u591a\u6a21\u6001\u8f6f\u5bf9\u6bd4\u5b66\u4e60\u635f\u5931\uff0c\u5bf9\u9f50\u89c6\u89c9\u548c\u8bed\u8a00\u7279\u5f81\u7a7a\u95f4\u3002", "result": "\u5728\u7ecf\u5178\u57df\u504f\u79fb\uff08DomainNet\uff09\u548c\u590d\u6742\u57df\u504f\u79fb\uff08GeoNet\uff09\u4e0a\u8fbe\u5230\u65b0\u7684\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "TRUST\u901a\u8fc7\u8bed\u8a00\u6a21\u6001\u7684\u5f15\u5bfc\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u57df\u9002\u5e94\u7684\u6027\u80fd\u3002"}}
{"id": "2508.06492", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06492", "abs": "https://arxiv.org/abs/2508.06492", "authors": ["Yuwei Yang", "Zeyu Zhang", "Yunzhong Hou", "Zhuowan Li", "Gaowen Liu", "Ali Payani", "Yuan-Sen Ting", "Liang Zheng"], "title": "Effective Training Data Synthesis for Improving MLLM Chart Understanding", "comment": "Accepted by ICCV 2025 (poster). 26 pages, 17 figures", "summary": "Being able to effectively read scientific plots, or chart understanding, is a\ncentral part toward building effective agents for science. However, existing\nmultimodal large language models (MLLMs), especially open-source ones, are\nstill falling behind with a typical success rate of 30%-50% on challenging\nbenchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are\noften restricted by their inadequate similarity to the real charts, which could\ncompromise model training and performance on complex real-world charts. In this\nstudy, we show that modularizing chart generation and diversifying visual\ndetails improves chart understanding capabilities. In particular, we design a\nfive-step data synthesis pipeline, where we separate data and function creation\nfor single plot generation, condition the generation of later subplots on\nearlier ones for multi-subplot figures, visually diversify the generated\nfigures, filter out low quality data, and finally generate the question-answer\n(QA) pairs with GPT-4o. This approach allows us to streamline the generation of\nfine-tuning datasets and introduce the effective chart dataset (ECD), which\ncontains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring\n250+ chart type combinations with high visual complexity. We show that ECD\nconsistently improves the performance of various MLLMs on a range of real-world\nand synthetic test sets. Code, data and models are available at:\nhttps://github.com/yuweiyang-anu/ECD.", "AI": {"tldr": "\u901a\u8fc7\u6a21\u5757\u5316\u548c\u591a\u6837\u5316\u89c6\u89c9\u7ec6\u8282\u7684\u56fe\u8868\u751f\u6210\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u56fe\u8868\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u5305\u542b10k+\u56fe\u8868\u548c300k+\u95ee\u7b54\u5bf9\u7684\u6709\u6548\u56fe\u8868\u6570\u636e\u96c6\uff08ECD\uff09\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90MLLMs\u5728\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u6210\u529f\u7387\u8f83\u4f4e\uff0830%-50%\uff09\uff0c\u4e14\u73b0\u6709\u5408\u6210\u56fe\u8868\u4e0e\u771f\u5b9e\u56fe\u8868\u76f8\u4f3c\u5ea6\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u8bad\u7ec3\u548c\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e94\u6b65\u6570\u636e\u5408\u6210\u6d41\u7a0b\uff0c\u5305\u62ec\u6a21\u5757\u5316\u56fe\u8868\u751f\u6210\u3001\u591a\u6837\u5316\u89c6\u89c9\u7ec6\u8282\u3001\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u6570\u636e\uff0c\u4ee5\u53ca\u7528GPT-4\u751f\u6210\u95ee\u7b54\u5bf9\u3002", "result": "\u63d0\u51fa\u7684ECD\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cdMLLMs\u5728\u771f\u5b9e\u548c\u5408\u6210\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u6a21\u5757\u5316\u548c\u591a\u6837\u5316\u7684\u56fe\u8868\u751f\u6210\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86MLLMs\u7684\u56fe\u8868\u7406\u89e3\u80fd\u529b\uff0cECD\u6570\u636e\u96c6\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u8d44\u6e90\u3002"}}
{"id": "2508.06494", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06494", "abs": "https://arxiv.org/abs/2508.06494", "authors": ["Yehonathan Litman", "Fernando De la Torre", "Shubham Tulsiani"], "title": "LightSwitch: Multi-view Relighting with Material-guided Diffusion", "comment": "ICCV 2025, Project page & Code:\n  https://yehonathanlitman.github.io/light_switch/", "summary": "Recent approaches for 3D relighting have shown promise in integrating 2D\nimage relighting generative priors to alter the appearance of a 3D\nrepresentation while preserving the underlying structure. Nevertheless,\ngenerative priors used for 2D relighting that directly relight from an input\nimage do not take advantage of intrinsic properties of the subject that can be\ninferred or cannot consider multi-view data at scale, leading to subpar\nrelighting. In this paper, we propose Lightswitch, a novel finetuned\nmaterial-relighting diffusion framework that efficiently relights an arbitrary\nnumber of input images to a target lighting condition while incorporating cues\nfrom inferred intrinsic properties. By using multi-view and material\ninformation cues together with a scalable denoising scheme, our method\nconsistently and efficiently relights dense multi-view data of objects with\ndiverse material compositions. We show that our 2D relighting prediction\nquality exceeds previous state-of-the-art relighting priors that directly\nrelight from images. We further demonstrate that LightSwitch matches or\noutperforms state-of-the-art diffusion inverse rendering methods in relighting\nsynthetic and real objects in as little as 2 minutes.", "AI": {"tldr": "Lightswitch\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5fae\u8c03\u6750\u8d28\u91cd\u5149\u7167\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u591a\u89c6\u89d2\u548c\u6750\u8d28\u4fe1\u606f\uff0c\u9ad8\u6548\u5730\u5c06\u4efb\u610f\u6570\u91cf\u7684\u8f93\u5165\u56fe\u50cf\u91cd\u5149\u7167\u5230\u76ee\u6807\u5149\u7167\u6761\u4ef6\u3002", "motivation": "\u73b0\u67092D\u91cd\u5149\u7167\u751f\u6210\u5148\u9a8c\u672a\u80fd\u5145\u5206\u5229\u7528\u4e3b\u4f53\u7684\u5185\u5728\u5c5e\u6027\u6216\u5927\u89c4\u6a21\u591a\u89c6\u89d2\u6570\u636e\uff0c\u5bfc\u81f4\u91cd\u5149\u7167\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faLightswitch\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u89c6\u89d2\u548c\u6750\u8d28\u4fe1\u606f\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u7684\u53bb\u566a\u65b9\u6848\uff0c\u9ad8\u6548\u91cd\u5149\u7167\u591a\u89c6\u89d2\u6570\u636e\u3002", "result": "Lightswitch\u57282D\u91cd\u5149\u7167\u9884\u6d4b\u8d28\u91cf\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u7269\u4f53\u91cd\u5149\u7167\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Lightswitch\u901a\u8fc7\u6574\u5408\u5185\u5728\u5c5e\u6027\u548c\u591a\u89c6\u89d2\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5149\u7167\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002"}}
