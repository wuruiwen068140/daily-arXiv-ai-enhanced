<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 23]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Unifying Post-hoc Explanations of Knowledge Graph Completions](https://arxiv.org/abs/2507.22951)
*Alessandro Lonardi,Samy Badreddine,Tarek R. Besold,Pablo Sanchez Martin*

Main category: cs.AI

TL;DR: 本文提出了一种统一的知识图谱补全（KGC）后验解释性方法，通过多目标优化框架平衡解释的有效性和简洁性，并改进了评估协议。


<details>
  <summary>Details</summary>
Motivation: 当前KGC后验解释性缺乏形式化和一致的评估，阻碍了可重复性和跨研究比较。

Method: 提出多目标优化框架统一现有解释性算法，改进评估协议（如Mean Reciprocal Rank和Hits@k）。

Result: 通过统一方法和优化评估标准，提升了KGC解释性的可重复性和实际影响。

Conclusion: 本文为KGC解释性研究提供了统一框架和评估标准，旨在推动该领域的可重复性和实用性。

Abstract: Post-hoc explainability for Knowledge Graph Completion (KGC) lacks
formalization and consistent evaluations, hindering reproducibility and
cross-study comparisons. This paper argues for a unified approach to post-hoc
explainability in KGC. First, we propose a general framework to characterize
post-hoc explanations via multi-objective optimization, balancing their
effectiveness and conciseness. This unifies existing post-hoc explainability
algorithms in KGC and the explanations they produce. Next, we suggest and
empirically support improved evaluation protocols using popular metrics like
Mean Reciprocal Rank and Hits@$k$. Finally, we stress the importance of
interpretability as the ability of explanations to address queries meaningful
to end-users. By unifying methods and refining evaluation standards, this work
aims to make research in KGC explainability more reproducible and impactful.

</details>


### [2] [Data Readiness for Scientific AI at Scale](https://arxiv.org/abs/2507.23018)
*Wesley Brewer,Patrick Widener,Valentine Anantharaj,Feiyi Wang,Tom Beck,Arjun Shankar,Sarp Oral*

Main category: cs.AI

TL;DR: 本文探讨了数据准备度（DRAI）原则在领导级科学数据集中的应用，提出了一个针对高性能计算环境的两维准备度框架，用于指导可扩展和可复现的科学AI基础设施开发。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决科学数据在训练基础模型时的预处理挑战，特别是在跨领域的高性能计算环境中。

Method: 方法包括分析四个代表性领域（气候、核聚变、生物/健康、材料）的典型工作流程，并提出一个结合数据准备级别和处理阶段的两维框架。

Result: 研究结果是一个概念性的成熟度矩阵，用于表征科学数据的准备度，并指导跨领域的标准化AI训练基础设施开发。

Conclusion: 结论是该框架为科学数据的AI训练提供了标准化和可扩展的支持，特别适用于基于Transformer的生成模型。

Abstract: This paper examines how Data Readiness for AI (DRAI) principles apply to
leadership-scale scientific datasets used to train foundation models. We
analyze archetypal workflows across four representative domains - climate,
nuclear fusion, bio/health, and materials - to identify common preprocessing
patterns and domain-specific constraints. We introduce a two-dimensional
readiness framework composed of Data Readiness Levels (raw to AI-ready) and
Data Processing Stages (ingest to shard), both tailored to high performance
computing (HPC) environments. This framework outlines key challenges in
transforming scientific data for scalable AI training, emphasizing
transformer-based generative models. Together, these dimensions form a
conceptual maturity matrix that characterizes scientific data readiness and
guides infrastructure development toward standardized, cross-domain support for
scalable and reproducible AI for science.

</details>


### [3] [FairReason: Balancing Reasoning and Social Bias in MLLMs](https://arxiv.org/abs/2507.23067)
*Zhenyu Pan,Yutong Zhang,Jianshu Zhang,Haoran Lu,Haozheng Luo,Yuwei Han,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: 研究发现，通过强化学习训练的1:4混合样本比例能在减少10%偏见的同时保留88%推理准确率。


<details>
  <summary>Details</summary>
Motivation: 探讨多模态大语言模型在推理能力提升与偏见缓解之间的权衡问题。

Method: 比较了三种偏见缓解策略（SFT、KD、RL），并调整样本比例以分析推理与偏见的权衡。

Result: 强化学习训练的1:4混合样本比例效果最佳，偏见减少10%，推理准确率保留88%。

Conclusion: 研究为平衡MLLMs的公平性与能力提供了具体指导。

Abstract: Multimodal Large Language Models (MLLMs) already achieve state-of-the-art
results across a wide range of tasks and modalities. To push their reasoning
ability further, recent studies explore advanced prompting schemes and
post-training fine-tuning. Although these techniques improve logical accuracy,
they frequently leave the models' outputs burdened with pronounced social
biases. Clarifying how reasoning gains interact with bias mitigation-and
whether the two objectives inherently trade off-therefore remains an open and
pressing research problem. Our study begins by benchmarking three
bias-mitigation strategies-supervised fine-uning (SFT), knowledge distillation
(KD), and rule-based reinforcement learning (RL)-under identical conditions,
establishing their baseline strengths and weaknesses. Building on these
results, we vary the proportion of debias-focused and reasoning-centric samples
within each paradigm to chart the reasoning-versus-bias trade-off. Our sweeps
reveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement
learning cuts stereotype scores by 10% while retaining 88% of the model's
original reasoning accuracy, offering concrete guidance for balancing fairness
and capability in MLLMs.

</details>


### [4] [Moravec's Paradox: Towards an Auditory Turing Test](https://arxiv.org/abs/2507.23091)
*David Noever,Forrest McKee*

Main category: cs.AI

TL;DR: 当前AI系统在人类轻松完成的听觉任务上表现糟糕，失败率超过93%，而人类成功率是AI的7.5倍。


<details>
  <summary>Details</summary>
Motivation: 受Moravec悖论启发，研究旨在量化AI与人类在听觉任务上的差距，并揭示失败原因。

Method: 设计包含917个挑战的听觉图灵测试，涵盖七类任务，评估了GPT-4和Whisper等先进模型。

Result: AI模型表现极差，最高准确率仅6.9%，而人类达到52%。

Conclusion: 当前AI架构缺乏人类听觉场景分析的基本机制，需整合选择性注意力和上下文感知等新方法。

Abstract: This research work demonstrates that current AI systems fail catastrophically
on auditory tasks that humans perform effortlessly. Drawing inspiration from
Moravec's paradox (i.e., tasks simple for humans often prove difficult for
machines, and vice versa), we introduce an auditory Turing test comprising 917
challenges across seven categories: overlapping speech, speech in noise,
temporal distortion, spatial audio, coffee-shop noise, phone distortion, and
perceptual illusions. Our evaluation of state-of-the-art audio models including
GPT-4's audio capabilities and OpenAI's Whisper reveals a striking failure rate
exceeding 93%, with even the best-performing model achieving only 6.9% accuracy
on tasks that humans solved at 7.5 times higher success (52%). These results
expose focusing failures in how AI systems process complex auditory scenes,
particularly in selective attention, noise robustness, and contextual
adaptation. Our benchmark not only quantifies the human-machine auditory gap
but also provides insights into why these failures occur, suggesting that
current architectures lack fundamental mechanisms for human-like auditory scene
analysis. The traditional design of audio CAPTCHAs highlights common filters
that humans evolved but machines fail to select in multimodal language models.
This work establishes a diagnostic framework for measuring progress toward
human-level machine listening and highlights the need for novel approaches
integrating selective attention, physics-based audio understanding, and
context-aware perception into multimodal AI systems.

</details>


### [5] [Argumentatively Coherent Judgmental Forecasting](https://arxiv.org/abs/2507.23163)
*Deniz Gorur,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 该论文提出并形式化定义了‘论证连贯性’属性，要求预测者的推理与其预测一致，并通过实验验证了其在实际预测中的价值。


<details>
  <summary>Details</summary>
Motivation: 研究论证结构对预测的影响，特别是在人类和LLM预测中，论证连贯性对预测准确性的提升作用。

Method: 通过三种评估方式：1) 对人类和LLM预测者强制连贯性的影响；2) 过滤不连贯预测的效果；3) 用户实验验证连贯性的直观性和实用性。

Result: 过滤不连贯预测显著提高了人类和LLM预测的准确性，但用户实验显示用户通常不遵循连贯性原则。

Conclusion: 论证连贯性在预测中具有实际价值，但需在群体预测中引入机制过滤不连贯意见。

Abstract: Judgmental forecasting employs human opinions to make predictions about
future events, rather than exclusively historical data as in quantitative
forecasting. When these opinions form an argumentative structure around
forecasts, it is useful to study the properties of the forecasts from an
argumentative perspective. In this paper, we advocate and formally define a
property of argumentative coherence, which, in essence, requires that a
forecaster's reasoning is coherent with their forecast. We then conduct three
evaluations with our notion of coherence. First, we assess the impact of
enforcing coherence on human forecasters as well as on Large Language Model
(LLM)-based forecasters, given that they have recently shown to be competitive
with human forecasters. In both cases, we show that filtering out incoherent
predictions improves forecasting accuracy consistently, supporting the
practical value of coherence in both human and LLM-based forecasting. Then, via
crowd-sourced user experiments, we show that, despite its apparent
intuitiveness and usefulness, users do not generally align with this coherence
property. This points to the need to integrate, within argumentation-based
judgmental forecasting, mechanisms to filter out incoherent opinions before
obtaining group forecasting predictions.

</details>


### [6] [Tractable Responsibility Measures for Ontology-Mediated Query Answering](https://arxiv.org/abs/2507.23191)
*Meghyn Bienvenu,Diego Figueira,Pierre Lafourcade*

Main category: cs.AI

TL;DR: 研究了基于Shapley值的责任度量在ontology-mediated查询中的计算复杂性，发现其在某些查询类中具有多项式数据复杂度，但在其他情况下可能变得困难。


<details>
  <summary>Details</summary>
Motivation: 量化查询答案中各个事实的贡献是重要的，但现有方法在ontology-mediated查询中的复杂性尚未充分研究。

Method: 利用数据库设置的结果，分析不同查询类和ontology语言下的WSMS计算复杂性。

Result: 对于first-order-rewritable查询类，WSMS计算具有多项式数据复杂度；而在某些情况下，问题变得困难。

Conclusion: 研究揭示了WSMS计算的复杂性边界，并为特定查询类提供了高效计算方法。

Abstract: Recent work on quantitative approaches to explaining query answers employs
responsibility measures to assign scores to facts in order to quantify their
respective contributions to obtaining a given answer. In this paper, we study
the complexity of computing such responsibility scores in the setting of
ontology-mediated query answering, focusing on a very recently introduced
family of Shapley-value-based responsibility measures defined in terms of
weighted sums of minimal supports (WSMS). By exploiting results from the
database setting, we can show that such measures enjoy polynomial data
complexity for classes of ontology-mediated queries that are
first-order-rewritable, whereas the problem becomes "shP"-hard when the
ontology language can encode reachability queries (via axioms like $\exists R.
A \sqsubseteq A$). To better understand the tractability frontier, we next
explore the combined complexity of WSMS computation. We prove that
intractability applies already to atomic queries if the ontology language
supports conjunction, as well as to unions of `well-behaved' conjunctive
queries, even in the absence of an ontology. By contrast, our study yields
positive results for common DL-Lite dialects: by means of careful analysis, we
identify classes of structurally restricted conjunctive queries (which
intuitively disallow undesirable interactions between query atoms) that admit
tractable WSMS computation.

</details>


### [7] [Solution-aware vs global ReLU selection: partial MILP strikes back for DNN verification](https://arxiv.org/abs/2507.23197)
*Yuke Liao,Blaise Genest,Kuldeep Meel,Shaan Aryaman*

Main category: cs.AI

TL;DR: 论文提出了一种新的解决方案，通过部分MILP调用和ReLU评分方法（SAS和GS）来减少二进制变量数量，提高验证效率。


<details>
  <summary>Details</summary>
Motivation: 处理复杂实例时，传统方法效率低下，需要更优的ReLU变量选择方法。

Method: 采用部分MILP调用和新型ReLU评分方法（SAS和GS），结合Hybrid MILP实现。

Result: SAS减少了6倍的二进制变量，验证效率提升40%，保持高精度。

Conclusion: 新方法在保持精度的同时显著提升了验证效率，适用于大规模CNN。

Abstract: To handle complex instances, we revisit a divide-and-conquer approach to
break down the complexity: instead of few complex BaB calls, we rely on many
small {\em partial} MILP calls. The crucial step is to select very few but very
important ReLUs to treat using (costly) binary variables. The previous attempts
were suboptimal in that respect. To select these important ReLU variables, we
propose a novel {\em solution-aware} ReLU scoring ({\sf SAS}), as well as adapt
the BaB-SR and BaB-FSB branching functions as {\em global} ReLU scoring ({\sf
GS}) functions. We compare them theoretically as well as experimentally, and
{\sf SAS} is more efficient at selecting a set of variables to open using
binary variables. Compared with previous attempts, SAS reduces the number of
binary variables by around 6 times, while maintaining the same level of
accuracy. Implemented in {\em Hybrid MILP}, calling first $\alpha,\beta$-CROWN
with a short time-out to solve easier instances, and then partial MILP,
produces a very accurate yet efficient verifier, reducing by up to $40\%$ the
number of undecided instances to low levels ($8-15\%$), while keeping a
reasonable runtime ($46s-417s$ on average per instance), even for fairly large
CNNs with 2 million parameters.

</details>


### [8] [How Far Are AI Scientists from Changing the World?](https://arxiv.org/abs/2507.23276)
*Qiujie Xie,Yixuan Weng,Minjun Zhu,Fuchen Shen,Shulin Huang,Zhen Lin,Jiahui Zhou,Zilan Mao,Zijie Yang,Linyi Yang,Jian Wu,Yue Zhang*

Main category: cs.AI

TL;DR: 综述探讨AI科学家系统如何改变世界和科研范式，分析当前成就、瓶颈及未来目标。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）推动自动化科学发现，AI科学家系统可能重塑科研模式。

Method: 通过前瞻性综述，全面分析AI科学家系统的现状、瓶颈和关键组件。

Result: 识别当前系统的局限性，提出解决重大挑战所需的关键要素。

Conclusion: 希望明确当前AI科学家系统的不足，并指明未来科学AI的终极目标。

Abstract: The emergence of large language models (LLMs) is propelling automated
scientific discovery to the next level, with LLM-based Artificial Intelligence
(AI) Scientist systems now taking the lead in scientific research. Several
influential works have already appeared in the field of AI Scientist systems,
with AI-generated research papers having been accepted at the ICLR 2025
workshop, suggesting that a human-level AI Scientist capable of uncovering
phenomena previously unknown to humans, may soon become a reality. In this
survey, we focus on the central question: How far are AI scientists from
changing the world and reshaping the scientific research paradigm? To answer
this question, we provide a prospect-driven review that comprehensively
analyzes the current achievements of AI Scientist systems, identifying key
bottlenecks and the critical components required for the emergence of a
scientific agent capable of producing ground-breaking discoveries that solve
grand challenges. We hope this survey will contribute to a clearer
understanding of limitations of current AI Scientist systems, showing where we
are, what is missing, and what the ultimate goals for scientific AI should be.

</details>


### [9] [AI Must not be Fully Autonomous](https://arxiv.org/abs/2507.23330)
*Tosin Adewumi,Lama Alkhaled,Florent Imbert,Hui Han,Nudrat Habib,Karl Löwenmark*

Main category: cs.AI

TL;DR: 论文主张AI不应完全自主，提出了3级自主AI分类，并提供了12个论点和6个反驳论点的反驳。


<details>
  <summary>Details</summary>
Motivation: 探讨AI自主性的风险，尤其是超级智能可能带来的威胁。

Method: 通过理论分析、12个论点和6个反驳论点的讨论，以及附录中的15个案例。

Result: 强调人类监督对降低AI风险的重要性。

Conclusion: AI应保持在非完全自主的级别，以避免潜在风险。

Abstract: Autonomous Artificial Intelligence (AI) has many benefits. It also has many
risks. In this work, we identify the 3 levels of autonomous AI. We are of the
position that AI must not be fully autonomous because of the many risks,
especially as artificial superintelligence (ASI) is speculated to be just
decades away. Fully autonomous AI, which can develop its own objectives, is at
level 3 and without responsible human oversight. However, responsible human
oversight is crucial for mitigating the risks. To ague for our position, we
discuss theories of autonomy, AI and agents. Then, we offer 12 distinct
arguments and 6 counterarguments with rebuttals to the counterarguments. We
also present 15 pieces of recent evidence of AI misaligned values and other
risks in the appendix.

</details>


### [10] [DSBC : Data Science task Benchmarking with Context engineering](https://arxiv.org/abs/2507.23336)
*Ram Mohan Rao Kadiyala,Siddhant Gupta,Jebish Purbey,Giulio Martini,Suman Debnath,Hamza Farooq*

Main category: cs.AI

TL;DR: 本文介绍了一个针对数据科学代理的全面基准测试，评估了三种大型语言模型在不同方法下的表现，揭示了性能差异和关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 尽管数据科学代理在自动化分析任务中迅速普及，但缺乏系统性评估其效能和局限性的基准测试。

Method: 通过观察商业应用的使用情况，设计了一个反映真实用户交互的基准测试，评估了三种模型（Claude-4.0-Sonnet、Gemini-2.5-Flash、OpenAI-o4-Mini）在三种方法（零样本上下文工程、多步上下文工程、SmolAgent）下的表现。

Result: 研究发现不同模型和方法之间存在明显的性能差异，并揭示了温度参数对结果的影响。

Conclusion: 提出的基准数据集和评估框架为未来研究更鲁棒和高效的数据科学代理奠定了基础。

Abstract: Recent advances in large language models (LLMs) have significantly impacted
data science workflows, giving rise to specialized data science agents designed
to automate analytical tasks. Despite rapid adoption, systematic benchmarks
evaluating the efficacy and limitations of these agents remain scarce. In this
paper, we introduce a comprehensive benchmark specifically crafted to reflect
real-world user interactions with data science agents by observing usage of our
commercial applications. We evaluate three LLMs: Claude-4.0-Sonnet,
Gemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with
context engineering, multi-step with context engineering, and with SmolAgent.
Our benchmark assesses performance across a diverse set of eight data science
task categories, additionally exploring the sensitivity of models to common
prompting issues, such as data leakage and slightly ambiguous instructions. We
further investigate the influence of temperature parameters on overall and
task-specific outcomes for each model and approach. Our findings reveal
distinct performance disparities among the evaluated models and methodologies,
highlighting critical factors that affect practical deployment. The benchmark
dataset and evaluation framework introduced herein aim to provide a foundation
for future research of more robust and effective data science agents.

</details>


### [11] [LLM4Rail: An LLM-Augmented Railway Service Consulting Platform](https://arxiv.org/abs/2507.23377)
*Zhuo Li,Xianghuai Deng,Chiwei Feng,Hanmeng Li,Shenjie Wang,Haichao Zhang,Teng Jia,Conlin Chen,Louis Linchun Wu,Jia Wang*

Main category: cs.AI

TL;DR: LLM4Rail是一个基于大语言模型（LLM）的铁路服务平台，通过QTAO提示框架整合推理与任务导向行动，提供个性化铁路服务，包括票务、餐饮推荐等。


<details>
  <summary>Details</summary>
Motivation: 为满足日益增长的个性化铁路服务需求，开发一个能够整合多种服务的智能平台。

Method: 提出QTAO提示框架，结合推理与行动选择；构建CRFD-25数据集，并开发基于LLM的零样本对话推荐系统。

Result: LLM4Rail能够提供准确的个性化服务，特别是在餐饮推荐方面，通过后处理步骤确保推荐内容与数据集一致。

Conclusion: LLM4Rail展示了LLM在铁路服务中的潜力，通过智能推理和个性化推荐提升了用户体验。

Abstract: Large language models (LLMs) have significantly reshaped different walks of
business. To meet the increasing demands for individualized railway service, we
develop LLM4Rail - a novel LLM-augmented railway service consulting platform.
Empowered by LLM, LLM4Rail can provide custom modules for ticketing, railway
food & drink recommendations, weather information, and chitchat. In LLM4Rail,
we propose the iterative "Question-Thought-Action-Observation (QTAO)" prompting
framework. It meticulously integrates verbal reasoning with task-oriented
actions, that is, reasoning to guide action selection, to effectively retrieve
external observations relevant to railway operation and service to generate
accurate responses. To provide personalized onboard dining services, we first
construct the Chinese Railway Food and Drink (CRFD-25) - a publicly accessible
takeout dataset tailored for railway services. CRFD-25 covers a wide range of
signature dishes categorized by cities, cuisines, age groups, and spiciness
levels. We further introduce an LLM-based zero-shot conversational recommender
for railway catering. To address the unconstrained nature of open
recommendations, the feature similarity-based post-processing step is
introduced to ensure all the recommended items are aligned with CRFD-25
dataset.

</details>


### [12] [Chatting with your ERP: A Recipe](https://arxiv.org/abs/2507.23429)
*Jorge Ruiz Gómez,Lidia Andrés Susinos,Jorge Alamo Olivé,Sonia Rey Osorno,Manuel Luis Gonzalez Hernández*

Main category: cs.AI

TL;DR: 论文介绍了一种基于大型语言模型（LLM）的代理，用于与工业级ERP系统交互，通过自然语言查询生成可执行SQL语句。


<details>
  <summary>Details</summary>
Motivation: 解决工业级ERP系统中自然语言查询的自动化处理问题。

Method: 提出了一种结合推理和批判阶段的双代理架构，以提高查询生成的可靠性。

Result: 代理能够可靠地将自然语言查询转换为可执行SQL语句。

Conclusion: 双代理架构有效提升了LLM在工业环境中的实用性。

Abstract: This paper presents the design, implementation, and evaluation behind a Large
Language Model (LLM) agent that chats with an industrial production-grade ERP
system. The agent is capable of interpreting natural language queries and
translating them into executable SQL statements, leveraging open-weight LLMs. A
novel dual-agent architecture combining reasoning and critique stages was
proposed to improve query generation reliability.

</details>


### [13] [Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation](https://arxiv.org/abs/2507.23440)
*Mingzhe Li,Xin Lu,Yanyan Zhao*

Main category: cs.AI

TL;DR: Self-Foveate是一种创新的LLM驱动方法，通过多级注视方法增强合成指令的多样性和难度。


<details>
  <summary>Details</summary>
Motivation: 现有自动化合成方法在确保合成指令的多样性和难度方面存在显著局限性。

Method: 提出Self-Foveate方法，采用“Micro-Scatter-Macro”多级注视技术，引导LLM深入挖掘无监督文本中的细粒度信息。

Result: 在多个无监督语料库和不同模型架构上的实验验证了该方法的有效性和优越性。

Conclusion: Self-Foveate显著提升了合成指令的多样性和难度，为LLM训练提供了更高效的数据合成方案。

Abstract: Large language models (LLMs) with instruction following capabilities have
demonstrated impressive problem-solving abilities. While synthesizing
instructional data from unsupervised text has become a common approach for
training such models, conventional methods rely heavily on human effort for
data annotation. Although existing automated synthesis paradigms have
alleviated this constraint, they still exhibit significant limitations in
ensuring adequate diversity and difficulty of synthesized instructions. To
address these challenges, we propose Self-Foveate, an innovative LLM-driven
method for instruction synthesis. This approach introduces a
"Micro-Scatter-Macro" multi-level foveation methodology that effectively guides
the LLM to deeply excavate fine-grained information embedded in unsupervised
text, thereby enhancing both the diversity and difficulty of synthesized
instructions. Comprehensive experiments across multiple unsupervised corpora
and diverse model architectures validate the effectiveness and superiority of
our proposed method. We publicly release our data and codes:
https://github.com/Mubuky/Self-Foveate

</details>


### [14] [Causal Reasoning in Pieces: Modular In-Context Learning for Causal Discovery](https://arxiv.org/abs/2507.23488)
*Kacper Kadziolka,Saber Salehkaleybar*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型在因果推断中的表现，提出了一种基于Tree-of-Thoughts和Chain-of-Thoughts的模块化方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 因果推断是大型语言模型的核心挑战，现有模型在数据扰动下表现不佳，需要更鲁棒的方法。

Method: 采用OpenAI的o系列和DeepSeek-R模型，结合模块化的上下文管道方法，优化因果发现任务。

Result: 新方法在Corr2Cause基准测试中表现显著优于传统方法，性能提升近三倍。

Conclusion: 高级推理模型结合结构化上下文框架可显著提升因果发现能力，为跨领域应用提供通用方案。

Abstract: Causal inference remains a fundamental challenge for large language models.
Recent advances in internal reasoning with large language models have sparked
interest in whether state-of-the-art reasoning models can robustly perform
causal discovery-a task where conventional models often suffer from severe
overfitting and near-random performance under data perturbations. We study
causal discovery on the Corr2Cause benchmark using the emergent OpenAI's
o-series and DeepSeek-R model families and find that these reasoning-first
architectures achieve significantly greater native gains than prior approaches.
To capitalize on these strengths, we introduce a modular in-context pipeline
inspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies, yielding
nearly three-fold improvements over conventional baselines. We further probe
the pipeline's impact by analyzing reasoning chain length, complexity, and
conducting qualitative and quantitative comparisons between conventional and
reasoning models. Our findings suggest that while advanced reasoning models
represent a substantial leap forward, carefully structured in-context
frameworks are essential to maximize their capabilities and offer a
generalizable blueprint for causal discovery across diverse domains.

</details>


### [15] [Causal Identification of Sufficient, Contrastive and Complete Feature Sets in Image Classification](https://arxiv.org/abs/2507.23497)
*David A Kelly,Hana Chockler*

Main category: cs.AI

TL;DR: 提出了一种基于因果解释的图像分类器解释方法，兼具形式严谨性和计算可行性。


<details>
  <summary>Details</summary>
Motivation: 现有图像分类器解释方法缺乏形式严谨性，而逻辑解释虽严谨但计算假设严格，不适用于图像分类器。

Method: 引入因果解释，证明其形式性质，并提出对比性和置信度感知的完整因果解释。

Result: 实验表明不同模型在充分性、对比性和完整性上有不同模式，算法高效且完全黑盒。

Conclusion: 因果解释在图像分类器中兼具形式严谨性和实际可行性，为解释方法提供了新方向。

Abstract: Existing algorithms for explaining the outputs of image classifiers are based
on a variety of approaches and produce explanations that lack formal rigor. On
the other hand, logic-based explanations are formally and rigorously defined
but their computability relies on strict assumptions about the model that do
not hold on image classifiers.
  In this paper, we show that causal explanations, in addition to being
formally and rigorously defined, enjoy the same formal properties as
logic-based ones, while still lending themselves to black-box algorithms and
being a natural fit for image classifiers. We prove formal properties of causal
explanations and introduce contrastive causal explanations for image
classifiers. Moreover, we augment the definition of explanation with confidence
awareness and introduce complete causal explanations: explanations that are
classified with exactly the same confidence as the original image.
  We implement our definitions, and our experimental results demonstrate that
different models have different patterns of sufficiency, contrastiveness, and
completeness. Our algorithms are efficiently computable, taking on average 6s
per image on a ResNet50 model to compute all types of explanations, and are
totally black-box, needing no knowledge of the model, no access to model
internals, no access to gradient, nor requiring any properties, such as
monotonicity, of the model.

</details>


### [16] [DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer](https://arxiv.org/abs/2507.23554)
*Ruoyu Wang,Junda Wu,Yu Xia,Tong Yu,Ryan A. Rossi,Julian McAuley,Lina Yao*

Main category: cs.AI

TL;DR: DICE是一个基于理论基础的动态上下文示例选择框架，旨在提升LLM代理在复杂推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在示例选择上依赖启发式或任务特定设计，缺乏通用的理论依据，导致性能不稳定。

Method: 通过因果视角分解演示知识，提出逐步选择标准，并确保性能提升的正式保证。

Result: 实验表明DICE在多个领域均有效且通用，能显著提升代理性能。

Conclusion: DICE是一个无需额外训练成本的通用解决方案，强调了基于理论的示例选择对LLM代理的重要性。

Abstract: Large language model-based agents, empowered by in-context learning (ICL),
have demonstrated strong capabilities in complex reasoning and tool-use tasks.
However, existing works have shown that the effectiveness of ICL is highly
sensitive to the choice of demonstrations, with suboptimal examples often
leading to unstable or degraded performance. While prior work has explored
example selection, including in some agentic or multi-step settings, existing
approaches typically rely on heuristics or task-specific designs and lack a
general, theoretically grounded criterion for what constitutes an effective
demonstration across reasoning steps. Therefore, it is non-trivial to develop a
principled, general-purpose method for selecting demonstrations that
consistently benefit agent performance. In this paper, we address this
challenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a
theoretically grounded ICL framework for agentic tasks that selects the most
relevant demonstrations at each step of reasoning. Our approach decomposes
demonstration knowledge into transferable and non-transferable components
through a causal lens, showing how the latter can introduce spurious
dependencies that impair generalization. We further propose a stepwise
selection criterion with a formal guarantee of improved agent performance.
Importantly, DICE is a general, framework-agnostic solution that can be
integrated as a plug-in module into existing agentic frameworks without any
additional training cost. Extensive experiments across diverse domains
demonstrate our method's effectiveness and generality, highlighting the
importance of principled, context-aware demo selection for robust and efficient
LLM agents.

</details>


### [17] [Semantic Chain-of-Trust: Autonomous Trust Orchestration for Collaborator Selection via Hypergraph-Aided Agentic AI](https://arxiv.org/abs/2507.23565)
*Botao Zhu,Xianbin Wang,Dusit Niyato*

Main category: cs.AI

TL;DR: 提出了一种基于语义信任链的自主信任编排方法，利用代理AI和超图技术优化分布式协作中的信任评估。


<details>
  <summary>Details</summary>
Motivation: 分布式协作中，复杂的任务和动态资源增加了信任评估的复杂性和资源消耗，影响协作效率。

Method: 采用代理AI和超图技术，通过历史数据和空闲期评估设备信任，并结合任务需求分析资源匹配。

Result: 实验表明，该方法实现了资源高效的信任评估，平衡了开销与准确性。

Conclusion: 提出的方法有效解决了分布式协作中的信任评估问题，提升了资源利用率和协作效率。

Abstract: In collaborative systems, the effective completion of tasks hinges on
task-specific trust evaluations of potential devices for distributed
collaboration. However, the complexity of tasks, the spatiotemporal dynamism of
distributed device resources, and the inevitable assessment overhead
dramatically increase the complexity and resource consumption of the trust
evaluation process. As a result, ill-timed or overly frequent trust evaluations
can reduce utilization rate of constrained resources, negatively affecting
collaborative task execution. To address this challenge, this paper proposes an
autonomous trust orchestration method based on a new concept of semantic
chain-of-trust. Our technique employs agentic AI and hypergraph to establish
and maintain trust relationships among devices. By leveraging its strengths in
autonomous perception, task decomposition, and semantic reasoning, we propose
agentic AI to perceive device states and autonomously perform trust evaluations
of collaborators based on historical performance data only during device idle
periods, thereby enabling efficient utilization of distributed resources. In
addition, agentic AI performs task-specific trust evaluations on collaborator
resources by analyzing the alignment between resource capabilities and task
requirements. Moreover, by maintaining a trust hypergraph embedded with trust
semantics for each device, agentic AI enables hierarchical management of
collaborators and identifies collaborators requiring trust evaluation based on
trust semantics, thereby achieving a balance between overhead and trust
accuracy. Furthermore, local trust hypergraphs from multiple devices can be
chained together to support multi-hop collaboration, enabling efficient
coordination in large-scale systems. Experimental results demonstrate that the
proposed method achieves resource-efficient trust evaluation.

</details>


### [18] [MemoCue: Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying](https://arxiv.org/abs/2507.23633)
*Qian Zhao,Zhuo Sun,Bin Guo,Zhiwen Yu*

Main category: cs.AI

TL;DR: 提出了一种基于策略引导的代理辅助记忆回忆方法，通过设计策略将原始查询转化为线索丰富的查询，以帮助用户回忆记忆。


<details>
  <summary>Details</summary>
Motivation: 传统方法中代理的记忆模块大小有限，影响了记忆回忆的完整性和性能。受记忆理论启发，通过有效线索主动激活用户的相关记忆。

Method: 设计了Recall Router框架，包括5W Recall Map分类记忆查询场景和15种策略模式，结合蒙特卡洛树搜索算法优化策略选择和响应生成。

Result: 实验表明MemoCue在回忆灵感上优于基于LLM的方法17.74%，人类评估也验证了其在记忆回忆应用中的优势。

Conclusion: MemoCue通过策略引导和优化算法显著提升了记忆回忆效果，为代理辅助记忆回忆提供了新思路。

Abstract: Agent-assisted memory recall is one critical research problem in the field of
human-computer interaction. In conventional methods, the agent can retrieve
information from its equipped memory module to help the person recall
incomplete or vague memories. The limited size of memory module hinders the
acquisition of complete memories and impacts the memory recall performance in
practice. Memory theories suggest that the person's relevant memory can be
proactively activated through some effective cues. Inspired by this, we propose
a novel strategy-guided agent-assisted memory recall method, allowing the agent
to transform an original query into a cue-rich one via the judiciously designed
strategy to help the person recall memories. To this end, there are two key
challenges. (1) How to choose the appropriate recall strategy for diverse
forgetting scenarios with distinct memory-recall characteristics? (2) How to
obtain the high-quality responses leveraging recall strategies, given only
abstract and sparsely annotated strategy patterns? To address the challenges,
we propose a Recall Router framework. Specifically, we design a 5W Recall Map
to classify memory queries into five typical scenarios and define fifteen
recall strategy patterns across the corresponding scenarios. We then propose a
hierarchical recall tree combined with the Monte Carlo Tree Search algorithm to
optimize the selection of strategy and the generation of strategy responses. We
construct an instruction tuning dataset and fine-tune multiple open-source
large language models (LLMs) to develop MemoCue, an agent that excels in
providing memory-inspired responses. Experiments on three representative
datasets show that MemoCue surpasses LLM-based methods by 17.74% in recall
inspiration. Further human evaluation highlights its advantages in
memory-recall applications.

</details>


### [19] [Personalized Education with Ranking Alignment Recommendation](https://arxiv.org/abs/2507.23664)
*Haipeng Liu,Yuxuan Liu,Ting Long*

Main category: cs.AI

TL;DR: 论文提出了一种名为RAR的个性化问题推荐方法，通过将协作思想融入探索机制，解决了传统强化学习方法在训练中探索效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法将个性化问题推荐建模为马尔可夫决策过程并使用强化学习解决，但探索效率低，难以在训练中为每个学生找到最佳问题。

Method: 提出了Ranking Alignment Recommendation (RAR)，将协作思想融入探索机制，以在有限训练轮次内实现更高效的探索。

Result: 实验表明，RAR显著提升了推荐性能，且该框架可应用于任何基于强化学习的问题推荐系统。

Conclusion: RAR通过改进探索机制，有效解决了传统方法的局限性，为个性化问题推荐提供了更高效的解决方案。

Abstract: Personalized question recommendation aims to guide individual students
through questions to enhance their mastery of learning targets. Most previous
methods model this task as a Markov Decision Process and use reinforcement
learning to solve, but they struggle with efficient exploration, failing to
identify the best questions for each student during training. To address this,
we propose Ranking Alignment Recommendation (RAR), which incorporates
collaborative ideas into the exploration mechanism, enabling more efficient
exploration within limited training episodes. Experiments show that RAR
effectively improves recommendation performance, and our framework can be
applied to any RL-based question recommender. Our code is available in
https://github.com/wuming29/RAR.git.

</details>


### [20] [TextQuests: How Good are LLMs at Text-Based Video Games?](https://arxiv.org/abs/2507.23701)
*Long Phan,Mantas Mazeika,Andy Zou,Dan Hendrycks*

Main category: cs.AI

TL;DR: TextQuests是一个基于交互式小说的基准测试，用于评估AI代理在长上下文推理和自主问题解决中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能充分评估AI代理在探索性环境中的自主推理能力，因此需要新的测试方法。

Method: 基于Infocom交互式小说游戏设计TextQuests基准测试，禁止外部工具使用，专注于长上下文推理。

Result: TextQuests提供了一个有效评估AI代理在复杂、探索性环境中自主问题解决能力的平台。

Conclusion: TextQuests推动了AI代理在长上下文推理和自主问题解决能力方面的研究。

Abstract: Evaluating AI agents within complex, interactive environments that mirror
real-world challenges is critical for understanding their practical
capabilities. While existing agent benchmarks effectively assess skills like
tool use or performance on structured tasks, they often do not fully capture an
agent's ability to operate autonomously in exploratory environments that demand
sustained, self-directed reasoning over a long and growing context. To spur the
development of agents capable of more robust intrinsic reasoning over long
horizons, we introduce TextQuests, a benchmark based on the Infocom suite of
interactive fiction games. These text-based adventures, which can take human
players over 30 hours and require hundreds of precise actions to solve, serve
as an effective proxy for evaluating AI agents on focused, stateful tasks. The
benchmark is specifically designed to assess an LLM agent's capacity for
self-contained problem-solving by precluding the use of external tools, thereby
focusing on intrinsic long-context reasoning capabilities in an exploratory
environment characterized by the need for trial-and-error learning and
sustained problem-solving within a single interactive session. We release
TextQuests at https://textquests.ai.

</details>


### [21] [Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving](https://arxiv.org/abs/2507.23726)
*Luoxin Chen,Jinming Gu,Liankai Huang,Wenhao Huang,Zhicheng Jiang,Allan Jie,Xiaoran Jin,Xing Jin,Chenggang Li,Kaijing Ma,Cheng Ren,Jiawei Shen,Wenlei Shi,Tong Sun,He Sun,Jiahui Wang,Siran Wang,Zhihong Wang,Chenrui Wei,Shufa Wei,Yonghui Wu,Yuchen Wu,Yihang Xia,Huajian Xin,Fan Yang,Huaiyuan Ying,Hongyi Yuan,Zheng Yuan,Tianyang Zhan,Chi Zhang,Yue Zhang,Ge Zhang,Tianyun Zhao,Jianqiu Zhao,Yichi Zhou,Thomas Hanwen Zhu*

Main category: cs.AI

TL;DR: Seed-Prover 是一种基于强化学习和形式化验证的定理证明模型，通过迭代优化证明过程，显著提升了数学推理能力，并在 IMO 和 PutnamBench 等竞赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在数学推理中缺乏明确的监督信号，尤其是在定理证明任务中表现不佳。形式化验证语言（如 Lean）可以提供清晰的监督信号，从而提升模型的推理能力。

Method: Seed-Prover 是一种基于强化学习的模型，通过 Lean 的反馈、已证明的引理和自我总结来迭代优化证明过程。此外，还设计了三种推理策略以支持深度和广度的推理。

Result: Seed-Prover 在 IMO 和 MiniF2F 等竞赛中表现优异，证明了 78.1% 的 IMO 问题，并在 PutnamBench 上超过 50%。此外，Seed-Geometry 在几何推理中表现优于现有引擎。

Conclusion: Seed-Prover 和 Seed-Geometry 的结合显著提升了自动数学推理的能力，展示了形式化验证与长链推理的有效性。

Abstract: LLMs have demonstrated strong mathematical reasoning abilities by leveraging
reinforcement learning with long chain-of-thought, yet they continue to
struggle with theorem proving due to the lack of clear supervision signals when
solely using natural language. Dedicated domain-specific languages like Lean
provide clear supervision via formal verification of proofs, enabling effective
training through reinforcement learning. In this work, we propose
\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover
can iteratively refine its proof based on Lean feedback, proved lemmas, and
self-summarization. To solve IMO-level contest problems, we design three
test-time inference strategies that enable both deep and broad reasoning.
Seed-Prover proves $78.1\%$ of formalized past IMO problems, saturates MiniF2F,
and achieves over 50\% on PutnamBench, outperforming the previous
state-of-the-art by a large margin. To address the lack of geometry support in
Lean, we introduce a geometry reasoning engine \textbf{Seed-Geometry}, which
outperforms previous formal geometry engines. We use these two systems to
participate in IMO 2025 and fully prove 5 out of 6 problems. This work
represents a significant advancement in automated mathematical reasoning,
demonstrating the effectiveness of formal verification with long
chain-of-thought reasoning.

</details>


### [22] [CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks](https://arxiv.org/abs/2507.23751)
*Ping Yu,Jack Lanchantin,Tianlu Wang,Weizhe Yuan,Olga Golovneva,Ilia Kulikov,Sainbayar Sukhbaatar,Jason Weston,Jing Xu*

Main category: cs.AI

TL;DR: CoT-Self-Instruct是一种通过Chain-of-Thought生成高质量合成数据的方法，显著提升了LLM在可验证推理和非可验证任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有训练数据集在可验证推理和指令跟随任务中表现不足，需要更高质量的合成数据来提升模型性能。

Method: 基于种子任务，通过Chain-of-Thought引导LLM生成新提示，并通过自动指标过滤高质量数据。

Result: 在MATH500等可验证推理任务和AlpacaEval 2.0等非可验证任务中，性能显著优于现有数据集。

Conclusion: CoT-Self-Instruct能有效生成高质量合成数据，提升LLM在多种任务中的表现。

Abstract: We propose CoT-Self-Instruct, a synthetic data generation method that
instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the
given seed tasks, and then to generate a new synthetic prompt of similar
quality and complexity for use in LLM training, followed by filtering for
high-quality data with automatic metrics. In verifiable reasoning, our
synthetic data significantly outperforms existing training datasets, such as
s1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For
non-verifiable instruction-following tasks, our method surpasses the
performance of human or standard self-instruct prompts on both AlpacaEval 2.0
and Arena-Hard.

</details>


### [23] [SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model](https://arxiv.org/abs/2507.23773)
*Mingkai Deng,Jinyu Hou,Yilin Shen,Hongxia Jin,Graham Neubig,Zhiting Hu,Eric Xing*

Main category: cs.AI

TL;DR: SimuRA是一种基于世界模型的通用AI代理架构，通过模拟规划克服自回归LLM的局限性，在复杂任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理多为单一任务设计，缺乏通用性和可扩展性，而人类通过模拟推理实现通用性。SimuRA旨在实现类似人类的通用推理能力。

Method: 基于最优代理的理论框架，SimuRA引入世界模型进行模拟规划，利用LLM的潜在语言空间实现灵活环境适应。

Result: 在航班搜索任务中，SimuRA成功率从0%提升至32.2%，世界模型规划比自回归规划优势高达124%。

Conclusion: SimuRA展示了世界模型模拟作为推理范式的优势，为训练通用AI代理提供了可能。

Abstract: AI agents built on large language models (LLMs) hold enormous promise, but
current practice focuses on a one-task-one-agent approach, which not only falls
short of scalability and generality, but also suffers from the fundamental
limitations of autoregressive LLMs. On the other hand, humans are general
agents who reason by mentally simulating the outcomes of their actions and
plans. Moving towards a more general and powerful AI agent, we introduce
SimuRA, a goal-oriented architecture for generalized agentic reasoning. Based
on a principled formulation of optimal agent in any environment, \modelname
overcomes the limitations of autoregressive reasoning by introducing a world
model for planning via simulation. The generalized world model is implemented
using LLM, which can flexibly plan in a wide range of environments using the
concept-rich latent space of natural language. Experiments on difficult web
browsing tasks show that \modelname improves the success of flight search from
0\% to 32.2\%. World-model-based planning, in particular, shows consistent
advantage of up to 124\% over autoregressive planning, demonstrating the
advantage of world model simulation as a reasoning paradigm. We are excited
about the possibility for training a single, general agent model based on LLMs
that can act superintelligently in all environments. To start, we make SimuRA,
a web-browsing agent built on \modelname with pretrained LLMs, available as a
research demo for public testing.

</details>
