<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 83]
- [cs.AI](#cs.AI) [Total: 41]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [YOLO11-CR: a Lightweight Convolution-and-Attention Framework for Accurate Fatigue Driving Detection](https://arxiv.org/abs/2508.13205)
*Zhebin Jin,Ligang Dong*

Main category: cs.CV

TL;DR: YOLO11-CR是一种轻量级高效的目标检测模型，专为实时疲劳检测设计，通过CAFM和RCM模块提升特征表达和空间定位能力，实验表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 驾驶疲劳检测对智能交通系统至关重要，但现有方法存在侵入性、硬件依赖或鲁棒性不足的问题，视觉技术虽非侵入但面临小目标检测和多尺度特征建模的挑战。

Method: 提出YOLO11-CR模型，包含CAFM模块（融合CNN与Transformer特征）和RCM模块（捕获水平和垂直上下文信息），优化特征表达和空间定位。

Result: 在DSM数据集上，YOLO11-CR的精确率87.17%、召回率83.86%、mAP@50为88.09%，显著优于基线模型。消融实验验证了模块的有效性。

Conclusion: YOLO11-CR为车载疲劳监测提供了实用高效的解决方案，具备实际部署潜力，未来可扩展至时序建模和多模态数据集成。

Abstract: Driver fatigue detection is of paramount importance for intelligent
transportation systems due to its critical role in mitigating road traffic
accidents. While physiological and vehicle dynamics-based methods offer
accuracy, they are often intrusive, hardware-dependent, and lack robustness in
real-world environments. Vision-based techniques provide a non-intrusive and
scalable alternative, but still face challenges such as poor detection of small
or occluded objects and limited multi-scale feature modeling. To address these
issues, this paper proposes YOLO11-CR, a lightweight and efficient object
detection model tailored for real-time fatigue detection. YOLO11-CR introduces
two key modules: the Convolution-and-Attention Fusion Module (CAFM), which
integrates local CNN features with global Transformer-based context to enhance
feature expressiveness; and the Rectangular Calibration Module (RCM), which
captures horizontal and vertical contextual information to improve spatial
localization, particularly for profile faces and small objects like mobile
phones. Experiments on the DSM dataset demonstrated that YOLO11-CR achieves a
precision of 87.17%, recall of 83.86%, mAP@50 of 88.09%, and mAP@50-95 of
55.93%, outperforming baseline models significantly. Ablation studies further
validate the effectiveness of the CAFM and RCM modules in improving both
sensitivity and localization accuracy. These results demonstrate that YOLO11-CR
offers a practical and high-performing solution for in-vehicle fatigue
monitoring, with strong potential for real-world deployment and future
enhancements involving temporal modeling, multi-modal data integration, and
embedded optimization.

</details>


### [2] [MIRAGE: Towards AI-Generated Image Detection in the Wild](https://arxiv.org/abs/2508.13223)
*Cheng Xia,Manxi Lin,Jiexiang Tan,Xiaoxiong Du,Yang Qiu,Junjun Zheng,Xiangheng Kong,Yuning Jiang,Bo Zheng*

Main category: cs.CV

TL;DR: 论文提出Mirage基准和Mirage-R1模型，用于检测真实场景中的AI生成图像（AIGI），显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: AI生成图像（AIGI）的传播威胁信息安全和公众信任，现有检测器在真实场景中泛化能力不足。

Method: 构建Mirage基准模拟真实AIGI复杂性，提出Mirage-R1模型，结合启发式到分析推理和强化学习。

Result: Mirage-R1在Mirage和公开基准上分别领先现有方法5%和10%。

Conclusion: Mirage-R1有效平衡推理速度和性能，为真实场景AIGI检测提供新解决方案。

Abstract: The spreading of AI-generated images (AIGI), driven by advances in generative
AI, poses a significant threat to information security and public trust.
Existing AIGI detectors, while effective against images in clean laboratory
settings, fail to generalize to in-the-wild scenarios. These real-world images
are noisy, varying from ``obviously fake" images to realistic ones derived from
multiple generative models and further edited for quality control. We address
in-the-wild AIGI detection in this paper. We introduce Mirage, a challenging
benchmark designed to emulate the complexity of in-the-wild AIGI. Mirage is
constructed from two sources: (1) a large corpus of Internet-sourced AIGI
verified by human experts, and (2) a synthesized dataset created through the
collaboration between multiple expert generators, closely simulating the
realistic AIGI in the wild. Building on this benchmark, we propose Mirage-R1, a
vision-language model with heuristic-to-analytic reasoning, a reflective
reasoning mechanism for AIGI detection. Mirage-R1 is trained in two stages: a
supervised-fine-tuning cold start, followed by a reinforcement learning stage.
By further adopting an inference-time adaptive thinking strategy, Mirage-R1 is
able to provide either a quick judgment or a more robust and accurate
conclusion, effectively balancing inference speed and performance. Extensive
experiments show that our model leads state-of-the-art detectors by 5% and 10%
on Mirage and the public benchmark, respectively. The benchmark and code will
be made publicly available.

</details>


### [3] [DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool Interleaved Vision-Language Model](https://arxiv.org/abs/2508.13238)
*Qian Chen,Xianyin Zhang,Lifan Guo,Feng Chen,Chi Zhang*

Main category: cs.CV

TL;DR: DianJin-OCR-R1是一个推理增强的框架，通过结合推理和工具交互的视觉语言模型（VLMs）来解决大型视觉语言模型（LVLMs）在OCR任务中的幻觉问题和性能不足。


<details>
  <summary>Details</summary>
Motivation: LVLMs在OCR任务中容易产生幻觉（生成不存在的文本），且性能不如针对特定任务训练的专家模型。

Method: DianJin-OCR-R1结合自身OCR能力和外部专家模型的结果，通过重新推理生成最终识别内容。

Result: 在ReST和OmniDocBench上的实验表明，DianJin-OCR-R1优于非推理模型和专家OCR模型。

Conclusion: 该方法有效解决了LVLMs的幻觉问题，并提升了OCR任务的性能。

Abstract: Recent advances in large vision-language models (LVLMs) have enabled a new
paradigm of end-to-end document image parsing, excelling in Optical Character
Recognition (OCR) tasks such as text, table, and formula recognition. However,
generative LVLMs, similarly to large language models (LLMs), are prone to
hallucinations--generating words that do not exist in input images.
Furthermore, LVLMs are designed for general purposes and tend to be less
effective on OCR tasks compared to expert models that are trained on
domain-specific datasets. In this paper, we propose DianJin-OCR-R1, a
reasoning-enhanced framework designed to address these limitations through
training reasoning-and-tool interleaved VLMs. Given a recognition instruction,
our DianJin-OCR-R1 model first recognizes the content in the input image by its
own OCR capabilities, and then calls other tools (i.e., other expert models) to
obtain their results as references, finally looks again the image and rethinks
about the reasoning process to provide the final recognized content. Since
architectures of expert models are tailored for specific OCR tasks, which makes
them less prone to hallucinations, their results can help VLMs mitigate
hallucinations. Additionally, expert models are typically smaller in scale and
easy to iterate, enabling performance improvements for VLMs at a lower cost. We
evaluate our model on ReST and OmniDocBench, and experimental results show that
our DianJin-OCR-R1 models consistently outperform their non-reasoning
counterparts and expert OCR models, which proves the effectiveness of our
method.

</details>


### [4] [Exploration of Deep Learning Based Recognition for Urdu Text](https://arxiv.org/abs/2508.13245)
*Sumaiya Fazal,Sheeraz Ahmed*

Main category: cs.CV

TL;DR: 提出了一种基于卷积神经网络（CNN）的乌尔都语光学字符识别系统，通过自动特征学习技术处理复杂的几何和形态结构，实现了99%的组件分类准确率。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语因其复杂的几何和形态结构难以分类，传统的基于分割的识别方法错误率高。

Method: 采用基于组件的分类方法，利用CNN进行自动特征学习，并通过连接组件技术筛选图像，使用分层神经网络处理字符排列和组件分类。

Result: 模型在组件分类上达到了99%的准确率。

Conclusion: 该方法有效解决了乌尔都语字符识别的挑战，展示了CNN在复杂脚本语言处理中的潜力。

Abstract: Urdu is a cursive script language and has similarities with Arabic and many
other South Asian languages. Urdu is difficult to classify due to its complex
geometrical and morphological structure. Character classification can be
processed further if segmentation technique is efficient, but due to context
sensitivity in Urdu, segmentation-based recognition often results with high
error rate. Our proposed approach for Urdu optical character recognition system
is a component-based classification relying on automatic feature learning
technique called convolutional neural network. CNN is trained and tested on
Urdu text dataset, which is generated through permutation process of three
characters and further proceeds to discarding unnecessary images by applying
connected component technique in order to obtain ligature only. Hierarchical
neural network is implemented with two levels to deal with three degrees of
character permutations and component classification Our model successfully
achieved 0.99% for component classification.

</details>


### [5] [CLoE: Curriculum Learning on Endoscopic Images for Robust MES Classification](https://arxiv.org/abs/2508.13280)
*Zeynep Ozdemir,Hacer Yalim Keles,Omer Ozgur Tanriover*

Main category: cs.CV

TL;DR: CLoE框架通过课程学习和数据增强提升溃疡性结肠炎内镜图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决MES分类中标签噪声和顺序性被忽视的问题。

Method: 结合图像质量评估和ResizeMix增强的课程学习框架。

Result: 在LIMUC和HyperKvasir数据集上表现优于基线模型。

Conclusion: CLoE展示了难度感知训练在标签不确定性下提升顺序分类的潜力。

Abstract: Estimating disease severity from endoscopic images is essential in assessing
ulcerative colitis, where the Mayo Endoscopic Subscore (MES) is widely used to
grade inflammation. However, MES classification remains challenging due to
label noise from inter-observer variability and the ordinal nature of the
score, which standard models often ignore. We propose CLoE, a curriculum
learning framework that accounts for both label reliability and ordinal
structure. Image quality, estimated via a lightweight model trained on Boston
Bowel Preparation Scale (BBPS) labels, is used as a proxy for annotation
confidence to order samples from easy (clean) to hard (noisy). This curriculum
is further combined with ResizeMix augmentation to improve robustness.
Experiments on the LIMUC and HyperKvasir datasets, using both CNNs and
Transformers, show that CLoE consistently improves performance over strong
supervised and self-supervised baselines. For instance, ConvNeXt-Tiny reaches
82.5\% accuracy and a QWK of 0.894 on LIMUC with low computational cost. These
results highlight the potential of difficulty-aware training strategies for
improving ordinal classification under label uncertainty. Code will be released
at https://github.com/zeynepozdemir/CLoE.

</details>


### [6] [GaitCrafter: Diffusion Model for Biometric Preserving Gait Synthesis](https://arxiv.org/abs/2508.13300)
*Sirshapan Mitra,Yogesh S. Rawat*

Main category: cs.CV

TL;DR: GaitCrafter是一个基于扩散模型的框架，用于合成逼真的步态序列，解决了步态识别中数据不足和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 步态识别因缺乏大规模标记数据集和收集多样化步态样本的困难而受限，同时需保护隐私。

Method: GaitCrafter通过训练视频扩散模型生成时间一致且保留身份的步态序列，并可控制生成条件（如服装、视角等）。

Result: 合成样本提升了步态识别性能，尤其在挑战性条件下；还能生成新身份样本，保护真实数据隐私。

Conclusion: GaitCrafter为高质量、可控且隐私保护的步态数据生成提供了重要进展。

Abstract: Gait recognition is a valuable biometric task that enables the identification
of individuals from a distance based on their walking patterns. However, it
remains limited by the lack of large-scale labeled datasets and the difficulty
of collecting diverse gait samples for each individual while preserving
privacy. To address these challenges, we propose GaitCrafter, a diffusion-based
framework for synthesizing realistic gait sequences in the silhouette domain.
Unlike prior works that rely on simulated environments or alternative
generative models, GaitCrafter trains a video diffusion model from scratch,
exclusively on gait silhouette data. Our approach enables the generation of
temporally consistent and identity-preserving gait sequences. Moreover, the
generation process is controllable-allowing conditioning on various covariates
such as clothing, carried objects, and view angle. We show that incorporating
synthetic samples generated by GaitCrafter into the gait recognition pipeline
leads to improved performance, especially under challenging conditions.
Additionally, we introduce a mechanism to generate novel identities-synthetic
individuals not present in the original dataset-by interpolating identity
embeddings. These novel identities exhibit unique, consistent gait patterns and
are useful for training models while maintaining privacy of real subjects.
Overall, our work takes an important step toward leveraging diffusion models
for high-quality, controllable, and privacy-aware gait data generation.

</details>


### [7] [Prune2Drive: A Plug-and-Play Framework for Accelerating Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2508.13305)
*Minhao Xiong,Zichen Wen,Zhuangcheng Gu,Xuyang Liu,Rui Zhang,Hengrui Kang,Jiabing Yang,Junyuan Zhang,Weijia Li,Conghui He,Yafei Wang,Linfeng Zhang*

Main category: cs.CV

TL;DR: Prune2Drive是一种用于自动驾驶中多视角视觉语言模型（VLMs）的视觉令牌剪枝框架，显著降低计算开销和内存消耗。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的多视角VLMs因处理高分辨率图像产生大量视觉令牌，导致计算和内存开销大，影响部署效率。

Method: 提出多样性感知令牌选择机制和视图自适应剪枝控制器，无需重新训练模型或访问注意力图。

Result: 在DriveLM和DriveLMM-o1基准测试中，仅保留10%令牌即可实现6.4倍加速和13.4%的FLOPs消耗，性能仅下降3%。

Conclusion: Prune2Drive是一种高效且兼容性强的解决方案，显著提升了多视角VLMs在自动驾驶中的实用性。

Abstract: Vision-Language Models (VLMs) have emerged as a promising paradigm in
autonomous driving (AD), offering a unified framework for perception,
reasoning, and decision-making by jointly modeling visual inputs and natural
language instructions. However, their deployment is hindered by the significant
computational overhead incurred when processing high-resolution, multi-view
images, a standard setup in AD systems with six or more synchronized cameras.
This overhead stems from the large number of visual tokens generated during
encoding, increasing inference latency and memory consumption due to the
quadratic complexity of self-attention. To address these challenges, we propose
Prune2Drive, a plug-and-play visual token pruning framework for multi-view VLMs
in autonomous driving. Prune2Drive introduces two core innovations: (i) a
diversity-aware token selection mechanism inspired by farthest point sampling,
which prioritizes semantic and spatial coverage across views rather than
relying solely on attention scores, and (ii) a view-adaptive pruning controller
that learns optimal pruning ratios for each camera view based on their
importance to downstream driving tasks. Unlike prior methods, Prune2Drive does
not require model retraining or access to attention maps, making it compatible
with modern efficient attention implementations. Extensive experiments on two
large-scale multi-view driving benchmarks, DriveLM and DriveLMM-o1, show that
Prune2Drive achieves significant speedups and memory savings while maintaining
or improving task performance. When retaining only 10% of the visual tokens,
our method achieves a 6.40$\times$ speedup in the prefilling phase and consumes
13.4% of the original FLOPs, with only a 3% performance drop on the DriveLM
benchmark.

</details>


### [8] [DAASH: A Meta-Attack Framework for Synthesizing Effective and Stealthy Adversarial Examples](https://arxiv.org/abs/2508.13309)
*Abdullah Al Nomaan Nafi,Habibur Rahaman,Zafaryab Haider,Tanzim Mahfuz,Fnu Suya,Swarup Bhunia,Prabuddha Chakraborty*

Main category: cs.CV

TL;DR: DAASH是一个完全可微分的元攻击框架，通过组合现有的Lp攻击方法生成感知对齐的对抗样本，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Lp范数约束的对抗样本与人类感知对齐不佳，且缺乏利用Lp攻击提升感知效果的方法。

Method: DAASH采用多阶段策略，动态聚合多个基础攻击的候选样本，并通过元损失函数联合优化分类错误和感知失真。

Result: 在CIFAR-10、CIFAR-100和ImageNet上，DAASH攻击成功率和视觉质量显著优于AdvAD等现有方法。

Conclusion: DAASH是一种实用且强大的基准方法，无需针对新防御手工调整攻击策略。

Abstract: Numerous techniques have been proposed for generating adversarial examples in
white-box settings under strict Lp-norm constraints. However, such norm-bounded
examples often fail to align well with human perception, and only recently have
a few methods begun specifically exploring perceptually aligned adversarial
examples. Moreover, it remains unclear whether insights from Lp-constrained
attacks can be effectively leveraged to improve perceptual efficacy. In this
paper, we introduce DAASH, a fully differentiable meta-attack framework that
generates effective and perceptually aligned adversarial examples by
strategically composing existing Lp-based attack methods. DAASH operates in a
multi-stage fashion: at each stage, it aggregates candidate adversarial
examples from multiple base attacks using learned, adaptive weights and
propagates the result to the next stage. A novel meta-loss function guides this
process by jointly minimizing misclassification loss and perceptual distortion,
enabling the framework to dynamically modulate the contribution of each base
attack throughout the stages. We evaluate DAASH on adversarially trained models
across CIFAR-10, CIFAR-100, and ImageNet. Despite relying solely on
Lp-constrained based methods, DAASH significantly outperforms state-of-the-art
perceptual attacks such as AdvAD -- achieving higher attack success rates
(e.g., 20.63\% improvement) and superior visual quality, as measured by SSIM,
LPIPS, and FID (improvements $\approx$ of 11, 0.015, and 5.7, respectively).
Furthermore, DAASH generalizes well to unseen defenses, making it a practical
and strong baseline for evaluating robustness without requiring handcrafted
adaptive attacks for each new defense.

</details>


### [9] [Automated Assessment of Aesthetic Outcomes in Facial Plastic Surgery](https://arxiv.org/abs/2508.13363)
*Pegah Varghaei,Kiran Abraham-Aggarwal,Manoj T. Abraham,Arun Ross*

Main category: cs.CV

TL;DR: 提出了一种可扩展、可解释的计算机视觉框架，用于量化面部整形手术的美学效果。


<details>
  <summary>Details</summary>
Motivation: 通过量化分析面部整形手术的效果，为手术规划、患者咨询和结果评估提供数据支持。

Method: 利用自动标志点检测、几何面部对称性计算、基于深度学习的年龄估计和鼻形态分析。

Result: 在鼻整形患者中，96.2%的患者至少一项鼻部测量指标显著改善；整体面部对称性或感知年龄显著改善的比例为71.3%。

Conclusion: 该框架为数据驱动的手术规划和客观结果评估提供了可重复的定量基准和新数据集。

Abstract: We introduce a scalable, interpretable computer-vision framework for
quantifying aesthetic outcomes of facial plastic surgery using frontal
photographs. Our pipeline leverages automated landmark detection, geometric
facial symmetry computation, deep-learning-based age estimation, and nasal
morphology analysis. To perform this study, we first assemble the largest
curated dataset of paired pre- and post-operative facial images to date,
encompassing 7,160 photographs from 1,259 patients. This dataset includes a
dedicated rhinoplasty-only subset consisting of 732 images from 366 patients,
96.2% of whom showed improvement in at least one of the three nasal
measurements with statistically significant group-level change. Among these
patients, the greatest statistically significant improvements (p < 0.001)
occurred in the alar width to face width ratio (77.0%), nose length to face
height ratio (41.5%), and alar width to intercanthal ratio (39.3%). Among the
broader frontal-view cohort, comprising 989 rigorously filtered subjects, 71.3%
exhibited significant enhancements in global facial symmetry or perceived age
(p < 0.01). Importantly, our analysis shows that patient identity remains
consistent post-operatively, with True Match Rates of 99.5% and 99.6% at a
False Match Rate of 0.01% for the rhinoplasty-specific and general patient
cohorts, respectively. Additionally, we analyze inter-practitioner variability
in improvement rates. By providing reproducible, quantitative benchmarks and a
novel dataset, our pipeline facilitates data-driven surgical planning, patient
counseling, and objective outcome evaluation across practices.

</details>


### [10] [Applications of Small Language Models in Medical Imaging Classification with a Focus on Prompt Strategies](https://arxiv.org/abs/2508.13378)
*Yiting Wang,Ziwei Wang,Jiachen Zhong,Di Zhu,Weiyi Li*

Main category: cs.CV

TL;DR: 研究探讨小型语言模型（SLMs）在医疗影像分类任务中的表现，通过不同模型和提示设计比较，发现优化提示可显著提升SLMs在医疗应用中的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在医疗资源受限环境中存在计算成本高、可访问性有限和数据隐私问题，因此研究SLMs的替代方案。

Method: 使用NIH胸部X光数据集，评估多种SLMs在分类胸部X光位置（AP vs. PA）任务中的表现，比较三种提示策略。

Result: 某些SLMs通过精心设计的提示达到与LLMs竞争的准确性。

Conclusion: 提示工程可显著提升SLMs在医疗应用中的性能，无需用户具备深度AI专业知识。

Abstract: Large language models (LLMs) have shown remarkable capabilities in natural
language processing and multi-modal understanding. However, their high
computational cost, limited accessibility, and data privacy concerns hinder
their adoption in resource-constrained healthcare environments. This study
investigates the performance of small language models (SLMs) in a medical
imaging classification task, comparing different models and prompt designs to
identify the optimal combination for accuracy and usability. Using the NIH
Chest X-ray dataset, we evaluate multiple SLMs on the task of classifying chest
X-ray positions (anteroposterior [AP] vs. posteroanterior [PA]) under three
prompt strategies: baseline instruction, incremental summary prompts, and
correction-based reflective prompts. Our results show that certain SLMs achieve
competitive accuracy with well-crafted prompts, suggesting that prompt
engineering can substantially enhance SLM performance in healthcare
applications without requiring deep AI expertise from end users.

</details>


### [11] [AIM 2025 Rip Current Segmentation (RipSeg) Challenge Report](https://arxiv.org/abs/2508.13401)
*Andrei Dumitriu,Florin Miron,Florin Tatui,Radu Tudor Ionescu,Radu Timofte,Aakash Ralhan,Florin-Alexandru Vasluianu,Shenyang Qian,Mitchell Harley,Imran Razzak,Yang Song,Pu Luo,Yumei Li,Cong Xu,Jinming Chai,Kexin Zhang,Licheng Jiao,Lingling Li,Siqi Yu,Chao Zhang,Kehuan Song,Fang Liu,Puhua Chen,Xu Liu,Jin Hu,Jinyang Xu,Biao Liu*

Main category: cs.CV

TL;DR: AIM 2025 RipSeg Challenge 是一个专注于自动分割静止图像中离岸流的竞赛，旨在提升海滩安全。竞赛基于最大的离岸流数据集 RipVIS，吸引了75名参与者，最终有5个有效提交。


<details>
  <summary>Details</summary>
Motivation: 离岸流是全球海滩安全的主要威胁，但视觉检测技术尚未充分探索。竞赛旨在推动自动分割技术的发展。

Method: 竞赛使用 RipVIS 数据集，聚焦单类实例分割，采用复合评分（F1、F2、AP50 等）评估参与者。

Result: 表现最佳的方法结合了深度学习架构、领域适应技术和预训练模型，提升了多样条件下的性能。

Conclusion: 报告总结了竞赛的关键挑战和未来方向，为离岸流分割技术的发展提供了参考。

Abstract: This report presents an overview of the AIM 2025 RipSeg Challenge, a
competition designed to advance techniques for automatic rip current
segmentation in still images. Rip currents are dangerous, fast-moving flows
that pose a major risk to beach safety worldwide, making accurate visual
detection an important and underexplored research task. The challenge builds on
RipVIS, the largest available rip current dataset, and focuses on single-class
instance segmentation, where precise delineation is critical to fully capture
the extent of rip currents. The dataset spans diverse locations, rip current
types, and camera orientations, providing a realistic and challenging
benchmark.
  In total, $75$ participants registered for this first edition, resulting in
$5$ valid test submissions. Teams were evaluated on a composite score combining
$F_1$, $F_2$, $AP_{50}$, and $AP_{[50:95]}$, ensuring robust and
application-relevant rankings. The top-performing methods leveraged deep
learning architectures, domain adaptation techniques, pretrained models, and
domain generalization strategies to improve performance under diverse
conditions.
  This report outlines the dataset details, competition framework, evaluation
metrics, and final results, providing insights into the current state of rip
current segmentation. We conclude with a discussion of key challenges, lessons
learned from the submissions, and future directions for expanding RipSeg.

</details>


### [12] [Mitigating Easy Option Bias in Multiple-Choice Question Answering](https://arxiv.org/abs/2508.13428)
*Hao Zhang,Chen Li,Basura Fernando*

Main category: cs.CV

TL;DR: 研究发现视觉问答基准中存在Easy-Options Bias（EOB），导致模型仅通过视觉和选项即可预测答案，无需问题。提出GroundAttack工具生成难负选项，修复偏差。


<details>
  <summary>Details</summary>
Motivation: 揭示并解决视觉问答基准中的EOB问题，以更真实评估视觉语言模型的问答能力。

Method: 通过GroundAttack工具自动生成视觉上合理的难负选项，创建无EOB的新标注数据。

Result: 在无EOB数据上，模型在仅视觉和选项输入下准确率接近随机，结合问题时准确率显著下降。

Conclusion: GroundAttack有效消除EOB，为视觉问答模型提供更真实的评估基准。

Abstract: In this early study, we observe an Easy-Options Bias (EOB) issue in some
multiple-choice Visual Question Answering (VQA) benchmarks such as MMStar,
RealWorldQA, SEED-Bench, Next-QA, STAR benchmark and Video-MME. This bias
allows vision-language models (VLMs) to select the correct answer using only
the vision (V) and options (O) as inputs, without the need for the question
(Q). Through grounding experiments, we attribute the bias to an imbalance in
visual relevance: the correct answer typically aligns more closely with the
visual contents than the negative options in feature space, creating a shortcut
for VLMs to infer the answer via simply vision-option similarity matching. To
fix this, we introduce GroundAttack, a toolkit that automatically generates
hard negative options as visually plausible as the correct answer. We apply it
to the NExT-QA and MMStar datasets, creating new EOB-free annotations. On these
EOB-free annotations, current VLMs approach to random accuracies under (V+O)
settings, and drop to non-saturated accuracies under (V+Q+O) settings,
providing a more realistic evaluation of VLMs' QA ability. Codes and new
annotations will be released soon.

</details>


### [13] [Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference](https://arxiv.org/abs/2508.13439)
*Yunxiang Yang,Ningning Xu,Jidong J. Yang*

Main category: cs.CV

TL;DR: 提出了一种名为VISTA的轻量级视觉语言模型，通过结构化提示和知识蒸馏框架，实现了高效的交通场景理解和风险评估。


<details>
  <summary>Details</summary>
Motivation: 传统方法在复杂动态的真实环境中难以扩展和泛化，因此需要一种更高效且轻量的解决方案。

Method: 使用GPT-4o和o3-mini两个大型视觉语言模型，通过结构化思维链策略生成高质量伪标注，用于训练小型学生模型VISTA。

Result: VISTA在低分辨率交通视频中表现优异，尽管参数较少，但在多个评测指标上表现接近教师模型。

Conclusion: 通过知识蒸馏和多智能体监督，轻量级模型也能具备复杂推理能力，适合边缘设备实时部署。

Abstract: Comprehensive highway scene understanding and robust traffic risk inference
are vital for advancing Intelligent Transportation Systems (ITS) and autonomous
driving. Traditional approaches often struggle with scalability and
generalization, particularly under the complex and dynamic conditions of
real-world environments. To address these challenges, we introduce a novel
structured prompting and knowledge distillation framework that enables
automatic generation of high-quality traffic scene annotations and contextual
risk assessments. Our framework orchestrates two large Vision-Language Models
(VLMs): GPT-4o and o3-mini, using a structured Chain-of-Thought (CoT) strategy
to produce rich, multi-perspective outputs. These outputs serve as
knowledge-enriched pseudo-annotations for supervised fine-tuning of a much
smaller student VLM. The resulting compact 3B-scale model, named VISTA (Vision
for Intelligent Scene and Traffic Analysis), is capable of understanding
low-resolution traffic videos and generating semantically faithful, risk-aware
captions. Despite its significantly reduced parameter count, VISTA achieves
strong performance across established captioning metrics (BLEU-4, METEOR,
ROUGE-L, and CIDEr) when benchmarked against its teacher models. This
demonstrates that effective knowledge distillation and structured multi-agent
supervision can empower lightweight VLMs to capture complex reasoning
capabilities. The compact architecture of VISTA facilitates efficient
deployment on edge devices, enabling real-time risk monitoring without
requiring extensive infrastructure upgrades.

</details>


### [14] [EDTalk++: Full Disentanglement for Controllable Talking Head Synthesis](https://arxiv.org/abs/2508.13442)
*Shuai Tan,Bin Ji*

Main category: cs.CV

TL;DR: EDTalk++ 是一个用于可控说话头生成的全解耦框架，能够独立控制嘴形、头部姿势、眼睛运动和情感表达，支持视频或音频输入。


<details>
  <summary>Details</summary>
Motivation: 现有方法在面部特征的解耦空间探索上存在不足，无法确保特征独立性和多模态输入的共享性。

Method: 使用四个轻量级模块将面部动态分解为嘴、姿势、眼睛和表情四个潜在空间，并通过正交性约束和高效训练策略确保独立性。

Result: 实验证明了 EDTalk++ 的有效性，能够实现独立的面部运动控制和多模态输入支持。

Conclusion: EDTalk++ 提供了一种高效且灵活的面部特征解耦方法，为可控说话头生成提供了新思路。

Abstract: Achieving disentangled control over multiple facial motions and accommodating
diverse input modalities greatly enhances the application and entertainment of
the talking head generation. This necessitates a deep exploration of the
decoupling space for facial features, ensuring that they a) operate
independently without mutual interference and b) can be preserved to share with
different modal inputs, both aspects often neglected in existing methods. To
address this gap, this paper proposes EDTalk++, a novel full disentanglement
framework for controllable talking head generation. Our framework enables
individual manipulation of mouth shape, head pose, eye movement, and emotional
expression, conditioned on video or audio inputs. Specifically, we employ four
lightweight modules to decompose the facial dynamics into four distinct latent
spaces representing mouth, pose, eye, and expression, respectively. Each space
is characterized by a set of learnable bases whose linear combinations define
specific motions. To ensure independence and accelerate training, we enforce
orthogonality among bases and devise an efficient training strategy to allocate
motion responsibilities to each space without relying on external knowledge.
The learned bases are then stored in corresponding banks, enabling shared
visual priors with audio input. Furthermore, considering the properties of each
space, we propose an Audio-to-Motion module for audio-driven talking head
synthesis. Experiments are conducted to demonstrate the effectiveness of
EDTalk++.

</details>


### [15] [Revisiting MLLM Token Technology through the Lens of Classical Visual Coding](https://arxiv.org/abs/2508.13460)
*Jinming Liu,Junyan Lin,Yuntao Wei,Kele Shao,Keda Tao,Jianguo Huang,Xudong Yang,Zhibo Chen,Huan Wang,Xin Jin*

Main category: cs.CV

TL;DR: 本文通过视觉编码的成熟原则重新审视MLLM的token技术，建立了统一框架，并探讨了双向优化的可能性。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过视觉编码原则提升MLLM token技术的效率和鲁棒性，同时利用token技术改进下一代语义视觉编解码器设计。

Method: 建立统一框架，系统比较token技术与视觉编码的模块化差异，并探讨双向优化的潜力。

Result: 提出了首个全面的MLLM token与视觉编码技术比较，为高效多模态模型和强大视觉编解码器设计铺路。

Conclusion: 研究为未来多模态模型和视觉编码技术的发展提供了新视角和潜在方向。

Abstract: Classical visual coding and Multimodal Large Language Model (MLLM) token
technology share the core objective - maximizing information fidelity while
minimizing computational cost. Therefore, this paper reexamines MLLM token
technology, including tokenization, token compression, and token reasoning,
through the established principles of long-developed visual coding area. From
this perspective, we (1) establish a unified formulation bridging token
technology and visual coding, enabling a systematic, module-by-module
comparative analysis; (2) synthesize bidirectional insights, exploring how
visual coding principles can enhance MLLM token techniques' efficiency and
robustness, and conversely, how token technology paradigms can inform the
design of next-generation semantic visual codecs; (3) prospect for promising
future research directions and critical unsolved challenges. In summary, this
study presents the first comprehensive and structured technology comparison of
MLLM token and visual coding, paving the way for more efficient multimodal
models and more powerful visual codecs simultaneously.

</details>


### [16] [Vision Transformers for Kidney Stone Image Classification: A Comparative Study with CNNs](https://arxiv.org/abs/2508.13461)
*Ivan Reyes-Amezcua,Francisco Lopez-Tiro,Clement Larose,Andres Mendez-Vazquez,Gilberto Ochoa-Ruiz,Christian Daul*

Main category: cs.CV

TL;DR: 比较Vision Transformers (ViTs)与CNN在肾结石内窥镜图像分类中的性能，ViT表现更优。


<details>
  <summary>Details</summary>
Motivation: 肾结石分类对个性化治疗和预防复发至关重要，但CNN在长距离依赖捕捉上表现有限。

Method: 在两种离体数据集上比较ViT和ResNet50的性能。

Result: ViT在复杂图像子集上准确率达95.2%，显著优于ResNet50的64.5%。

Conclusion: ViT在肾结石图像分类中优于CNN，是可扩展的替代方案。

Abstract: Kidney stone classification from endoscopic images is critical for
personalized treatment and recurrence prevention. While convolutional neural
networks (CNNs) have shown promise in this task, their limited ability to
capture long-range dependencies can hinder performance under variable imaging
conditions. This study presents a comparative analysis between Vision
Transformers (ViTs) and CNN-based models, evaluating their performance on two
ex vivo datasets comprising CCD camera and flexible ureteroscope images. The
ViT-base model pretrained on ImageNet-21k consistently outperformed a ResNet50
baseline across multiple imaging conditions. For instance, in the most visually
complex subset (Section patches from endoscopic images), the ViT model achieved
95.2% accuracy and 95.1% F1-score, compared to 64.5% and 59.3% with ResNet50.
In the mixed-view subset from CCD-camera images, ViT reached 87.1% accuracy
versus 78.4% with CNN. These improvements extend across precision and recall as
well. The results demonstrate that ViT-based architectures provide superior
classification performance and offer a scalable alternative to conventional
CNNs for kidney stone image analysis.

</details>


### [17] [STER-VLM: Spatio-Temporal With Enhanced Reference Vision-Language Models](https://arxiv.org/abs/2508.13470)
*Tinh-Anh Nguyen-Nhu,Triet Dao Hoang Minh,Dat To-Thanh,Phuc Le-Gia,Tuan Vo-Lan,Tien-Huy Nguyen*

Main category: cs.CV

TL;DR: STER-VLM是一种高效的计算框架，通过分解空间和时间信息、优化帧选择、参考驱动理解和提示技术，提升了视觉语言模型在交通分析中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在交通分析中需要大量计算资源且难以处理细粒度的时空信息，STER-VLM旨在解决这些问题。

Method: STER-VLM采用四种技术：1) 空间和时间信息分解；2) 最佳视图过滤的时序帧选择；3) 参考驱动理解；4) 优化的视觉/文本提示技术。

Result: 在WTS和BDD数据集上，STER-VLM显著提升了语义丰富性和交通场景理解能力，并在AI City Challenge 2025 Track 2中取得了55.655的测试分数。

Conclusion: STER-VLM是一种资源高效且准确的交通分析框架，适用于实际应用。

Abstract: Vision-language models (VLMs) have emerged as powerful tools for enabling
automated traffic analysis; however, current approaches often demand
substantial computational resources and struggle with fine-grained
spatio-temporal understanding. This paper introduces STER-VLM, a
computationally efficient framework that enhances VLM performance through (1)
caption decomposition to tackle spatial and temporal information separately,
(2) temporal frame selection with best-view filtering for sufficient temporal
information, and (3) reference-driven understanding for capturing fine-grained
motion and dynamic context and (4) curated visual/textual prompt techniques.
Experimental results on the WTS \cite{kong2024wts} and BDD \cite{BDD} datasets
demonstrate substantial gains in semantic richness and traffic scene
interpretation. Our framework is validated through a decent test score of
55.655 in the AI City Challenge 2025 Track 2, showing its effectiveness in
advancing resource-efficient and accurate traffic analysis for real-world
applications.

</details>


### [18] [MINR: Efficient Implicit Neural Representations for Multi-Image Encoding](https://arxiv.org/abs/2508.13471)
*Wenyong Zhou,Taiqiang Wu,Zhengwu Liu,Yuxin Cheng,Chen Zhang,Ngai Wong*

Main category: cs.CV

TL;DR: MINR通过共享中间层和设计投影层，显著减少了多图像编码的参数数量，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 传统INR为每张图像单独使用MLP导致计算和存储效率低下，MINR旨在解决这一问题。

Method: 共享中间层并保留输入输出层为图像特定，同时为每张图像设计投影层以捕捉独特特征。

Result: MINR节省60%参数，PSNR保持34 dB，可扩展到100张图像。

Conclusion: MINR在多图像编码中高效且鲁棒，适用于图像重建和超分辨率任务。

Abstract: Implicit Neural Representations (INRs) aim to parameterize discrete signals
through implicit continuous functions. However, formulating each image with a
separate neural network~(typically, a Multi-Layer Perceptron (MLP)) leads to
computational and storage inefficiencies when encoding multi-images. To address
this issue, we propose MINR, sharing specific layers to encode multi-image
efficiently. We first compare the layer-wise weight distributions for several
trained INRs and find that corresponding intermediate layers follow highly
similar distribution patterns. Motivated by this, we share these intermediate
layers across multiple images while preserving the input and output layers as
input-specific. In addition, we design an extra novel projection layer for each
image to capture its unique features. Experimental results on image
reconstruction and super-resolution tasks demonstrate that MINR can save up to
60\% parameters while maintaining comparable performance. Particularly, MINR
scales effectively to handle 100 images, maintaining an average peak
signal-to-noise ratio (PSNR) of 34 dB. Further analysis of various backbones
proves the robustness of the proposed MINR.

</details>


### [19] [Distribution-Aware Hadamard Quantization for Hardware-Efficient Implicit Neural Representations](https://arxiv.org/abs/2508.13478)
*Wenyong Zhou,Jiachen Ren,Taiqiang Wu,Yuxin Cheng,Zhengwu Liu,Ngai Wong*

Main category: cs.CV

TL;DR: DHQ是一种新型的分布感知Hadamard量化方案，针对INRs中的权重和激活进行量化，显著提升了硬件效率。


<details>
  <summary>Details</summary>
Motivation: INRs依赖高精度计算导致硬件开销大，现有量化方法仅关注权重量化，硬件节省有限。

Method: 利用Hadamard变换将权重和激活的多样化分布标准化为统一钟形，再应用标准量化器。

Result: DHQ在图像重建任务中表现优异，延迟降低32.7%，能耗减少40.1%，资源利用率最高降低98.3%。

Conclusion: DHQ通过同时量化权重和激活，显著提升了INRs的硬件效率，优于现有方法。

Abstract: Implicit Neural Representations (INRs) encode discrete signals using
Multi-Layer Perceptrons (MLPs) with complex activation functions. While INRs
achieve superior performance, they depend on full-precision number
representation for accurate computation, resulting in significant hardware
overhead. Previous INR quantization approaches have primarily focused on weight
quantization, offering only limited hardware savings due to the lack of
activation quantization. To fully exploit the hardware benefits of
quantization, we propose DHQ, a novel distribution-aware Hadamard quantization
scheme that targets both weights and activations in INRs. Our analysis shows
that the weights in the first and last layers have distributions distinct from
those in the intermediate layers, while the activations in the last layer
differ significantly from those in the preceding layers. Instead of customizing
quantizers individually, we utilize the Hadamard transformation to standardize
these diverse distributions into a unified bell-shaped form, supported by both
empirical evidence and theoretical analysis, before applying a standard
quantizer. To demonstrate the practical advantages of our approach, we present
an FPGA implementation of DHQ that highlights its hardware efficiency.
Experiments on diverse image reconstruction tasks show that DHQ outperforms
previous quantization methods, reducing latency by 32.7\%, energy consumption
by 40.1\%, and resource utilization by up to 98.3\% compared to full-precision
counterparts.

</details>


### [20] [AIM 2025 challenge on Inverse Tone Mapping Report: Methods and Results](https://arxiv.org/abs/2508.13479)
*Chao Wang,Francesco Banterle,Bin Ren,Radu Timofte,Xin Lu,Yufeng Peng,Chengjie Ge,Zhijing Sun,Ziang Zhou,Zihao Li,Zishun Liao,Qiyu Kang,Xueyang Fu,Zheng-Jun Zha,Zhijing Sun,Xingbo Wang,Kean Liu,Senyan Xu,Yang Qiu,Yifan Ding,Gabriel Eilertsen,Jonas Unger,Zihao Wang,Ke Wu,Jinshan Pan,Zhen Liu,Zhongyang Li,Shuaicheng Liu,S. M Nadim Uddin*

Main category: cs.CV

TL;DR: AIM 2025挑战赛综述，聚焦逆色调映射（ITM）算法，旨在提升HDR图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 推动ITM算法发展，提升从单LDR输入重建HDR图像的感知保真度和数值一致性。

Method: 67名参与者提交319份结果，分析前五名团队的方法论和性能。

Result: 最佳PU21-PSNR为29.22 dB，展示了提升HDR重建质量的创新策略。

Conclusion: 为未来逆色调映射研究确立了强基准。

Abstract: This paper presents a comprehensive review of the AIM 2025 Challenge on
Inverse Tone Mapping (ITM). The challenge aimed to push forward the development
of effective ITM algorithms for HDR image reconstruction from single LDR
inputs, focusing on perceptual fidelity and numerical consistency. A total of
\textbf{67} participants submitted \textbf{319} valid results, from which the
best five teams were selected for detailed analysis. This report consolidates
their methodologies and performance, with the lowest PU21-PSNR among the top
entries reaching 29.22 dB. The analysis highlights innovative strategies for
enhancing HDR reconstruction quality and establishes strong benchmarks to guide
future research in inverse tone mapping.

</details>


### [21] [Enhancing Robustness of Implicit Neural Representations Against Weight Perturbations](https://arxiv.org/abs/2508.13481)
*Wenyong Zhou,Yuxin Cheng,Zhengwu Liu,Taiqiang Wu,Chen Zhang,Ngai Wong*

Main category: cs.CV

TL;DR: 该论文首次研究了隐式神经表示（INRs）的鲁棒性问题，并提出了一种新的鲁棒损失函数来增强其抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: INRs在多媒体应用中表现出色，但其对权重扰动的脆弱性限制了实际应用。

Method: 通过最小化有无权重扰动时的损失差异，并设计了一种新的鲁棒损失函数来调节梯度。

Result: 实验表明，该方法在噪声条件下将PSNR值提高了7.5 dB。

Conclusion: 提出的方法显著提升了INRs的鲁棒性，为实际部署提供了保障。

Abstract: Implicit Neural Representations (INRs) encode discrete signals in a
continuous manner using neural networks, demonstrating significant value across
various multimedia applications. However, the vulnerability of INRs presents a
critical challenge for their real-world deployments, as the network weights
might be subjected to unavoidable perturbations. In this work, we investigate
the robustness of INRs for the first time and find that even minor
perturbations can lead to substantial performance degradation in the quality of
signal reconstruction. To mitigate this issue, we formulate the robustness
problem in INRs by minimizing the difference between loss with and without
weight perturbations. Furthermore, we derive a novel robust loss function to
regulate the gradient of the reconstruction loss with respect to weights,
thereby enhancing the robustness. Extensive experiments on reconstruction tasks
across multiple modalities demonstrate that our method achieves up to a 7.5~dB
improvement in peak signal-to-noise ratio (PSNR) values compared to original
INRs under noisy conditions.

</details>


### [22] [FAMNet: Integrating 2D and 3D Features for Micro-expression Recognition via Multi-task Learning and Hierarchical Attention](https://arxiv.org/abs/2508.13483)
*Liangyu Fu,Xuecheng Wu,Danlei Huang,Xinyi Yin*

Main category: cs.CV

TL;DR: 提出了一种基于多任务学习和分层注意力的微表情识别方法FAMNet，通过融合2D和3D CNN提取全方位特征，显著提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 微表情识别因持续时间短、强度低而具有挑战性，现有方法难以有效提取细粒度和时空特征。

Method: 结合2D CNN AMNet2D和3D CNN AMNet3D，采用多任务学习（MER和FAUD）和参数硬共享策略。

Result: 在多个数据集上表现优异，如SAMM（UAR 83.75%）、CASME II（UF1 84.03%）和CAS(ME)^3（UAR 51%）。

Conclusion: FAMNet通过多任务学习和分层注意力机制，显著提升了微表情识别的性能。

Abstract: Micro-expressions recognition (MER) has essential application value in many
fields, but the short duration and low intensity of micro-expressions (MEs)
bring considerable challenges to MER. The current MER methods in deep learning
mainly include three data loading methods: static images, dynamic image
sequence, and a combination of the two streams. How to effectively extract MEs'
fine-grained and spatiotemporal features has been difficult to solve. This
paper proposes a new MER method based on multi-task learning and hierarchical
attention, which fully extracts MEs' omni-directional features by merging 2D
and 3D CNNs. The fusion model consists of a 2D CNN AMNet2D and a 3D CNN
AMNet3D, with similar structures consisting of a shared backbone network
Resnet18 and attention modules. During training, the model adopts different
data loading methods to adapt to two specific networks respectively, jointly
trains on the tasks of MER and facial action unit detection (FAUD), and adopts
the parameter hard sharing for information association, which further improves
the effect of the MER task, and the final fused model is called FAMNet.
Extensive experimental results show that our proposed FAMNet significantly
improves task performance. On the SAMM, CASME II and MMEW datasets, FAMNet
achieves 83.75% (UAR) and 84.03% (UF1). Furthermore, on the challenging
CAS(ME)$^3$ dataset, FAMNet achieves 51% (UAR) and 43.42% (UF1).

</details>


### [23] [CORENet: Cross-Modal 4D Radar Denoising Network with LiDAR Supervision for Autonomous Driving](https://arxiv.org/abs/2508.13485)
*Fuyang Liu,Jilin Mei,Fangyuan Mao,Chen Min,Yan Xing,Yu Hu*

Main category: cs.CV

TL;DR: CORENet是一种利用LiDAR监督的跨模态去噪框架，用于从稀疏且嘈杂的4D雷达点云中提取特征，提升检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 4D雷达点云稀疏且噪声多，限制了其在感知任务中的有效性。

Method: 通过LiDAR监督识别噪声模式，提取雷达数据的判别性特征，设计为即插即用架构。

Result: 在噪声水平高的Dual-Radar数据集上表现优异，优于主流方法。

Conclusion: CORENet在训练时利用LiDAR监督，推理时仅需雷达数据，显著提升了检测性能。

Abstract: 4D radar-based object detection has garnered great attention for its
robustness in adverse weather conditions and capacity to deliver rich spatial
information across diverse driving scenarios. Nevertheless, the sparse and
noisy nature of 4D radar point clouds poses substantial challenges for
effective perception. To address the limitation, we present CORENet, a novel
cross-modal denoising framework that leverages LiDAR supervision to identify
noise patterns and extract discriminative features from raw 4D radar data.
Designed as a plug-and-play architecture, our solution enables seamless
integration into voxel-based detection frameworks without modifying existing
pipelines. Notably, the proposed method only utilizes LiDAR data for
cross-modal supervision during training while maintaining full radar-only
operation during inference. Extensive evaluation on the challenging Dual-Radar
dataset, which is characterized by elevated noise level, demonstrates the
effectiveness of our framework in enhancing detection robustness. Comprehensive
experiments validate that CORENet achieves superior performance compared to
existing mainstream approaches.

</details>


### [24] [Multi-view Clustering via Bi-level Decoupling and Consistency Learning](https://arxiv.org/abs/2508.13499)
*Shihao Dong,Yuhui Zheng,Huiying Xu,Xinzhong Zhu*

Main category: cs.CV

TL;DR: 提出了一种新的Bi-level Decoupling and Consistency Learning框架（BDCL），用于多视图聚类，通过解耦特征和聚类空间以及一致性学习，提高了聚类性能。


<details>
  <summary>Details</summary>
Motivation: 多视图聚类中，学习一致性和互补性可以提升性能，但面向聚类的表示学习常被忽视。

Method: 框架包含三个模块：多视图实例学习、双级解耦和一致性学习，分别处理信息对齐、特征空间解耦和聚类一致性。

Result: 在五个基准数据集上的实验表明，BDCL优于现有方法。

Conclusion: BDCL通过解耦和一致性学习，有效提升了多视图聚类的性能。

Abstract: Multi-view clustering has shown to be an effective method for analyzing
underlying patterns in multi-view data. The performance of clustering can be
improved by learning the consistency and complementarity between multi-view
features, however, cluster-oriented representation learning is often
overlooked. In this paper, we propose a novel Bi-level Decoupling and
Consistency Learning framework (BDCL) to further explore the effective
representation for multi-view data to enhance inter-cluster discriminability
and intra-cluster compactness of features in multi-view clustering. Our
framework comprises three modules: 1) The multi-view instance learning module
aligns the consistent information while preserving the private features between
views through reconstruction autoencoder and contrastive learning. 2) The
bi-level decoupling of features and clusters enhances the discriminability of
feature space and cluster space. 3) The consistency learning module treats the
different views of the sample and their neighbors as positive pairs, learns the
consistency of their clustering assignments, and further compresses the
intra-cluster space. Experimental results on five benchmark datasets
demonstrate the superiority of the proposed method compared with the SOTA
methods. Our code is published on https://github.com/LouisDong95/BDCL.

</details>


### [25] [AdaptiveAE: An Adaptive Exposure Strategy for HDR Capturing in Dynamic Scenes](https://arxiv.org/abs/2508.13503)
*Tianyi Xu,Fan Zhang,Boxin Shi,Tianfan Xue,Yujin Wang*

Main category: cs.CV

TL;DR: AdaptiveAE是一种基于强化学习的方法，用于优化快门速度和ISO组合的选择，以在动态环境中最大化HDR重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了快门速度和ISO之间的复杂交互，且未考虑动态场景中的运动模糊效应。

Method: 提出AdaptiveAE，结合运动模糊和噪声模拟的图像合成流程，利用语义信息和曝光直方图进行训练。

Result: 在多个数据集上的实验结果表明，该方法达到了最先进的性能。

Conclusion: AdaptiveAE能够根据用户定义的曝光时间预算自适应选择最优的ISO和快门速度序列，优于传统解决方案。

Abstract: Mainstream high dynamic range imaging techniques typically rely on fusing
multiple images captured with different exposure setups (shutter speed and
ISO). A good balance between shutter speed and ISO is crucial for achieving
high-quality HDR, as high ISO values introduce significant noise, while long
shutter speeds can lead to noticeable motion blur. However, existing methods
often overlook the complex interaction between shutter speed and ISO and fail
to account for motion blur effects in dynamic scenes.
  In this work, we propose AdaptiveAE, a reinforcement learning-based method
that optimizes the selection of shutter speed and ISO combinations to maximize
HDR reconstruction quality in dynamic environments. AdaptiveAE integrates an
image synthesis pipeline that incorporates motion blur and noise simulation
into our training procedure, leveraging semantic information and exposure
histograms. It can adaptively select optimal ISO and shutter speed sequences
based on a user-defined exposure time budget, and find a better exposure
schedule than traditional solutions. Experimental results across multiple
datasets demonstrate that it achieves the state-of-the-art performance.

</details>


### [26] [Bridging the Gap: Doubles Badminton Analysis with Singles-Trained Models](https://arxiv.org/abs/2508.13507)
*Seungheon Baek,Jinhyuk Yun*

Main category: cs.CV

TL;DR: 提出了一种将单打训练的模型迁移到双打分析的方法，解决了双打数据不足和多目标跟踪的挑战。


<details>
  <summary>Details</summary>
Motivation: 双打比赛在国际比赛中更为普遍，但现有研究主要集中在单打，因数据获取和多目标跟踪困难。

Method: 使用ViT-Pose提取关键点，基于ST-GCN的对比学习框架嵌入，结合自定义多目标跟踪算法提升稳定性，用Transformer分类器识别击球动作。

Result: 证明了基于姿态的击球识别在双打羽毛球中的可行性。

Conclusion: 为双打专用数据集奠定了基础，扩展了对这一主流但研究不足的羽毛球形式的分析能力。

Abstract: Badminton is known as one of the fastest racket sports in the world. Despite
doubles matches being more prevalent in international tournaments than singles,
previous research has mainly focused on singles due to the challenges in data
availability and multi-person tracking. To address this gap, we designed an
approach that transfers singles-trained models to doubles analysis. We
extracted keypoints from the ShuttleSet single matches dataset using ViT-Pose
and embedded them through a contrastive learning framework based on ST-GCN. To
improve tracking stability, we incorporated a custom multi-object tracking
algorithm that resolves ID switching issues from fast and overlapping player
movements. A Transformer-based classifier then determines shot occurrences
based on the learned embeddings. Our findings demonstrate the feasibility of
extending pose-based shot recognition to doubles badminton, broadening
analytics capabilities. This work establishes a foundation for doubles-specific
datasets to enhance understanding of this predominant yet understudied format
of the fast racket sport.

</details>


### [27] [2D Gaussians Meet Visual Tokenizer](https://arxiv.org/abs/2508.13515)
*Yiang Shi,Xiaoyang Guo,Wei Yin,Mingkai Jia,Qian Zhang,Xiaolin Hu,Wenyu Liu,Xinggang Wan*

Main category: cs.CV

TL;DR: VGQ是一种新型图像标记器，通过整合2D高斯分布增强结构建模，显著提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于量化的标记器（如VQ-GAN）主要关注外观特征（如纹理和颜色），而忽略了几何结构。VGQ旨在解决这一问题。

Method: VGQ将图像潜在表示编码为2D高斯分布，直接建模位置、旋转和尺度等结构相关参数。

Result: 在ImageNet 256x256基准测试中，VGQ实现了rFID 0.556和PSNR 24.93的优异重建性能。

Conclusion: VGQ通过增强结构建模，显著优于现有方法，为图像生成提供了更丰富的视觉表示。

Abstract: The image tokenizer is a critical component in AR image generation, as it
determines how rich and structured visual content is encoded into compact
representations. Existing quantization-based tokenizers such as VQ-GAN
primarily focus on appearance features like texture and color, often neglecting
geometric structures due to their patch-based design. In this work, we explored
how to incorporate more visual information into the tokenizer and proposed a
new framework named Visual Gaussian Quantization (VGQ), a novel tokenizer
paradigm that explicitly enhances structural modeling by integrating 2D
Gaussians into traditional visual codebook quantization frameworks. Our
approach addresses the inherent limitations of naive quantization methods such
as VQ-GAN, which struggle to model structured visual information due to their
patch-based design and emphasis on texture and color. In contrast, VGQ encodes
image latents as 2D Gaussian distributions, effectively capturing geometric and
spatial structures by directly modeling structure-related parameters such as
position, rotation and scale. We further demonstrate that increasing the
density of 2D Gaussians within the tokens leads to significant gains in
reconstruction fidelity, providing a flexible trade-off between token
efficiency and visual richness. On the ImageNet 256x256 benchmark, VGQ achieves
strong reconstruction quality with an rFID score of 1.00. Furthermore, by
increasing the density of 2D Gaussians within the tokens, VGQ gains a
significant boost in reconstruction capability and achieves a state-of-the-art
reconstruction rFID score of 0.556 and a PSNR of 24.93, substantially
outperforming existing methods. Codes will be released soon.

</details>


### [28] [Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency](https://arxiv.org/abs/2508.13518)
*Yanbiao Ma,Wei Dai,Bowei Liu,Jiayi Chen,Wenke Huang,Guancheng Wan,Zhiwu Lu,Junchi Yan*

Main category: cs.CV

TL;DR: 论文提出了一种基于几何知识引导的分布校准框架，利用基础模型提取的特征分布几何形状的跨域可迁移性，解决了联邦学习和长尾识别中的信息不足问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习中观察到的训练样本与真实分布之间存在差距（如采样偏差、噪声等），论文旨在利用基础模型的几何形状可迁移性来校准分布。

Method: 通过基础模型（如CLIP、DINOv2）提取特征，利用其几何形状的跨域可迁移性，设计分布校准框架，应用于联邦学习和长尾识别。

Result: 在联邦学习中生成全局几何形状以弥补局部与全局观察的差距；在长尾学习中利用样本丰富类别的几何知识恢复尾部类别的真实分布。

Conclusion: 几何知识引导的分布校准有效克服了数据异构性和样本不平衡导致的信息不足，提升了多个基准任务的性能。

Abstract: Despite the fast progress of deep learning, one standing challenge is the gap
of the observed training samples and the underlying true distribution. There
are multiple reasons for the causing of this gap e.g. sampling bias, noise etc.
In the era of foundation models, we show that when leveraging the off-the-shelf
(vision) foundation models (e.g., CLIP, DINOv2) for feature extraction, the
geometric shapes of the resulting feature distributions exhibit remarkable
transferability across domains and datasets. To verify its practical
usefulness, we embody our geometric knowledge-guided distribution calibration
framework in two popular and challenging settings: federated learning and
long-tailed recognition. In the federated setting, we devise a technique of
acquiring the global geometric shape under privacy constraints, then leverage
this knowledge to generate new samples for clients, in the aim of bridging the
gap between local and global observations. In long-tailed learning, it utilizes
the geometric knowledge transferred from sample-rich categories to recover the
true distribution for sample-scarce tail classes. Comprehensive experiments
show that our proposed geometric knowledge-guided distribution calibration
effectively overcomes information deficits caused by data heterogeneity and
sample imbalance, with boosted performance across benchmarks.

</details>


### [29] [Evaluating Open-Source Vision Language Models for Facial Emotion Recognition against Traditional Deep Learning Models](https://arxiv.org/abs/2508.13524)
*Vamsi Krishna Mulukutla,Sai Supriya Pavarala,Srinivasa Raju Rudraraju,Sridevi Bonthu*

Main category: cs.CV

TL;DR: 本文首次比较了开源视觉语言模型（VLMs）与传统深度学习模型在FER-2013数据集上的表现，发现传统模型显著优于VLMs，并提出了针对低质量视觉任务的改进方案。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索视觉语言模型在低质量面部表情识别任务中的表现，并解决其与噪声数据的匹配问题。

Method: 方法包括引入基于GFPGAN的图像恢复流程，并与FER评估结合，同时对比了多种模型的性能与计算成本。

Result: 结果显示传统模型（如EfficientNet-B0和ResNet-50）在准确率上显著优于VLMs（如CLIP和Phi-3.5 Vision）。

Conclusion: 结论指出VLMs在低质量视觉任务中存在局限性，需进一步适应噪声环境，并为未来研究提供了可复现的基准。

Abstract: Facial Emotion Recognition (FER) is crucial for applications such as
human-computer interaction and mental health diagnostics. This study presents
the first empirical comparison of open-source Vision-Language Models (VLMs),
including Phi-3.5 Vision and CLIP, against traditional deep learning models
VGG19, ResNet-50, and EfficientNet-B0 on the challenging FER-2013 dataset,
which contains 35,887 low-resolution grayscale images across seven emotion
classes. To address the mismatch between VLM training assumptions and the noisy
nature of FER data, we introduce a novel pipeline that integrates GFPGAN-based
image restoration with FER evaluation. Results show that traditional models,
particularly EfficientNet-B0 (86.44%) and ResNet-50 (85.72%), significantly
outperform VLMs like CLIP (64.07%) and Phi-3.5 Vision (51.66%), highlighting
the limitations of VLMs in low-quality visual tasks. In addition to performance
evaluation using precision, recall, F1-score, and accuracy, we provide a
detailed computational cost analysis covering preprocessing, training,
inference, and evaluation phases, offering practical insights for deployment.
This work underscores the need for adapting VLMs to noisy environments and
provides a reproducible benchmark for future research in emotion recognition.

</details>


### [30] [EAvatar: Expression-Aware Head Avatar Reconstruction with Generative Geometry Priors](https://arxiv.org/abs/2508.13537)
*Shikun Zhang,Cunjian Chen,Yiqun Wang,Qiuhong Ke,Yong Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为EAvatar的新型3DGS框架，用于高保真头部重建，解决了现有方法在捕捉细微表情和保持局部纹理连续性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 高保真头部重建在AR/VR、游戏和多媒体内容创作中至关重要，但现有3DGS方法在高度可变形区域的表现仍有不足。

Method: EAvatar引入稀疏表情控制机制，利用少量关键高斯点影响邻近高斯点的变形，并结合预训练生成模型的高质量3D先验。

Result: 实验表明，该方法能生成更准确、视觉连贯的头部重建，提升了表情控制性和细节保真度。

Conclusion: EAvatar通过结合稀疏控制和3D先验，显著改善了头部重建的质量和表现力。

Abstract: High-fidelity head avatar reconstruction plays a crucial role in AR/VR,
gaming, and multimedia content creation. Recent advances in 3D Gaussian
Splatting (3DGS) have demonstrated effectiveness in modeling complex geometry
with real-time rendering capability and are now widely used in high-fidelity
head avatar reconstruction tasks. However, existing 3DGS-based methods still
face significant challenges in capturing fine-grained facial expressions and
preserving local texture continuity, especially in highly deformable regions.
To mitigate these limitations, we propose a novel 3DGS-based framework termed
EAvatar for head reconstruction that is both expression-aware and
deformation-aware. Our method introduces a sparse expression control mechanism,
where a small number of key Gaussians are used to influence the deformation of
their neighboring Gaussians, enabling accurate modeling of local deformations
and fine-scale texture transitions. Furthermore, we leverage high-quality 3D
priors from pretrained generative models to provide a more reliable facial
geometry, offering structural guidance that improves convergence stability and
shape accuracy during training. Experimental results demonstrate that our
method produces more accurate and visually coherent head reconstructions with
improved expression controllability and detail fidelity.

</details>


### [31] [FLAIR: Frequency- and Locality-Aware Implicit Neural Representations](https://arxiv.org/abs/2508.13544)
*Sukhun Ko,Dahyeon Kye,Kyle Min,Chanho Eom,Jihyong Oh*

Main category: cs.CV

TL;DR: FLAIR提出了一种频率和局部感知的隐式神经表示方法，通过RC-GAUSS激活和WEGE编码解决了现有INRs的频率选择性和空间局部化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有INRs缺乏频率选择性、空间局部化和稀疏表示，导致对冗余信号成分的过度依赖和频谱偏差。

Method: FLAIR引入RC-GAUSS激活函数（基于时间-频率不确定性原理）和WEGE编码（利用离散小波变换计算能量分数）来显式引导频率信息。

Result: FLAIR在2D图像表示与恢复以及3D重建任务中均优于现有INRs。

Conclusion: FLAIR通过频率和局部感知的创新设计，显著提升了隐式神经表示的性能。

Abstract: Implicit Neural Representations (INRs) leverage neural networks to map
coordinates to corresponding signals, enabling continuous and compact
representations. This paradigm has driven significant advances in various
vision tasks. However, existing INRs lack frequency selectivity, spatial
localization, and sparse representations, leading to an over-reliance on
redundant signal components. Consequently, they exhibit spectral bias, tending
to learn low-frequency components early while struggling to capture fine
high-frequency details. To address these issues, we propose FLAIR (Frequency-
and Locality-Aware Implicit Neural Representations), which incorporates two key
innovations. The first is RC-GAUSS, a novel activation designed for explicit
frequency selection and spatial localization under the constraints of the
time-frequency uncertainty principle (TFUP). The second is
Wavelet-Energy-Guided Encoding (WEGE), which leverages the discrete wavelet
transform (DWT) to compute energy scores and explicitly guide frequency
information to the network. Our method consistently outperforms existing INRs
in 2D image representation and restoration, as well as 3D reconstruction.

</details>


### [32] [GazeProphet: Software-Only Gaze Prediction for VR Foveated Rendering](https://arxiv.org/abs/2508.13546)
*Farhaan Ebadulla,Chiraag Mudlapur,Gaurav BV*

Main category: cs.CV

TL;DR: GazeProphet是一种纯软件方法，用于预测VR环境中的注视位置，无需专用硬件，显著降低了计算需求。


<details>
  <summary>Details</summary>
Motivation: 当前基于硬件的眼动追踪系统成本高且复杂，限制了其广泛应用。

Method: 结合球形视觉Transformer处理360度VR场景和LSTM时序编码器，通过多模态融合网络预测未来注视位置。

Result: 在VR数据集上，GazeProphet的中值角度误差为3.83度，优于传统方法24%，且性能稳定。

Conclusion: 纯软件注视预测可用于VR聚焦渲染，提升性能并降低硬件依赖。

Abstract: Foveated rendering significantly reduces computational demands in virtual
reality applications by concentrating rendering quality where users focus their
gaze. Current approaches require expensive hardware-based eye tracking systems,
limiting widespread adoption due to cost, calibration complexity, and hardware
compatibility constraints. This paper presents GazeProphet, a software-only
approach for predicting gaze locations in VR environments without requiring
dedicated eye tracking hardware. The approach combines a Spherical Vision
Transformer for processing 360-degree VR scenes with an LSTM-based temporal
encoder that captures gaze sequence patterns. A multi-modal fusion network
integrates spatial scene features with temporal gaze dynamics to predict future
gaze locations with associated confidence estimates. Experimental evaluation on
a comprehensive VR dataset demonstrates that GazeProphet achieves a median
angular error of 3.83 degrees, outperforming traditional saliency-based
baselines by 24% while providing reliable confidence calibration. The approach
maintains consistent performance across different spatial regions and scene
types, enabling practical deployment in VR systems without additional hardware
requirements. Statistical analysis confirms the significance of improvements
across all evaluation metrics. These results show that software-only gaze
prediction can work for VR foveated rendering, making this performance boost
more accessible to different VR platforms and apps.

</details>


### [33] [A Lightweight Dual-Mode Optimization for Generative Face Video Coding](https://arxiv.org/abs/2508.13547)
*Zihan Zhang,Shanzhi Yin,Bolin Chen,Ru-Ling Liao,Shiqi Wang,Yan Ye*

Main category: cs.CV

TL;DR: 提出了一种轻量级GFVC框架，通过双模式优化（架构重新设计和操作细化）降低复杂度，同时保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决GFVC因大模型参数和高计算成本而难以实际部署的问题。

Method: 采用双模式优化：架构上替换传统卷积层为更高效的层；操作上开发两阶段自适应通道剪枝策略。

Result: 实现了90.4%的参数减少和88.9%的计算节省，性能优于VVC标准。

Conclusion: 该方法有望在资源受限的环境中高效部署GFVC。

Abstract: Generative Face Video Coding (GFVC) achieves superior rate-distortion
performance by leveraging the strong inference capabilities of deep generative
models. However, its practical deployment is hindered by large model parameters
and high computational costs. To address this, we propose a lightweight GFVC
framework that introduces dual-mode optimization -- combining architectural
redesign and operational refinement -- to reduce complexity whilst preserving
reconstruction quality. Architecturally, we replace traditional 3 x 3
convolutions with slimmer and more efficient layers, reducing complexity
without compromising feature expressiveness. Operationally, we develop a
two-stage adaptive channel pruning strategy: (1) soft pruning during training
identifies redundant channels via learnable thresholds, and (2) hard pruning
permanently eliminates these channels post-training using a derived mask. This
dual-phase approach ensures both training stability and inference efficiency.
Experimental results demonstrate that the proposed lightweight dual-mode
optimization for GFVC can achieve 90.4% parameter reduction and 88.9%
computation saving compared to the baseline, whilst achieving superior
performance compared to state-of-the-art video coding standard Versatile Video
Coding (VVC) in terms of perceptual-level quality metrics. As such, the
proposed method is expected to enable efficient GFVC deployment in
resource-constrained environments such as mobile edge devices.

</details>


### [34] [Color Spike Data Generation via Bio-inspired Neuron-like Encoding with an Artificial Photoreceptor Layer](https://arxiv.org/abs/2508.13558)
*Hsieh Ching-Teng,Wang Yuan-Kai*

Main category: cs.CV

TL;DR: 提出了一种基于生物神经元原理的编码方法，增强SNN性能。


<details>
  <summary>Details</summary>
Motivation: 解决SNN因脉冲数据信息容量有限而性能落后于CNN的问题，同时保持神经形态计算的初衷。

Method: 采用类神经元编码方法，结合人工光感受器层生成包含颜色和亮度信息的脉冲数据。

Result: 实验表明该方法有效增加脉冲信号信息量并提升SNN性能。

Conclusion: 该方法有潜力推动神经形态计算发展，扩大SNN应用范围。

Abstract: In recent years, neuromorphic computing and spiking neural networks (SNNs)
have ad-vanced rapidly through integration with deep learning. However, the
performance of SNNs still lags behind that of convolutional neural networks
(CNNs), primarily due to the limited information capacity of spike-based data.
Although some studies have attempted to improve SNN performance by training
them with non-spiking inputs such as static images, this approach deviates from
the original intent of neuromorphic computing, which emphasizes spike-based
information processing. To address this issue, we propose a Neuron-like
Encoding method that generates spike data based on the intrinsic operational
principles and functions of biological neurons. This method is further enhanced
by the incorporation of an artificial pho-toreceptor layer, enabling spike data
to carry both color and luminance information, thereby forming a complete
visual spike signal. Experimental results using the Integrate-and-Fire neuron
model demonstrate that this biologically inspired approach effectively
increases the information content of spike signals and improves SNN
performance, all while adhering to neuromorphic principles. We believe this
concept holds strong potential for future development and may contribute to
overcoming current limitations in neuro-morphic computing, facilitating broader
applications of SNNs.

</details>


### [35] [DictAS: A Framework for Class-Generalizable Few-Shot Anomaly Segmentation via Dictionary Lookup](https://arxiv.org/abs/2508.13560)
*Zhen Qu,Xian Tao,Xinyi Gong,ShiChen Qu,Xiaopei Zhang,Xingang Wang,Fei Shen,Zhengtao Zhang,Mukesh Prasad,Guiguang Ding*

Main category: cs.CV

TL;DR: DictAS是一种新颖的框架，通过自监督学习和字典查找能力，无需目标数据重新训练即可检测未见类别的视觉异常。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在少样本异常分割（FSAS）中依赖已知异常样本的先验知识，限制了跨类别泛化能力。

Method: DictAS包含字典构建、字典查找和查询判别正则化三个组件，通过自监督学习实现异常检测。

Result: 在七个公共工业和医学数据集上，DictAS表现优于现有FSAS方法。

Conclusion: DictAS通过字典查找和自监督学习，显著提升了未见类别的异常检测能力。

Abstract: Recent vision-language models (e.g., CLIP) have demonstrated remarkable
class-generalizable ability to unseen classes in few-shot anomaly segmentation
(FSAS), leveraging supervised prompt learning or fine-tuning on seen classes.
However, their cross-category generalization largely depends on prior knowledge
of real seen anomaly samples. In this paper, we propose a novel framework,
namely DictAS, which enables a unified model to detect visual anomalies in
unseen object categories without any retraining on the target data, only
employing a few normal reference images as visual prompts. The insight behind
DictAS is to transfer dictionary lookup capabilities to the FSAS task for
unseen classes via self-supervised learning, instead of merely memorizing the
normal and abnormal feature patterns from the training set. Specifically,
DictAS mainly consists of three components: (1) **Dictionary Construction** -
to simulate the index and content of a real dictionary using features from
normal reference images. (2) **Dictionary Lookup** - to retrieve queried region
features from the dictionary via a sparse lookup strategy. When a query feature
cannot be retrieved, it is classified as an anomaly. (3) **Query Discrimination
Regularization**- to enhance anomaly discrimination by making abnormal features
harder to retrieve from the dictionary. To achieve this, Contrastive Query
Constraint and Text Alignment Constraint are further proposed. Extensive
experiments on seven public industrial and medical datasets demonstrate that
DictAS consistently outperforms state-of-the-art FSAS methods.

</details>


### [36] [Learnable SMPLify: A Neural Solution for Optimization-Free Human Pose Inverse Kinematics](https://arxiv.org/abs/2508.13562)
*Yuchen Yang,Linfeng Dong,Wei Wang,Zhihang Zhong,Xiao Sun*

Main category: cs.CV

TL;DR: Learnable SMPLify是一种神经框架，通过单次回归模型替代SMPLify中的迭代优化，显著提升运行速度，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: SMPLify的迭代优化计算成本高，限制了其实用性。数据驱动的神经网络在多个领域已证明可以显著提升运行效率而不牺牲精度。

Method: 提出Learnable SMPLify，采用时间采样策略构建初始化-目标对，并提出人中心归一化和残差学习以提升泛化能力。

Result: 方法运行速度比SMPLify快近200倍，泛化能力强，适用于未见过的3DPW和RICH数据集，并可作为插件工具。

Conclusion: Learnable SMPLify是一种实用且简单的基线方法，适用于3D人体姿态和形状估计任务。

Abstract: In 3D human pose and shape estimation, SMPLify remains a robust baseline that
solves inverse kinematics (IK) through iterative optimization. However, its
high computational cost limits its practicality. Recent advances across domains
have shown that replacing iterative optimization with data-driven neural
networks can achieve significant runtime improvements without sacrificing
accuracy. Motivated by this trend, we propose Learnable SMPLify, a neural
framework that replaces the iterative fitting process in SMPLify with a
single-pass regression model. The design of our framework targets two core
challenges in neural IK: data construction and generalization. To enable
effective training, we propose a temporal sampling strategy that constructs
initialization-target pairs from sequential frames. To improve generalization
across diverse motions and unseen poses, we propose a human-centric
normalization scheme and residual learning to narrow the solution space.
Learnable SMPLify supports both sequential inference and plug-in
post-processing to refine existing image-based estimators. Extensive
experiments demonstrate that our method establishes itself as a practical and
simple baseline: it achieves nearly 200x faster runtime compared to SMPLify,
generalizes well to unseen 3DPW and RICH, and operates in a model-agnostic
manner when used as a plug-in tool on LucidAction. The code is available at
https://github.com/Charrrrrlie/Learnable-SMPLify.

</details>


### [37] [The 9th AI City Challenge](https://arxiv.org/abs/2508.13564)
*Zheng Tang,Shuo Wang,David C. Anastasiu,Ming-Ching Chang,Anuj Sharma,Quan Kong,Norimasa Kobori,Munkhjargal Gochoo,Ganzorig Batnasan,Munkh-Erdene Otgonbold,Fady Alnajjar,Jun-Wei Hsieh,Tomasz Kornuta,Xiaolong Li,Yilin Zhao,Han Zhang,Subhashree Radhakrishnan,Arihant Jain,Ratnesh Kumar,Vidya N. Murali,Yuxing Wang,Sameer Satish Pusegaonkar,Yizhou Wang,Sujit Biswas,Xunlei Wu,Zhedong Zheng,Pranamesh Chakraborty,Rama Chellappa*

Main category: cs.CV

TL;DR: 第九届AI City Challenge聚焦于计算机视觉和AI在交通、工业自动化和公共安全中的实际应用，包含四个赛道，吸引了245支团队参与。


<details>
  <summary>Details</summary>
Motivation: 推动计算机视觉和AI在现实世界中的应用，特别是在交通、工业自动化和公共安全领域。

Method: 四个赛道分别关注多类3D多摄像头跟踪、交通安全的视频问答、动态仓库环境中的细粒度空间推理以及鱼眼摄像头的高效道路物体检测。数据集部分在NVIDIA Omniverse中生成。

Result: 多个团队在多个任务中取得了顶级成绩，设立了新的基准。

Conclusion: 比赛通过公开数据集和严格的评估框架促进了可重复性，并推动了相关技术的发展。

Abstract: The ninth AI City Challenge continues to advance real-world applications of
computer vision and AI in transportation, industrial automation, and public
safety. The 2025 edition featured four tracks and saw a 17% increase in
participation, with 245 teams from 15 countries registered on the evaluation
server. Public release of challenge datasets led to over 30,000 downloads to
date. Track 1 focused on multi-class 3D multi-camera tracking, involving
people, humanoids, autonomous mobile robots, and forklifts, using detailed
calibration and 3D bounding box annotations. Track 2 tackled video question
answering in traffic safety, with multi-camera incident understanding enriched
by 3D gaze labels. Track 3 addressed fine-grained spatial reasoning in dynamic
warehouse environments, requiring AI systems to interpret RGB-D inputs and
answer spatial questions that combine perception, geometry, and language. Both
Track 1 and Track 3 datasets were generated in NVIDIA Omniverse. Track 4
emphasized efficient road object detection from fisheye cameras, supporting
lightweight, real-time deployment on edge devices. The evaluation framework
enforced submission limits and used a partially held-out test set to ensure
fair benchmarking. Final rankings were revealed after the competition
concluded, fostering reproducibility and mitigating overfitting. Several teams
achieved top-tier results, setting new benchmarks in multiple tasks.

</details>


### [38] [Generative Model-Based Feature Attention Module for Video Action Analysis](https://arxiv.org/abs/2508.13565)
*Guiqin Wang,Peng Zhao,Cong Zhao,Jing Huang,Siyan Guo,Shusen Yang*

Main category: cs.CV

TL;DR: 提出了一种基于生成注意力的模型，通过学习特征语义关系，提升视频动作分析的精度和可扩展性，适用于高性能物联网应用。


<details>
  <summary>Details</summary>
Motivation: 现有方法在特征提取中忽视特征语义，且专注于优化动作提案，导致精度不足，难以满足高性能物联网应用（如自动驾驶）的需求。

Method: 利用动作前景与背景的差异，同时学习帧和片段依赖性的时间动作特征语义，有效利用特征语义。

Result: 在动作识别和动作检测任务中，通过广泛实验验证了模型的有效性，并在公认数据集上证明了其优越性。

Conclusion: 提出的生成注意力模型显著提升了视频动作分析的性能，适用于更广泛的任务，如视频动作识别。

Abstract: Video action analysis is a foundational technology within the realm of
intelligent video comprehension, particularly concerning its application in
Internet of Things(IoT). However, existing methodologies overlook feature
semantics in feature extraction and focus on optimizing action proposals, thus
these solutions are unsuitable for widespread adoption in high-performance IoT
applications due to the limitations in precision, such as autonomous driving,
which necessitate robust and scalable intelligent video analytics analysis. To
address this issue, we propose a novel generative attention-based model to
learn the relation of feature semantics. Specifically, by leveraging the
differences of actions' foreground and background, our model simultaneously
learns the frame- and segment-dependencies of temporal action feature
semantics, which takes advantage of feature semantics in the feature extraction
effectively. To evaluate the effectiveness of our model, we conduct extensive
experiments on two benchmark video task, action recognition and action
detection. In the context of action detection tasks, we substantiate the
superiority of our approach through comprehensive validation on widely
recognized datasets. Moreover, we extend the validation of the effectiveness of
our proposed method to a broader task, video action recognition. Our code is
available at https://github.com/Generative-Feature-Model/GAF.

</details>


### [39] [Temporal-Conditional Referring Video Object Segmentation with Noise-Free Text-to-Video Diffusion Model](https://arxiv.org/abs/2508.13584)
*Ruixin Zhang,Jiaqing Fan,Yifan Liao,Qian Qiao,Fanzhang Li*

Main category: cs.CV

TL;DR: 提出了一种新的RVOS模型，通过改进分割头设计和简化模型结构，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有RVOS方法过于关注特征提取和时间建模，而忽略了分割头的设计，存在改进空间。

Method: 提出Temporal-Conditional RVOS模型，整合现有分割方法，移除噪声预测模块，并设计TCMR模块。

Result: 在四个公开RVOS基准测试中均达到最优性能。

Conclusion: 通过优化分割头和简化模型，显著提升了RVOS任务的性能。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment specific objects
in a video according to textual descriptions. We observe that recent RVOS
approaches often place excessive emphasis on feature extraction and temporal
modeling, while relatively neglecting the design of the segmentation head. In
fact, there remains considerable room for improvement in segmentation head
design. To address this, we propose a Temporal-Conditional Referring Video
Object Segmentation model, which innovatively integrates existing segmentation
methods to effectively enhance boundary segmentation capability. Furthermore,
our model leverages a text-to-video diffusion model for feature extraction. On
top of this, we remove the traditional noise prediction module to avoid the
randomness of noise from degrading segmentation accuracy, thereby simplifying
the model while improving performance. Finally, to overcome the limited feature
extraction capability of the VAE, we design a Temporal Context Mask Refinement
(TCMR) module, which significantly improves segmentation quality without
introducing complex designs. We evaluate our method on four public RVOS
benchmarks, where it consistently achieves state-of-the-art performance.

</details>


### [40] [Bridging Clear and Adverse Driving Conditions](https://arxiv.org/abs/2508.13592)
*Yoel Shapiro,Yahia Showgan,Koustav Mullick*

Main category: cs.CV

TL;DR: 提出了一种新的领域适应（DA）管道，将晴天图像转换为雾、雨、雪和夜间图像，以解决自动驾驶系统在恶劣环境下性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在恶劣环境（如低光照和降水）下性能显著下降，但现有数据集缺乏此类数据，获取和标注成本高昂。

Method: 开发了多种数据生成管道（模拟、GAN、混合扩散-GAN），结合模拟和真实图像训练，并引入方法减少Stable-Diffusion的幻觉和伪影。

Result: 在ACDC数据集上，语义分割整体提升1.85%，夜间场景提升4.62%。

Conclusion: 混合方法在恶劣条件下显著提升了自动驾驶感知的鲁棒性。

Abstract: Autonomous Driving (AD) systems exhibit markedly degraded performance under
adverse environmental conditions, such as low illumination and precipitation.
The underrepresentation of adverse conditions in AD datasets makes it
challenging to address this deficiency. To circumvent the prohibitive cost of
acquiring and annotating adverse weather data, we propose a novel Domain
Adaptation (DA) pipeline that transforms clear-weather images into fog, rain,
snow, and nighttime images. Here, we systematically develop and evaluate
several novel data-generation pipelines, including simulation-only, GAN-based,
and hybrid diffusion-GAN approaches, to synthesize photorealistic adverse
images from labelled clear images. We leverage an existing DA GAN, extend it to
support auxiliary inputs, and develop a novel training recipe that leverages
both simulated and real images. The simulated images facilitate exact
supervision by providing perfectly matched image pairs, while the real images
help bridge the simulation-to-real (sim2real) gap. We further introduce a
method to mitigate hallucinations and artifacts in Stable-Diffusion
Image-to-Image (img2img) outputs by blending them adaptively with their
progenitor images. We finetune downstream models on our synthetic data and
evaluate them on the Adverse Conditions Dataset with Correspondences (ACDC). We
achieve 1.85 percent overall improvement in semantic segmentation, and 4.62
percent on nighttime, demonstrating the efficacy of our hybrid method for
robust AD perception under challenging conditions.

</details>


### [41] [Towards Efficient Vision State Space Models via Token Merging](https://arxiv.org/abs/2508.13599)
*Jinyoung Park,Minseok Son,Changick Kim*

Main category: cs.CV

TL;DR: MaMe是一种针对SSM视觉模型的令牌合并策略，通过量化令牌重要性和保持序列特性，显著提升了计算效率与性能的平衡。


<details>
  <summary>Details</summary>
Motivation: 提高SSM在计算机视觉中的计算效率，同时保持其独特的序列建模能力。

Method: 利用状态转移参数Δ作为信息度量，设计令牌合并策略以保持序列信息流。

Result: MaMe在令牌减少时仍保持性能，适用于图像、视频和音频领域。

Conclusion: MaMe为SSM应用提供了一种高效且通用的效率提升方法。

Abstract: State Space Models (SSMs) have emerged as powerful architectures in computer
vision, yet improving their computational efficiency remains crucial for
practical and scalable deployment.While token reduction serves as an effective
approach for model efficiency, applying it to SSMs requires careful
consideration of their unique sequential modeling capabilities.In this work, we
propose MaMe, a token-merging strategy tailored for SSM-based vision
models.MaMe addresses two key challenges: quantifying token importance and
preserving sequential properties. Our approach leverages the state transition
parameter $\mathbf{\Delta}$ as an informativeness measure and introduces
strategic token arrangements to preserve sequential information flow.Extensive
experiments demonstrate that MaMe achieves superior efficiency-performance
trade-offs for both fine-tuned and off-the-shelf models. Particularly, our
approach maintains robustness even under aggressive token reduction where
existing methods undergo significant performance degradation.Beyond image
classification, MaMe shows strong generalization capabilities across video and
audio domains, establishing an effective approach for enhancing efficiency in
diverse SSM applications.

</details>


### [42] [Unleashing Semantic and Geometric Priors for 3D Scene Completion](https://arxiv.org/abs/2508.13601)
*Shiyuan Chen,Wei Sui,Bohao Zhang,Zeyd Boukhers,John See,Cong Yang*

Main category: cs.CV

TL;DR: FoundationSSC通过双解耦设计提升3D语义场景补全性能，在语义和几何指标上均超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在语义和几何先验上存在冲突，限制了整体性能。

Method: 提出双解耦框架，包括源级和路径级解耦，结合混合视图变换和轴感知融合模块。

Result: 在SemanticKITTI和SSCBench-KITTI-360上取得最佳性能，分别提升0.23 mIoU和2.03 IoU。

Conclusion: FoundationSSC通过解耦设计和创新模块显著提升了3D语义场景补全的效果。

Abstract: Camera-based 3D semantic scene completion (SSC) provides dense geometric and
semantic perception for autonomous driving and robotic navigation. However,
existing methods rely on a coupled encoder to deliver both semantic and
geometric priors, which forces the model to make a trade-off between
conflicting demands and limits its overall performance. To tackle these
challenges, we propose FoundationSSC, a novel framework that performs dual
decoupling at both the source and pathway levels. At the source level, we
introduce a foundation encoder that provides rich semantic feature priors for
the semantic branch and high-fidelity stereo cost volumes for the geometric
branch. At the pathway level, these priors are refined through specialised,
decoupled pathways, yielding superior semantic context and depth distributions.
Our dual-decoupling design produces disentangled and refined inputs, which are
then utilised by a hybrid view transformation to generate complementary 3D
features. Additionally, we introduce a novel Axis-Aware Fusion (AAF) module
that addresses the often-overlooked challenge of fusing these features by
anisotropically merging them into a unified representation. Extensive
experiments demonstrate the advantages of FoundationSSC, achieving simultaneous
improvements in both semantic and geometric metrics, surpassing prior bests by
+0.23 mIoU and +2.03 IoU on SemanticKITTI. Additionally, we achieve
state-of-the-art performance on SSCBench-KITTI-360, with 21.78 mIoU and 48.61
IoU. The code will be released upon acceptance.

</details>


### [43] [PersonaVlog: Personalized Multimodal Vlog Generation with Multi-Agent Collaboration and Iterative Self-Correction](https://arxiv.org/abs/2508.13602)
*Xiaolu Hou,Bing Ma,Jiaxiang Cheng,Xuhua Ren,Kai Yu,Wenyue Li,Tianxiang Zheng,Qinglin Lu*

Main category: cs.CV

TL;DR: PersonaVlog是一个基于多模态大语言模型的自动化Vlog生成框架，支持个性化内容创作，并通过反馈机制和评估框架提升质量。


<details>
  <summary>Details</summary>
Motivation: 现有Vlog生成方法依赖预设脚本，缺乏动态性和个性化表达，亟需一种支持多模态协作和高个性化的自动化方案。

Method: 提出多代理协作框架，利用MLLM生成高质量多模态内容提示，并引入反馈和回滚机制进行迭代优化。

Result: 实验表明，PersonaVlog在生成个性化Vlog方面显著优于基线方法，展示了其高效性和潜力。

Conclusion: PersonaVlog为自动化Vlog生成提供了创新解决方案，具有广泛的应用前景。

Abstract: With the growing demand for short videos and personalized content, automated
Video Log (Vlog) generation has become a key direction in multimodal content
creation. Existing methods mostly rely on predefined scripts, lacking dynamism
and personal expression. Therefore, there is an urgent need for an automated
Vlog generation approach that enables effective multimodal collaboration and
high personalization. To this end, we propose PersonaVlog, an automated
multimodal stylized Vlog generation framework that can produce personalized
Vlogs featuring videos, background music, and inner monologue speech based on a
given theme and reference image. Specifically, we propose a multi-agent
collaboration framework based on Multimodal Large Language Models (MLLMs). This
framework efficiently generates high-quality prompts for multimodal content
creation based on user input, thereby improving the efficiency and creativity
of the process. In addition, we incorporate a feedback and rollback mechanism
that leverages MLLMs to evaluate and provide feedback on generated results,
thereby enabling iterative self-correction of multimodal content. We also
propose ThemeVlogEval, a theme-based automated benchmarking framework that
provides standardized metrics and datasets for fair evaluation. Comprehensive
experiments demonstrate the significant advantages and potential of our
framework over several baselines, highlighting its effectiveness and great
potential for generating automated Vlogs.

</details>


### [44] [Two-Factor Authentication Smart Entryway Using Modified LBPH Algorithm](https://arxiv.org/abs/2508.13617)
*Zakiah Ayop,Wan Mohamad Hariz Bin Wan Mohamad Rosdi,Looi Wei Hua,Syarulnaziah Anawar,Nur Fadzilah Othman*

Main category: cs.CV

TL;DR: 提出了一种基于树莓派平台的智能门禁系统，结合人脸识别和密码验证，并通过Telegram远程控制。


<details>
  <summary>Details</summary>
Motivation: 疫情期间，人脸口罩检测需求增加，但现有IoT系统在此方面开发不足。

Method: 采用LBPH算法进行全脸识别，改进的LBPH算法用于遮挡人脸检测，结合远程控制功能。

Result: 系统平均准确率70%，精确率80%，召回率83.26%，用户接受度高。

Conclusion: 系统能有效实现人脸识别、口罩检测及远程控制，适合未来应用。

Abstract: Face mask detection has become increasingly important recently, particularly
during the COVID-19 pandemic. Many face detection models have been developed in
smart entryways using IoT. However, there is a lack of IoT development on face
mask detection. This paper proposes a two-factor authentication system for
smart entryway access control using facial recognition and passcode
verification and an automation process to alert the owner and activate the
surveillance system when a stranger is detected and controls the system
remotely via Telegram on a Raspberry Pi platform. The system employs the Local
Binary Patterns Histograms for the full face recognition algorithm and modified
LBPH algorithm for occluded face detection. On average, the system achieved an
Accuracy of approximately 70%, a Precision of approximately 80%, and a Recall
of approximately 83.26% across all tested users. The results indicate that the
system is capable of conducting face recognition and mask detection, automating
the operation of the remote control to register users, locking or unlocking the
door, and notifying the owner. The sample participants highly accept it for
future use in the user acceptance test.

</details>


### [45] [TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head Synthesis](https://arxiv.org/abs/2508.13618)
*Shunian Chen,Hejin Huang,Yexin Liu,Zihan Ye,Pengcheng Chen,Chenghao Zhu,Michael Guan,Rongsheng Wang,Junying Chen,Guanbin Li,Ser-Nam Lim,Harry Yang,Benyou Wang*

Main category: cs.CV

TL;DR: TalkVid是一个大规模、高质量、多样化的数据集，用于解决音频驱动说话头合成模型在种族、语言和年龄群体多样性上的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有训练数据在规模、质量和多样性上的不足导致模型泛化能力差，无法覆盖全人类多样性。

Method: 通过多阶段自动化流程筛选高质量视频，构建TalkVid数据集，并发布TalkVid-Bench评估集。

Result: 在TalkVid上训练的模型表现优于其他数据集，且TalkVid-Bench揭示了传统指标掩盖的性能差异。

Conclusion: TalkVid和TalkVid-Bench为未来研究提供了必要的数据和评估工具。

Abstract: Audio-driven talking head synthesis has achieved remarkable photorealism, yet
state-of-the-art (SOTA) models exhibit a critical failure: they lack
generalization to the full spectrum of human diversity in ethnicity, language,
and age groups. We argue that this generalization gap is a direct symptom of
limitations in existing training data, which lack the necessary scale, quality,
and diversity. To address this challenge, we introduce TalkVid, a new
large-scale, high-quality, and diverse dataset containing 1244 hours of video
from 7729 unique speakers. TalkVid is curated through a principled, multi-stage
automated pipeline that rigorously filters for motion stability, aesthetic
quality, and facial detail, and is validated against human judgments to ensure
its reliability. Furthermore, we construct and release TalkVid-Bench, a
stratified evaluation set of 500 clips meticulously balanced across key
demographic and linguistic axes. Our experiments demonstrate that a model
trained on TalkVid outperforms counterparts trained on previous datasets,
exhibiting superior cross-dataset generalization. Crucially, our analysis on
TalkVid-Bench reveals performance disparities across subgroups that are
obscured by traditional aggregate metrics, underscoring its necessity for
future research. Code and data can be found in
https://github.com/FreedomIntelligence/TalkVid

</details>


### [46] [RCGNet: RGB-based Category-Level 6D Object Pose Estimation with Geometric Guidance](https://arxiv.org/abs/2508.13623)
*Sheng Yu,Di-Hua Zhai,Yuanqing Xia*

Main category: cs.CV

TL;DR: 提出了一种仅依赖RGB图像的类别级物体姿态估计方法，通过Transformer网络预测几何特征，结合RANSAC-PnP算法实现高效且高精度的姿态估计。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-D方法在缺乏深度信息的场景中表现不佳，因此需要一种仅依赖RGB图像的解决方案。

Method: 设计基于Transformer的神经网络预测几何特征，并引入几何特征引导算法增强几何信息表示，最后使用RANSAC-PnP算法计算姿态。

Result: 在基准数据集上表现出高效性和高精度，优于现有RGB方法。

Conclusion: 该方法为仅使用RGB图像的类别级物体姿态估计提供了新思路。

Abstract: While most current RGB-D-based category-level object pose estimation methods
achieve strong performance, they face significant challenges in scenes lacking
depth information. In this paper, we propose a novel category-level object pose
estimation approach that relies solely on RGB images. This method enables
accurate pose estimation in real-world scenarios without the need for depth
data. Specifically, we design a transformer-based neural network for
category-level object pose estimation, where the transformer is employed to
predict and fuse the geometric features of the target object. To ensure that
these predicted geometric features faithfully capture the object's geometry, we
introduce a geometric feature-guided algorithm, which enhances the network's
ability to effectively represent the object's geometric information. Finally,
we utilize the RANSAC-PnP algorithm to compute the object's pose, addressing
the challenges associated with variable object scales in pose estimation.
Experimental results on benchmark datasets demonstrate that our approach is not
only highly efficient but also achieves superior accuracy compared to previous
RGB-based methods. These promising results offer a new perspective for
advancing category-level object pose estimation using RGB images.

</details>


### [47] [DiffIER: Optimizing Diffusion Models with Iterative Error Reduction](https://arxiv.org/abs/2508.13628)
*Ao Chen,Lihe Ding,Tianfan Xue*

Main category: cs.CV

TL;DR: 论文提出DiffIER方法，通过优化推理阶段的累积误差，提升条件生成质量，减少对引导权重的敏感性。


<details>
  <summary>Details</summary>
Motivation: 发现训练-推理差距影响条件生成性能，导致输出对引导权重高度敏感。

Method: 提出DiffIER，一种基于优化的方法，通过迭代误差最小化减少推理阶段的累积误差。

Result: DiffIER在条件生成任务中优于基线方法，并在文本到图像、图像超分辨率和文本到语音生成中表现一致。

Conclusion: DiffIER是一种通用且高效的优化框架，有望在未来的研究中广泛应用。

Abstract: Diffusion models have demonstrated remarkable capabilities in generating
high-quality samples and enhancing performance across diverse domains through
Classifier-Free Guidance (CFG). However, the quality of generated samples is
highly sensitive to the selection of the guidance weight. In this work, we
identify a critical ``training-inference gap'' and we argue that it is the
presence of this gap that undermines the performance of conditional generation
and renders outputs highly sensitive to the guidance weight. We quantify this
gap by measuring the accumulated error during the inference stage and establish
a correlation between the selection of guidance weight and minimizing this gap.
Furthermore, to mitigate this gap, we propose DiffIER, an optimization-based
method for high-quality generation. We demonstrate that the accumulated error
can be effectively reduced by an iterative error minimization at each step
during inference. By introducing this novel plug-and-play optimization
framework, we enable the optimization of errors at every single inference step
and enhance generation quality. Empirical results demonstrate that our proposed
method outperforms baseline approaches in conditional generation tasks.
Furthermore, the method achieves consistent success in text-to-image
generation, image super-resolution, and text-to-speech generation, underscoring
its versatility and potential for broad applications in future research.

</details>


### [48] [OmniTry: Virtual Try-On Anything without Masks](https://arxiv.org/abs/2508.13632)
*Yutong Feng,Linlin Zhang,Hengyuan Cao,Yiming Chen,Xiaoduan Feng,Jian Cao,Yuxiong Wu,Bin Wang*

Main category: cs.CV

TL;DR: OmniTry是一个统一的虚拟试穿框架，扩展了传统VTON任务，支持多种可穿戴物品，无需掩码，通过两阶段训练解决数据配对问题。


<details>
  <summary>Details</summary>
Motivation: 现有VTON方法主要关注服装，缺乏对其他可穿戴物品的支持，且依赖掩码限制了实用性。

Method: 采用两阶段训练：第一阶段利用无配对图像训练掩码无关的定位模型；第二阶段用配对图像微调以保持外观一致性。

Result: 在12类可穿戴物品的基准测试中，OmniTry在定位和ID保持上优于现有方法。

Conclusion: OmniTry扩展了VTON的适用范围，解决了数据配对问题，具有更高的实用性。

Abstract: Virtual Try-ON (VTON) is a practical and widely-applied task, for which most
of existing works focus on clothes. This paper presents OmniTry, a unified
framework that extends VTON beyond garment to encompass any wearable objects,
e.g., jewelries and accessories, with mask-free setting for more practical
application. When extending to various types of objects, data curation is
challenging for obtaining paired images, i.e., the object image and the
corresponding try-on result. To tackle this problem, we propose a two-staged
pipeline: For the first stage, we leverage large-scale unpaired images, i.e.,
portraits with any wearable items, to train the model for mask-free
localization. Specifically, we repurpose the inpainting model to automatically
draw objects in suitable positions given an empty mask. For the second stage,
the model is further fine-tuned with paired images to transfer the consistency
of object appearance. We observed that the model after the first stage shows
quick convergence even with few paired samples. OmniTry is evaluated on a
comprehensive benchmark consisting of 12 common classes of wearable objects,
with both in-shop and in-the-wild images. Experimental results suggest that
OmniTry shows better performance on both object localization and
ID-preservation compared with existing methods. The code, model weights, and
evaluation benchmark of OmniTry will be made publicly available at
https://omnitry.github.io/.

</details>


### [49] [DeH4R: A Decoupled and Hybrid Method for Road Network Graph Extraction](https://arxiv.org/abs/2508.13669)
*Dengxian Gong,Shunping Ji*

Main category: cs.CV

TL;DR: DeH4R是一种新型混合模型，结合了图生成效率和图生长动态性，解决了道路网络提取中的拓扑保真度和计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在道路网络提取中难以同时保持拓扑保真度和计算效率，需要一种更高效且动态的方法。

Method: DeH4R通过解耦任务为候选顶点检测、相邻顶点预测、初始图构建和图扩展，实现了动态顶点（边）插入。

Result: 在CityScale和SpaceNet基准测试中，DeH4R表现优于现有方法，APLS和IoU分别提升4.62和10.18，速度提升10倍。

Conclusion: DeH4R在道路网络提取中实现了高效、动态且拓扑保真的性能，具有显著优势。

Abstract: The automated extraction of complete and precise road network graphs from
remote sensing imagery remains a critical challenge in geospatial computer
vision. Segmentation-based approaches, while effective in pixel-level
recognition, struggle to maintain topology fidelity after vectorization
postprocessing. Graph-growing methods build more topologically faithful graphs
but suffer from computationally prohibitive iterative ROI cropping.
Graph-generating methods first predict global static candidate road network
vertices, and then infer possible edges between vertices. They achieve fast
topology-aware inference, but limits the dynamic insertion of vertices. To
address these challenges, we propose DeH4R, a novel hybrid model that combines
graph-generating efficiency and graph-growing dynamics. This is achieved by
decoupling the task into candidate vertex detection, adjacent vertex
prediction, initial graph contruction, and graph expansion. This architectural
innovation enables dynamic vertex (edge) insertions while retaining fast
inference speed and enhancing both topology fidelity and spatial consistency.
Comprehensive evaluations on CityScale and SpaceNet benchmarks demonstrate
state-of-the-art (SOTA) performance. DeH4R outperforms the prior SOTA
graph-growing method RNGDet++ by 4.62 APLS and 10.18 IoU on CityScale, while
being approximately 10 $\times$ faster. The code will be made publicly
available at https://github.com/7777777FAN/DeH4R.

</details>


### [50] [HumanPCR: Probing MLLM Capabilities in Diverse Human-Centric Scenes](https://arxiv.org/abs/2508.13692)
*Keliang Li,Hongze Shen,Hao Shi,Ruibing Hou,Hong Chang,Jie Huang,Chenghao Jia,Wen Wang,Yiling Wu,Dongmei Jiang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: HumanPCR是一个评估多模态模型在人类相关视觉上下文能力的测试套件，涵盖感知、理解和推理三个层次，揭示了现有模型在人类中心视觉理解中的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于多模态模型的快速发展，需要评估其在多样化环境中的人类可比性能，特别是在人类相关的视觉上下文中。

Method: HumanPCR包括三个层次：感知（Human-P）、理解（Human-C）和推理（Human-R），分别通过多选问题和手动策划的视频推理测试来评估模型能力。

Result: 评估显示，现有模型在人类中心视觉理解任务中存在显著挑战，尤其是在空间感知、时间理解和心理建模方面。推理任务中模型难以主动提取视觉证据。

Conclusion: HumanPCR及其发现将推动多模态模型的开发、评估和人类中心应用。

Abstract: The aspiration for artificial general intelligence, fueled by the rapid
progress of multimodal models, demands human-comparable performance across
diverse environments. We propose HumanPCR, an evaluation suite for probing
MLLMs' capacity about human-related visual contexts across three hierarchical
levels: Perception, Comprehension, and Reasoning (denoted by Human-P, Human-C,
and Human-R, respectively). Human-P and Human-C feature over 6,000
human-verified multiple choice questions, assessing massive tasks of 9
dimensions, including but not limited to essential skills frequently overlooked
by existing benchmarks. Human-R offers a challenging manually curated video
reasoning test that requires integrating multiple visual evidences, proactively
extracting context beyond question cues, and applying human-like expertise.
Each question includes human-annotated Chain-of-Thought (CoT) rationales with
key visual evidence to support further research. Extensive evaluations on over
30 state-of-the-art models exhibit significant challenges in human-centric
visual understanding, particularly in tasks involving detailed space
perception, temporal understanding, and mind modeling. Moreover, analysis of
Human-R reveals the struggle of models in extracting essential proactive visual
evidence from diverse human scenes and their faulty reliance on query-guided
retrieval. Even with advanced techniques like scaling visual contexts and
test-time thinking yield only limited benefits. We hope HumanPCR and our
findings will advance the development, evaluation, and human-centric
application of multimodal models.

</details>


### [51] [Diversity-enhanced Collaborative Mamba for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2508.13712)
*Shumeng Li,Jian Zhang,Lei Qi,Luping Zhou,Yinghuan Shi,Yang Gao*

Main category: cs.CV

TL;DR: DCMamba框架通过数据、网络和特征多样性增强，显著提升了半监督医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注成本高，半监督技术利用未标注数据生成伪标签，结合Mamba模型的长程依赖处理能力，探索其在半监督医学图像分割中的潜力。

Method: 提出DCMamba框架，包括数据层面的补丁级弱-强混合增强、网络层面的多样扫描协作模块和特征层面的不确定性加权对比学习机制。

Result: 在Synapse数据集上，仅用20%标注数据即超越现有方法6.69%。

Conclusion: DCMamba通过多样性增强，有效提升了半监督医学图像分割的性能。

Abstract: Acquiring high-quality annotated data for medical image segmentation is
tedious and costly. Semi-supervised segmentation techniques alleviate this
burden by leveraging unlabeled data to generate pseudo labels. Recently,
advanced state space models, represented by Mamba, have shown efficient
handling of long-range dependencies. This drives us to explore their potential
in semi-supervised medical image segmentation. In this paper, we propose a
novel Diversity-enhanced Collaborative Mamba framework (namely DCMamba) for
semi-supervised medical image segmentation, which explores and utilizes the
diversity from data, network, and feature perspectives. Firstly, from the data
perspective, we develop patch-level weak-strong mixing augmentation with
Mamba's scanning modeling characteristics. Moreover, from the network
perspective, we introduce a diverse-scan collaboration module, which could
benefit from the prediction discrepancies arising from different scanning
directions. Furthermore, from the feature perspective, we adopt an
uncertainty-weighted contrastive learning mechanism to enhance the diversity of
feature representation. Experiments demonstrate that our DCMamba significantly
outperforms other semi-supervised medical image segmentation methods, e.g.,
yielding the latest SSM-based method by 6.69% on the Synapse dataset with 20%
labeled data.

</details>


### [52] [Hierarchical Vision-Language Retrieval of Educational Metaverse Content in Agriculture](https://arxiv.org/abs/2508.13713)
*Ali Abdari,Alex Falcon,Giuseppe Serra*

Main category: cs.CV

TL;DR: 论文提出了一个包含457个农业主题虚拟博物馆的新数据集，并开发了一种分层视觉语言模型，用于通过自然语言查询检索相关内容。


<details>
  <summary>Details</summary>
Motivation: 在线教育内容庞大但缺乏有效组织，Metaverse虽能提供沉浸式学习体验，但相关场景的搜索和匹配仍具挑战性。

Method: 引入AgriMuseums数据集，并提出分层视觉语言模型进行检索。

Result: 模型在实验中达到62% R@1和78% MRR，并在现有基准上提升了6% R@1和11% MRR。

Conclusion: 提出的方法和数据集有效，设计选择得到验证，代码和数据集已开源。

Abstract: Every day, a large amount of educational content is uploaded online across
different areas, including agriculture and gardening. When these videos or
materials are grouped meaningfully, they can make learning easier and more
effective. One promising way to organize and enrich such content is through the
Metaverse, which allows users to explore educational experiences in an
interactive and immersive environment. However, searching for relevant
Metaverse scenarios and finding those matching users' interests remains a
challenging task. A first step in this direction has been done recently, but
existing datasets are small and not sufficient for training advanced models. In
this work, we make two main contributions: first, we introduce a new dataset
containing 457 agricultural-themed virtual museums (AgriMuseums), each enriched
with textual descriptions; and second, we propose a hierarchical
vision-language model to represent and retrieve relevant AgriMuseums using
natural language queries. In our experimental setting, the proposed method
achieves up to about 62\% R@1 and 78\% MRR, confirming its effectiveness, and
it also leads to improvements on existing benchmarks by up to 6\% R@1 and 11\%
MRR. Moreover, an extensive evaluation validates our design choices. Code and
dataset are available at
https://github.com/aliabdari/Agricultural_Metaverse_Retrieval .

</details>


### [53] [Enhancing Targeted Adversarial Attacks on Large Vision-Language Models through Intermediate Projector Guidance](https://arxiv.org/abs/2508.13739)
*Yiming Cao,Yanjie Li,Kaisheng Liang,Yuni Lai,Bin Xiao*

Main category: cs.CV

TL;DR: IPGA是一种针对视觉语言模型的新型对抗攻击方法，通过攻击投影模块的中间阶段（如Q-Former）实现更精细的扰动控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在全局相似性上扰动图像，忽略了投影模块的作用，导致攻击粒度不足和效果受限。

Method: IPGA利用Q-Former的中间阶段生成对抗样本，并结合Residual Query Alignment（RQA）保留无关视觉内容。

Result: 实验表明IPGA在全局图像描述和细粒度视觉问答任务中优于现有方法，并能迁移到商业模型如Gemini和GPT。

Conclusion: IPGA通过攻击投影模块的中间阶段，显著提升了对抗攻击的精细度和有效性。

Abstract: Targeted adversarial attacks are essential for proactively identifying
security flaws in Vision-Language Models before real-world deployment. However,
current methods perturb images to maximize global similarity with the target
text or reference image at the encoder level, collapsing rich visual semantics
into a single global vector. This limits attack granularity, hindering
fine-grained manipulations such as modifying a car while preserving its
background. Furthermore, these methods largely overlook the projector module, a
critical semantic bridge between the visual encoder and the language model in
VLMs, thereby failing to disrupt the full vision-language alignment pipeline
within VLMs and limiting attack effectiveness. To address these issues, we
propose the Intermediate Projector Guided Attack (IPGA), the first method to
attack using the intermediate stage of the projector module, specifically the
widely adopted Q-Former, which transforms global image embeddings into
fine-grained visual features. This enables more precise control over
adversarial perturbations by operating on semantically meaningful visual tokens
rather than a single global representation. Specifically, IPGA leverages the
Q-Former pretrained solely on the first vision-language alignment stage,
without LLM fine-tuning, which improves both attack effectiveness and
transferability across diverse VLMs. Furthermore, we propose Residual Query
Alignment (RQA) to preserve unrelated visual content, thereby yielding more
controlled and precise adversarial manipulations. Extensive experiments show
that our attack method consistently outperforms existing methods in both
standard global image captioning tasks and fine-grained visual
question-answering tasks in black-box environment. Additionally, IPGA
successfully transfers to multiple commercial VLMs, including Google Gemini and
OpenAI GPT.

</details>


### [54] [Mitigating Cross-Image Information Leakage in LVLMs for Multi-Image Tasks](https://arxiv.org/abs/2508.13744)
*Yeji Park,Minyoung Lee,Sanghyuk Chun,Junsuk Choe*

Main category: cs.CV

TL;DR: FOCUS是一种无需训练的解码策略，通过顺序掩码和对比优化，显著提升大视觉语言模型在多图像任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型在多图像输入时性能下降，原因是不同图像的视觉线索在输出中相互干扰（跨图像信息泄漏）。

Method: 提出FOCUS策略：顺序掩码所有图像（仅保留一张干净图像），重复处理所有目标图像后聚合logits，并通过噪声参考输入对比优化。

Result: FOCUS在四个多图像基准测试和多种LVLM模型中均表现一致提升。

Conclusion: FOCUS为多图像推理提供了一种通用且实用的解决方案，无需额外训练或架构修改。

Abstract: Large Vision-Language Models (LVLMs) demonstrate strong performance on
single-image tasks. However, we observe that their performance degrades
significantly when handling multi-image inputs. This occurs because visual cues
from different images become entangled in the model's output. We refer to this
phenomenon as cross-image information leakage. To address this issue, we
propose FOCUS, a training-free and architecture-agnostic decoding strategy that
mitigates cross-image information leakage during inference. FOCUS sequentially
masks all but one image with random noise, guiding the model to focus on the
single clean image. We repeat this process across all target images to obtain
logits under partially masked contexts. These logits are aggregated and then
contrastively refined using a noise-only reference input, which suppresses the
leakage and yields more accurate outputs. FOCUS consistently improves
performance across four multi-image benchmarks and diverse LVLM families. This
demonstrates that FOCUS offers a general and practical solution for enhancing
multi-image reasoning without additional training or architectural
modifications.

</details>


### [55] [MR6D: Benchmarking 6D Pose Estimation for Mobile Robots](https://arxiv.org/abs/2508.13775)
*Anas Gouda,Shrutarv Awasthi,Christian Blesing,Lokeshwaran Manohar,Frank Hoffmann,Alice Kirchheim*

Main category: cs.CV

TL;DR: MR6D是一个专为移动机器人设计的6D姿态估计数据集，针对工业环境中的挑战，填补了现有数据集的不足。


<details>
  <summary>Details</summary>
Motivation: 现有6D姿态估计数据集主要针对小型家用物体，不适用于移动机器人面临的远距离感知、大物体和复杂遮挡等问题。

Method: MR6D包含92个真实场景，涵盖16种物体，捕捉了移动平台特有的挑战，如远距离视角和大物体遮挡。

Result: 实验表明，现有6D方法在MR6D上表现不佳，尤其是2D分割任务。

Conclusion: MR6D为移动机器人的姿态估计方法提供了新的评估基准。

Abstract: Existing 6D pose estimation datasets primarily focus on small household
objects typically handled by robot arm manipulators, limiting their relevance
to mobile robotics. Mobile platforms often operate without manipulators,
interact with larger objects, and face challenges such as long-range
perception, heavy self-occlusion, and diverse camera perspectives. While recent
models generalize well to unseen objects, evaluations remain confined to
household-like settings that overlook these factors. We introduce MR6D, a
dataset designed for 6D pose estimation for mobile robots in industrial
environments. It includes 92 real-world scenes featuring 16 unique objects
across static and dynamic interactions. MR6D captures the challenges specific
to mobile platforms, including distant viewpoints, varied object
configurations, larger object sizes, and complex occlusion/self-occlusion
patterns. Initial experiments reveal that current 6D pipelines underperform in
these settings, with 2D segmentation being another hurdle. MR6D establishes a
foundation for developing and evaluating pose estimation methods tailored to
the demands of mobile robotics. The dataset is available at
https://huggingface.co/datasets/anas-gouda/mr6d.

</details>


### [56] [Shape-from-Template with Generalised Camera](https://arxiv.org/abs/2508.13791)
*Agniva Sengupta,Stefan Zachow*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法，用于将3D形状非刚性配准到由多摄像头观测到的2D关键点。该方法扩展了单图像形状从模板（SfT）的应用范围，并提出了三种解决方案。


<details>
  <summary>Details</summary>
Motivation: 多摄像头观测的3D形状非刚性配准（SfT）在医学成像和手持摄像头等领域有广泛应用潜力，但目前研究主要集中在单图像上。本文旨在利用多摄像头信息提高配准精度。

Method: 提出了三种方法：1）关键点位于已知3D点的方向向量上；2）关键点位于未知3D点的方向向量上，但已知局部参考系的朝向；3）结合关键点和物体轮廓信息。前两种方法通过凸规划求解，第三种方法通过迭代优化实现。

Result: 实验表明，所提方法在合成和真实数据上均表现出高精度。

Conclusion: 本文首次提出了基于广义摄像头的SfT解决方案，通过多视角约束提高了配准精度，为相关领域提供了新的研究方向。

Abstract: This article presents a new method for non-rigidly registering a 3D shape to
2D keypoints observed by a constellation of multiple cameras. Non-rigid
registration of a 3D shape to observed 2D keypoints, i.e., Shape-from-Template
(SfT), has been widely studied using single images, but SfT with information
from multiple-cameras jointly opens new directions for extending the scope of
known use-cases such as 3D shape registration in medical imaging and
registration from hand-held cameras, to name a few. We represent such
multi-camera setup with the generalised camera model; therefore any collection
of perspective or orthographic cameras observing any deforming object can be
registered. We propose multiple approaches for such SfT: the first approach
where the corresponded keypoints lie on a direction vector from a known 3D
point in space, the second approach where the corresponded keypoints lie on a
direction vector from an unknown 3D point in space but with known orientation
w.r.t some local reference frame, and a third approach where, apart from
correspondences, the silhouette of the imaged object is also known. Together,
these form the first set of solutions to the SfT problem with generalised
cameras. The key idea behind SfT with generalised camera is the improved
reconstruction accuracy from estimating deformed shape while utilising the
additional information from the mutual constraints between multiple views of a
deformed object. The correspondence-based approaches are solved with convex
programming while the silhouette-based approach is an iterative refinement of
the results from the convex solutions. We demonstrate the accuracy of our
proposed methods on many synthetic and real data

</details>


### [57] [VisionLaw: Inferring Interpretable Intrinsic Dynamics from Visual Observations via Bilevel Optimization](https://arxiv.org/abs/2508.13792)
*Jiajing Lin,Shu Jiang,Qingyuan Zeng,Zhenzhong Wang,Min Jiang*

Main category: cs.CV

TL;DR: VisionLaw是一个双层优化框架，通过视觉观察推断可解释的内在动力学表达式，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在推断物体内在动力学时面临泛化性和可解释性不足的问题，VisionLaw旨在解决这些挑战。

Method: 采用双层优化框架：上层使用LLMs生成和修订本构定律，下层通过视觉模拟评估一致性。

Result: 在合成和真实数据集上验证，VisionLaw能有效推断可解释的内在动力学，泛化性强。

Conclusion: VisionLaw在交互式模拟中表现出色，适用于新场景。

Abstract: The intrinsic dynamics of an object governs its physical behavior in the real
world, playing a critical role in enabling physically plausible interactive
simulation with 3D assets. Existing methods have attempted to infer the
intrinsic dynamics of objects from visual observations, but generally face two
major challenges: one line of work relies on manually defined constitutive
priors, making it difficult to generalize to complex scenarios; the other
models intrinsic dynamics using neural networks, resulting in limited
interpretability and poor generalization. To address these challenges, we
propose VisionLaw, a bilevel optimization framework that infers interpretable
expressions of intrinsic dynamics from visual observations. At the upper level,
we introduce an LLMs-driven decoupled constitutive evolution strategy, where
LLMs are prompted as a knowledgeable physics expert to generate and revise
constitutive laws, with a built-in decoupling mechanism that substantially
reduces the search complexity of LLMs. At the lower level, we introduce a
vision-guided constitutive evaluation mechanism, which utilizes visual
simulation to evaluate the consistency between the generated constitutive law
and the underlying intrinsic dynamics, thereby guiding the upper-level
evolution. Experiments on both synthetic and real-world datasets demonstrate
that VisionLaw can effectively infer interpretable intrinsic dynamics from
visual observations. It significantly outperforms existing state-of-the-art
methods and exhibits strong generalization for interactive simulation in novel
scenarios.

</details>


### [58] [A Fully Transformer Based Multimodal Framework for Explainable Cancer Image Segmentation Using Radiology Reports](https://arxiv.org/abs/2508.13796)
*Enobong Adahada,Isabel Sassoon,Kate Hone,Yongmin Li*

Main category: cs.CV

TL;DR: Med-CTX是一种基于Transformer的多模态框架，用于可解释的乳腺癌超声分割，结合临床放射学报告提升性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 通过整合临床放射学报告，提升乳腺癌超声分割的性能和可解释性，为计算机辅助诊断提供透明度和信心。

Method: 采用双分支视觉编码器（ViT和Swin Transformer）和不确定性感知融合，结合BioClinicalBERT编码的临床语言，通过跨模态注意力融合视觉和文本特征。

Result: 在BUS-BRA数据集上，Dice分数达99%，IoU达95%，优于U-Net、ViT和Swin基线模型。临床文本显著提升分割准确性和解释质量。

Conclusion: Med-CTX为可信赖的多模态医学架构设定了新标准，具有高分割性能和临床可解释性。

Abstract: We introduce Med-CTX, a fully transformer based multimodal framework for
explainable breast cancer ultrasound segmentation. We integrate clinical
radiology reports to boost both performance and interpretability. Med-CTX
achieves exact lesion delineation by using a dual-branch visual encoder that
combines ViT and Swin transformers, as well as uncertainty aware fusion.
Clinical language structured with BI-RADS semantics is encoded by
BioClinicalBERT and combined with visual features utilising cross-modal
attention, allowing the model to provide clinically grounded, model generated
explanations. Our methodology generates segmentation masks, uncertainty maps,
and diagnostic rationales all at once, increasing confidence and transparency
in computer assisted diagnosis. On the BUS-BRA dataset, Med-CTX achieves a Dice
score of 99% and an IoU of 95%, beating existing baselines U-Net, ViT, and
Swin. Clinical text plays a key role in segmentation accuracy and explanation
quality, as evidenced by ablation studies that show a -5.4% decline in Dice
score and -31% in CIDEr. Med-CTX achieves good multimodal alignment (CLIP
score: 85%) and increased confi dence calibration (ECE: 3.2%), setting a new
bar for trustworthy, multimodal medical architecture.

</details>


### [59] [Timestep-Compressed Attack on Spiking Neural Networks through Timestep-Level Backpropagation](https://arxiv.org/abs/2508.13812)
*Donghwa Kang,Doohyun Kim,Sang-Ki Ko,Jinkyu Lee,Hyeongboo Baek,Brent ByungHoon Kang*

Main category: cs.CV

TL;DR: 提出了一种名为TCA的新型攻击框架，显著降低了SNN的攻击延迟，通过TLBP和A-MPR两种方法优化了攻击效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的攻击方法在SNN中存在高延迟问题，无法满足实时应用需求，因此需要一种更高效的攻击框架。

Method: TCA框架包含两个核心组件：TLBP（基于时间步的反向传播）和A-MPR（对抗性膜电位重用），分别优化了攻击生成和初始计算效率。

Result: 实验表明，TCA在白盒和黑盒设置下分别减少了56.6%和57.1%的攻击延迟，同时保持了较高的攻击成功率。

Conclusion: TCA通过利用SNN的特性，显著提升了攻击效率，为实时应用提供了可行的解决方案。

Abstract: State-of-the-art (SOTA) gradient-based adversarial attacks on spiking neural
networks (SNNs), which largely rely on extending FGSM and PGD frameworks, face
a critical limitation: substantial attack latency from multi-timestep
processing, rendering them infeasible for practical real-time applications.
This inefficiency stems from their design as direct extensions of ANN
paradigms, which fail to exploit key SNN properties. In this paper, we propose
the timestep-compressed attack (TCA), a novel framework that significantly
reduces attack latency. TCA introduces two components founded on key insights
into SNN behavior. First, timestep-level backpropagation (TLBP) is based on our
finding that global temporal information in backpropagation to generate
perturbations is not critical for an attack's success, enabling per-timestep
evaluation for early stopping. Second, adversarial membrane potential reuse
(A-MPR) is motivated by the observation that initial timesteps are
inefficiently spent accumulating membrane potential, a warm-up phase that can
be pre-calculated and reused. Our experiments on VGG-11 and ResNet-17 with the
CIFAR-10/100 and CIFAR10-DVS datasets show that TCA significantly reduces the
required attack latency by up to 56.6% and 57.1% compared to SOTA methods in
white-box and black-box settings, respectively, while maintaining a comparable
attack success rate.

</details>


### [60] [RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation](https://arxiv.org/abs/2508.13968)
*Tianyi Niu,Jaemin Cho,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CV

TL;DR: 论文研究了多模态大语言模型（MLLMs）在识别图像旋转角度（0°、90°、180°、270°）方面的能力，发现现有模型表现不佳，尤其是区分90°和270°旋转。


<details>
  <summary>Details</summary>
Motivation: 评估MLLMs在视觉推理和空间关系理解方面的能力，尤其是在图像旋转任务中的表现。

Method: 提出了RotBench基准测试，包含350张手动筛选的图像，测试了多种MLLMs（如GPT-5、o3、Gemini-2.5-Pro），并尝试了辅助信息和链式思维提示等方法。

Result: 大多数模型能可靠识别0°图像，部分能识别180°图像，但无法区分90°和270°旋转。辅助信息和微调对性能提升有限。

Conclusion: MLLMs在空间推理能力上与人类感知存在显著差距，尤其是在识别图像旋转方面。

Abstract: We investigate to what extent Multimodal Large Language Models (MLLMs) can
accurately identify the orientation of input images rotated 0{\deg}, 90{\deg},
180{\deg}, and 270{\deg}. This task demands robust visual reasoning
capabilities to detect rotational cues and contextualize spatial relationships
within images, regardless of their orientation. To evaluate MLLMs on these
abilities, we introduce RotBench -- a 350-image manually-filtered benchmark
comprising lifestyle, portrait, and landscape images. Despite the relatively
simple nature of this task, we show that several state-of-the-art open and
proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably
identify rotation in input images. Providing models with auxiliary information
-- including captions, depth maps, and more -- or using chain-of-thought
prompting offers only small and inconsistent improvements. Our results indicate
that most models are able to reliably identify right-side-up (0{\deg}) images,
while certain models are able to identify upside-down (180{\deg}) images. None
can reliably distinguish between 90{\deg} and 270{\deg}. Simultaneously showing
the image rotated in different orientations leads to moderate performance gains
for reasoning models, while a modified setup using voting improves the
performance of weaker models. We further show that fine-tuning does not improve
models' ability to distinguish 90{\deg} and 270{\deg} rotations, despite
substantially improving the identification of 180{\deg} images. Together, these
results reveal a significant gap between MLLMs' spatial reasoning capabilities
and human perception in identifying rotation.

</details>


### [61] [Unsupervised Urban Tree Biodiversity Mapping from Street-Level Imagery Using Spatially-Aware Visual Clustering](https://arxiv.org/abs/2508.13814)
*Diaa Addeen Abuhani,Marco Seccaroni,Martina Mazzarello,Imran Zualkernan,Fabio Duarte,Carlo Ratti*

Main category: cs.CV

TL;DR: 提出了一种无监督聚类框架，结合街景图像和空间种植模式，无需标签即可估计城市树木生物多样性。


<details>
  <summary>Details</summary>
Motivation: 城市树木生物多样性对气候韧性、生态稳定性和宜居性至关重要，但大多数城市缺乏详细的树冠数据。传统方法成本高且耗时，而监督AI方法需要标签数据且难以跨区域泛化。

Method: 通过无监督聚类框架，整合街景图像的视觉嵌入和空间种植模式，无需标签即可估计生物多样性。

Result: 在北美八个城市中，该方法能够高保真地恢复属级多样性模式，与真实数据的Wasserstein距离较低，并保留了空间自相关性。

Conclusion: 该方法可扩展且精细，适用于缺乏详细数据的城市，支持低成本持续监测，促进绿化公平和城市生态系统适应性管理。

Abstract: Urban tree biodiversity is critical for climate resilience, ecological
stability, and livability in cities, yet most municipalities lack detailed
knowledge of their canopies. Field-based inventories provide reliable estimates
of Shannon and Simpson diversity but are costly and time-consuming, while
supervised AI methods require labeled data that often fail to generalize across
regions. We introduce an unsupervised clustering framework that integrates
visual embeddings from street-level imagery with spatial planting patterns to
estimate biodiversity without labels. Applied to eight North American cities,
the method recovers genus-level diversity patterns with high fidelity,
achieving low Wasserstein distances to ground truth for Shannon and Simpson
indices and preserving spatial autocorrelation. This scalable, fine-grained
approach enables biodiversity mapping in cities lacking detailed inventories
and offers a pathway for continuous, low-cost monitoring to support equitable
access to greenery and adaptive management of urban ecosystems.

</details>


### [62] [GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation](https://arxiv.org/abs/2508.14036)
*Ken Deng,Yunhan Yang,Jingxiang Sun,Xihui Liu,Yebin Liu,Ding Liang,Yan-Pei Cao*

Main category: cs.CV

TL;DR: DetailGen3D是一种生成方法，专注于增强现有3D生成模型的几何细节，通过数据依赖的潜在空间流和令牌匹配策略实现高效训练和高保真细节合成。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法因计算限制导致输出缺乏几何细节，DetailGen3D旨在解决这一问题。

Method: 通过数据依赖的潜在空间流建模粗到细的转换，引入令牌匹配策略确保空间对应，并设计匹配训练数据以增强合成形状。

Result: 实验表明，DetailGen3D能高效合成高保真几何细节，适用于多种3D生成和重建方法。

Conclusion: DetailGen3D通过创新方法有效提升了3D形状的细节质量，同时保持训练效率。

Abstract: Modern 3D generation methods can rapidly create shapes from sparse or single
views, but their outputs often lack geometric detail due to computational
constraints. We present DetailGen3D, a generative approach specifically
designed to enhance these generated 3D shapes. Our key insight is to model the
coarse-to-fine transformation directly through data-dependent flows in latent
space, avoiding the computational overhead of large-scale 3D generative models.
We introduce a token matching strategy that ensures accurate spatial
correspondence during refinement, enabling local detail synthesis while
preserving global structure. By carefully designing our training data to match
the characteristics of synthesized coarse shapes, our method can effectively
enhance shapes produced by various 3D generation and reconstruction approaches,
from single-view to sparse multi-view inputs. Extensive experiments demonstrate
that DetailGen3D achieves high-fidelity geometric detail synthesis while
maintaining efficiency in training.

</details>


### [63] [Self-Aware Adaptive Alignment: Enabling Accurate Perception for Intelligent Transportation Systems](https://arxiv.org/abs/2508.13823)
*Tong Xiang,Hongxia Zhao,Fenghua Zhu,Yuanyuan Chen,Yisheng Lv*

Main category: cs.CV

TL;DR: SA3方法通过自适应对齐机制和识别策略，在跨域智能交通检测中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决跨域场景下智能交通检测的挑战，提升检测性能。

Method: 采用基于注意力的对齐模块和实例到图像级别的对齐模块，实现源域和目标域的自适应对齐。

Result: 在跨域目标检测基准测试中，SA3优于现有最优方法。

Conclusion: SA3方法有效解决了跨域检测问题，性能显著提升。

Abstract: Achieving top-notch performance in Intelligent Transportation detection is a
critical research area. However, many challenges still need to be addressed
when it comes to detecting in a cross-domain scenario. In this paper, we
propose a Self-Aware Adaptive Alignment (SA3), by leveraging an efficient
alignment mechanism and recognition strategy. Our proposed method employs a
specified attention-based alignment module trained on source and target domain
datasets to guide the image-level features alignment process, enabling the
local-global adaptive alignment between the source domain and target domain.
Features from both domains, whose channel importance is re-weighted, are fed
into the region proposal network, which facilitates the acquisition of salient
region features. Also, we introduce an instance-to-image level alignment module
specific to the target domain to adaptively mitigate the domain gap. To
evaluate the proposed method, extensive experiments have been conducted on
popular cross-domain object detection benchmarks. Experimental results show
that SA3 achieves superior results to the previous state-of-the-art methods.

</details>


### [64] [SAGA: Learning Signal-Aligned Distributions for Improved Text-to-Image Generation](https://arxiv.org/abs/2508.13866)
*Paul Grimal,Michaël Soumm,Hervé Le Borgne,Olivier Ferret,Akihiro Sugimoto*

Main category: cs.CV

TL;DR: 提出了一种新方法，通过建模去噪过程中的信号成分，确保生成的图像更准确地反映文本提示，同时支持额外的条件模态（如边界框）以提高空间对齐。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在精确对齐文本提示方面存在困难，导致关键元素缺失或概念混淆。

Method: 学习基于目标提示的高成功率分布，建模去噪过程中的信号成分，提供细粒度控制。

Result: 实验表明，该方法优于当前最先进的方法。

Conclusion: 该方法无需训练，可与现有扩散和流匹配架构无缝集成，支持额外条件模态，显著提升生成图像与文本提示的对齐效果。

Abstract: State-of-the-art text-to-image models produce visually impressive results but
often struggle with precise alignment to text prompts, leading to missing
critical elements or unintended blending of distinct concepts. We propose a
novel approach that learns a high-success-rate distribution conditioned on a
target prompt, ensuring that generated images faithfully reflect the
corresponding prompts. Our method explicitly models the signal component during
the denoising process, offering fine-grained control that mitigates
over-optimization and out-of-distribution artifacts. Moreover, our framework is
training-free and seamlessly integrates with both existing diffusion and flow
matching architectures. It also supports additional conditioning modalities --
such as bounding boxes -- for enhanced spatial alignment. Extensive experiments
demonstrate that our approach outperforms current state-of-the-art methods. The
code is available at https://github.com/grimalPaul/gsn-factory.

</details>


### [65] [RED.AI Id-Pattern: First Results of Stone Deterioration Patterns with Multi-Agent Systems](https://arxiv.org/abs/2508.13872)
*Daniele Corradetti,José Delgado Rodrigues*

Main category: cs.CV

TL;DR: RED.AI项目中的Id-Pattern系统是一个多代理AI系统，用于识别石材劣化模式，相比传统方法更高效。


<details>
  <summary>Details</summary>
Motivation: 传统石材劣化识别方法依赖专家团队的直接观察，耗时耗资源。

Method: 采用多代理AI系统，模拟专家协作，通过视觉证据自动诊断石材病理。

Result: 在28张复杂图像上的测试显示，系统在所有指标上均显著优于基础模型。

Conclusion: 该系统为石材劣化诊断提供了高效且自动化的解决方案。

Abstract: The Id-Pattern system within the RED.AI project (Reabilita\c{c}\~ao
Estrutural Digital atrav\'es da AI) consists of an agentic system designed to
assist in the identification of stone deterioration patterns. Traditional
methodologies, based on direct observation by expert teams, are accurate but
costly in terms of time and resources. The system developed here introduces and
evaluates a multi-agent artificial intelligence (AI) system, designed to
simulate collaboration between experts and automate the diagnosis of stone
pathologies from visual evidence. The approach is based on a cognitive
architecture that orchestrates a team of specialized AI agents which, in this
specific case, are limited to five: a lithologist, a pathologist, an
environmental expert, a conservator-restorer, and a diagnostic coordinator. To
evaluate the system we selected 28 difficult images involving multiple
deterioration patterns. Our first results showed a huge boost on all metrics of
our system compared to the foundational model.

</details>


### [66] [RICO: Two Realistic Benchmarks and an In-Depth Analysis for Incremental Learning in Object Detection](https://arxiv.org/abs/2508.13878)
*Matthias Neuwirth-Trapp,Maarten Bieshaar,Danda Pani Paudel,Luc Van Gool*

Main category: cs.CV

TL;DR: 论文提出了两个真实增量学习目标检测基准（RICO），揭示了现有方法在适应性和知识保留上的不足，并发现少量数据回放已优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有增量学习评估多基于简化基准，掩盖了真实场景中的性能问题，因此需要更真实的基准来评估增量学习。

Method: 引入两个真实增量学习目标检测基准（D-RICO和EC-RICO），基于14个多样化数据集构建，涵盖多种实际挑战。

Result: 实验表明，现有增量学习方法在适应性和知识保留上表现不佳，少量数据回放已优于现有方法，但独立训练仍最优。

Conclusion: 增量学习方法在真实场景中仍有改进空间，需解决蒸馏中的弱教师问题、模型多样性任务管理能力不足及可塑性不足等问题。

Abstract: Incremental Learning (IL) trains models sequentially on new data without full
retraining, offering privacy, efficiency, and scalability. IL must balance
adaptability to new data with retention of old knowledge. However, evaluations
often rely on synthetic, simplified benchmarks, obscuring real-world IL
performance. To address this, we introduce two Realistic Incremental Object
Detection Benchmarks (RICO): Domain RICO (D-RICO) features domain shifts with a
fixed class set, and Expanding-Classes RICO (EC-RICO) integrates new domains
and classes per IL step. Built from 14 diverse datasets covering real and
synthetic domains, varying conditions (e.g., weather, time of day), camera
sensors, perspectives, and labeling policies, both benchmarks capture
challenges absent in existing evaluations. Our experiments show that all IL
methods underperform in adaptability and retention, while replaying a small
amount of previous data already outperforms all methods. However, individual
training on the data remains superior. We heuristically attribute this gap to
weak teachers in distillation, single models' inability to manage diverse
tasks, and insufficient plasticity. Our code will be made publicly available.

</details>


### [67] [In-hoc Concept Representations to Regularise Deep Learning in Medical Imaging](https://arxiv.org/abs/2508.13880)
*Valentina Corbetta,Floris Six Dijkstra,Regina Beets-Tan,Hoel Kervadec,Kristoffer Wickstrøm,Wilson Silva*

Main category: cs.CV

TL;DR: LCRReg是一种新的正则化方法，利用潜在概念表示（LCRs）引导模型学习更具临床意义的特征，提高医学影像模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医学影像中的深度学习模型在分布变化下泛化能力差，常依赖虚假相关性而非临床相关特征。

Method: 通过辅助数据集合成高质量、解耦的概念示例，提取LCRs，并引入正则化项引导CNN激活相关概念子空间。

Result: 在合成和真实医学任务中，LCRReg显著提升对虚假相关性的鲁棒性，并在糖尿病视网膜病变分类中改善OOD泛化性能。

Conclusion: LCRReg是一种轻量级、架构无关的策略，无需密集概念标注即可提升模型鲁棒性。

Abstract: Deep learning models in medical imaging often achieve strong in-distribution
performance but struggle to generalise under distribution shifts, frequently
relying on spurious correlations instead of clinically meaningful features. We
introduce LCRReg, a novel regularisation approach that leverages Latent Concept
Representations (LCRs) (e.g., Concept Activation Vectors (CAVs)) to guide
models toward semantically grounded representations. LCRReg requires no concept
labels in the main training set and instead uses a small auxiliary dataset to
synthesise high-quality, disentangled concept examples. We extract LCRs for
predefined relevant features, and incorporate a regularisation term that guides
a Convolutional Neural Network (CNN) to activate within latent subspaces
associated with those concepts. We evaluate LCRReg across synthetic and
real-world medical tasks. On a controlled toy dataset, it significantly
improves robustness to injected spurious correlations and remains effective
even in multi-concept and multiclass settings. On the diabetic retinopathy
binary classification task, LCRReg enhances performance under both synthetic
spurious perturbations and out-of-distribution (OOD) generalisation. Compared
to baselines, including multitask learning, linear probing, and post-hoc
concept-based models, LCRReg offers a lightweight, architecture-agnostic
strategy for improving model robustness without requiring dense concept
supervision. Code is available at the following link:
https://github.com/Trustworthy-AI-UU-NKI/lcr\_regularization

</details>


### [68] [Forecasting Smog Events Using ConvLSTM: A Spatio-Temporal Approach for Aerosol Index Prediction in South Asia](https://arxiv.org/abs/2508.13891)
*Taimur Khan*

Main category: cs.CV

TL;DR: 研究利用Sentinel-5P数据和ConvLSTM神经网络预测南亚雾霾事件，结果显示五天间隔的预测效果较好，但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 南亚雾霾对印度-恒河平原造成严重影响，但缺乏实时预测系统，研究旨在填补这一空白。

Method: 使用Sentinel-5P数据（2019-2023）和ConvLSTM神经网络，以紫外气溶胶指数为预测因子。

Result: 五天间隔的预测误差较小（MSE~0.0018），结构相似性指数为~0.74。

Conclusion: 模型有效但可通过整合更多数据和优化架构进一步提升。

Abstract: The South Asian Smog refers to the recurring annual air pollution events
marked by high contaminant levels, reduced visibility, and significant
socio-economic impacts, primarily affecting the Indo-Gangetic Plains (IGP) from
November to February. Over the past decade, increased air pollution sources
such as crop residue burning, motor vehicles, and changing weather patterns
have intensified these smog events. However, real-time forecasting systems for
increased particulate matter concentrations are still not established at
regional scale. The Aerosol Index, closely tied to smog formation and a key
component in calculating the Air Quality Index (AQI), reflects particulate
matter concentrations. This study forecasts aerosol events using Sentinel-5P
air constituent data (2019-2023) and a Convolutional Long-Short Term Memory
(ConvLSTM) neural network, which captures spatial and temporal correlations
more effectively than previous models. Using the Ultraviolet (UV) Aerosol Index
at 340-380 nm as the predictor, results show the Aerosol Index can be
forecasted at five-day intervals with a Mean Squared Error of ~0.0018, loss of
~0.3995, and Structural Similarity Index of ~0.74. While effective, the model
can be improved by integrating additional data and refining its architecture.

</details>


### [69] [SCRNet: Spatial-Channel Regulation Network for Medical Ultrasound Image Segmentation](https://arxiv.org/abs/2508.13899)
*Weixin Xu,Ziliang Wang*

Main category: cs.CV

TL;DR: 提出了一种结合卷积和交叉注意力机制的模块（FAM和CCAPM），并通过SCRM增强特征选择能力，最终在UNet架构中实现SOTA性能的SCRNet框架。


<details>
  <summary>Details</summary>
Motivation: 解决CNN忽视长距离依赖和Transformer忽略局部上下文信息的问题。

Method: 设计FAM和CCAPM模块，结合卷积与交叉注意力，并通过SCRM增强特征选择，集成到UNet中。

Result: SCRNet在实验中表现优于现有方法，达到SOTA性能。

Conclusion: 提出的SCRNet框架有效结合了长距离依赖和局部上下文信息，显著提升了医学超声图像分割的性能。

Abstract: Medical ultrasound image segmentation presents a formidable challenge in the
realm of computer vision. Traditional approaches rely on Convolutional Neural
Networks (CNNs) and Transformer-based methods to address the intricacies of
medical image segmentation. Nevertheless, inherent limitations persist, as
CNN-based methods tend to disregard long-range dependencies, while
Transformer-based methods may overlook local contextual information. To address
these deficiencies, we propose a novel Feature Aggregation Module (FAM)
designed to process two input features from the preceding layer. These features
are seamlessly directed into two branches of the Convolution and
Cross-Attention Parallel Module (CCAPM) to endow them with different roles in
each of the two branches to help establish a strong connection between the two
input features. This strategy enables our module to focus concurrently on both
long-range dependencies and local contextual information by judiciously merging
convolution operations with cross-attention mechanisms. Moreover, by
integrating FAM within our proposed Spatial-Channel Regulation Module (SCRM),
the ability to discern salient regions and informative features warranting
increased attention is enhanced. Furthermore, by incorporating the SCRM into
the encoder block of the UNet architecture, we introduce a novel framework
dubbed Spatial-Channel Regulation Network (SCRNet). The results of our
extensive experiments demonstrate the superiority of SCRNet, which consistently
achieves state-of-the-art (SOTA) performance compared to existing methods.

</details>


### [70] [PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis](https://arxiv.org/abs/2508.13911)
*Chunji Lv,Zequn Chen,Donglin Di,Weinan Zhang,Hao Li,Wei Chen,Changsheng Li*

Main category: cs.CV

TL;DR: PhysGM是一个从单张图像联合预测3D高斯表示及其物理属性的前馈框架，支持即时物理模拟和高保真4D渲染。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预重建的3D高斯表示，物理集成要么依赖不灵活的手动定义属性，要么依赖不稳定的视频模型优化指导。

Method: 联合优化高斯重建和概率物理预测，通过物理合理的参考视频增强渲染和物理预测，采用DPO避免复杂的SDS优化。

Result: 实验表明，PhysGM能在1分钟内从单张图像生成高保真4D模拟，速度显著优于现有方法。

Conclusion: PhysGM克服了现有方法的局限性，实现了高效且高质量的物理模拟和渲染。

Abstract: While physics-grounded 3D motion synthesis has seen significant progress,
current methods face critical limitations. They typically rely on
pre-reconstructed 3D Gaussian Splatting (3DGS) representations, while physics
integration depends on either inflexible, manually defined physical attributes
or unstable, optimization-heavy guidance from video models. To overcome these
challenges, we introduce PhysGM, a feed-forward framework that jointly predicts
a 3D Gaussian representation and its physical properties from a single image,
enabling immediate, physical simulation and high-fidelity 4D rendering. We
first establish a base model by jointly optimizing for Gaussian reconstruction
and probabilistic physics prediction. The model is then refined with physically
plausible reference videos to enhance both rendering fidelity and physics
prediction accuracy. We adopt the Direct Preference Optimization (DPO) to align
its simulations with reference videos, circumventing Score Distillation
Sampling (SDS) optimization which needs back-propagating gradients through the
complex differentiable simulation and rasterization. To facilitate the
training, we introduce a new dataset PhysAssets of over 24,000 3D assets,
annotated with physical properties and corresponding guiding videos.
Experimental results demonstrate that our method effectively generates
high-fidelity 4D simulations from a single image in one minute. This represents
a significant speedup over prior works while delivering realistic rendering
results. Our project page is at:https://hihixiaolv.github.io/PhysGM.github.io/

</details>


### [71] [DIME-Net: A Dual-Illumination Adaptive Enhancement Network Based on Retinex and Mixture-of-Experts](https://arxiv.org/abs/2508.13921)
*Ziang Wang,Xiaoqin Wang,Dingyi Wang,Qiang Li,Shushan Qiao*

Main category: cs.CV

TL;DR: DIME-Net是一个双光照增强框架，通过混合专家模块和损伤修复模块，统一处理低光和背光场景，提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法无法统一处理多种光照退化的问题，提升复杂光照条件下的图像质量。

Method: 提出混合专家光照估计模块和损伤修复模块，结合Retinex理论，构建混合光照数据集MixBL。

Result: 在合成和真实数据集上表现优异，无需重新训练即可适应多种光照条件。

Conclusion: DIME-Net具有泛化能力，适用于复杂光照下的多媒体应用。

Abstract: Image degradation caused by complex lighting conditions such as low-light and
backlit scenarios is commonly encountered in real-world environments,
significantly affecting image quality and downstream vision tasks. Most
existing methods focus on a single type of illumination degradation and lack
the ability to handle diverse lighting conditions in a unified manner. To
address this issue, we propose a dual-illumination enhancement framework called
DIME-Net. The core of our method is a Mixture-of-Experts illumination estimator
module, where a sparse gating mechanism adaptively selects suitable S-curve
expert networks based on the illumination characteristics of the input image.
By integrating Retinex theory, this module effectively performs enhancement
tailored to both low-light and backlit images. To further correct
illumination-induced artifacts and color distortions, we design a damage
restoration module equipped with Illumination-Aware Cross Attention and
Sequential-State Global Attention mechanisms. In addition, we construct a
hybrid illumination dataset, MixBL, by integrating existing datasets, allowing
our model to achieve robust illumination adaptability through a single training
process. Experimental results show that DIME-Net achieves competitive
performance on both synthetic and real-world low-light and backlit datasets
without any retraining. These results demonstrate its generalization ability
and potential for practical multimedia applications under diverse and complex
illumination conditions.

</details>


### [72] [ViT-FIQA: Assessing Face Image Quality using Vision Transformers](https://arxiv.org/abs/2508.13957)
*Andrea Atzori,Fadi Boutros,Naser Damer*

Main category: cs.CV

TL;DR: ViT-FIQA是一种基于Vision Transformer的Face Image Quality Assessment方法，通过可学习的质量标记预测人脸图像的效用分数。


<details>
  <summary>Details</summary>
Motivation: 现有FIQA方法主要依赖CNN，而Vision Transformer的潜力未被充分探索。

Method: 扩展标准ViT骨干网络，引入可学习的质量标记，通过自注意力机制聚合信息，并分为两个头部：一个用于学习人脸表示，另一个用于预测效用分数。

Result: 在多个基准测试和FR模型上，ViT-FIQA表现优异，证明了Transformer架构在FIQA中的有效性。

Conclusion: ViT-FIQA展示了Transformer在FIQA中的潜力，为未来研究提供了可扩展的基础。

Abstract: Face Image Quality Assessment (FIQA) aims to predict the utility of a face
image for face recognition (FR) systems. State-of-the-art FIQA methods mainly
rely on convolutional neural networks (CNNs), leaving the potential of Vision
Transformer (ViT) architectures underexplored. This work proposes ViT-FIQA, a
novel approach that extends standard ViT backbones, originally optimized for
FR, through a learnable quality token designed to predict a scalar utility
score for any given face image. The learnable quality token is concatenated
with the standard image patch tokens, and the whole sequence is processed via
global self-attention by the ViT encoders to aggregate contextual information
across all patches. At the output of the backbone, ViT-FIQA branches into two
heads: (1) the patch tokens are passed through a fully connected layer to learn
discriminative face representations via a margin-penalty softmax loss, and (2)
the quality token is fed into a regression head to learn to predict the face
sample's utility. Extensive experiments on challenging benchmarks and several
FR models, including both CNN- and ViT-based architectures, demonstrate that
ViT-FIQA consistently achieves top-tier performance. These results underscore
the effectiveness of transformer-based architectures in modeling face image
utility and highlight the potential of ViTs as a scalable foundation for future
FIQA research https://cutt.ly/irHlzXUC.

</details>


### [73] [ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving](https://arxiv.org/abs/2508.13977)
*Xianda Guo,Ruijun Zhang,Yiqun Duan,Ruilin Wang,Keyuan Zhou,Wenzhao Zheng,Wenke Huang,Gangwei Xu,Mike Horton,Yuan Si,Hao Zhao,Long Chen*

Main category: cs.CV

TL;DR: 论文提出了一种大规模、多样化的深度估计数据集，用于动态户外驾驶环境，填补了现有数据集在多样性和可扩展性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有深度估计数据集（如KITTI、nuScenes和DDAD）在多样性和可扩展性上存在局限，且基准性能接近饱和，需要新一代数据集支持基础模型和多模态学习。

Method: 通过轻量级采集流程构建了包含20K视频帧的大规模数据集，提供稀疏但统计上足够的地面真实数据。

Result: 与现有数据集相比，新数据集在驾驶场景多样性和深度密度上更具挑战性，基准实验验证了其有效性并揭示了性能差距。

Conclusion: 新数据集为深度估计研究提供了新平台，推动了该领域的进一步发展。

Abstract: Depth estimation is a fundamental task for 3D scene understanding in
autonomous driving, robotics, and augmented reality. Existing depth datasets,
such as KITTI, nuScenes, and DDAD, have advanced the field but suffer from
limitations in diversity and scalability. As benchmark performance on these
datasets approaches saturation, there is an increasing need for a new
generation of large-scale, diverse, and cost-efficient datasets to support the
era of foundation models and multi-modal learning. To address these challenges,
we introduce a large-scale, diverse, frame-wise continuous dataset for depth
estimation in dynamic outdoor driving environments, comprising 20K video frames
to evaluate existing methods. Our lightweight acquisition pipeline ensures
broad scene coverage at low cost, while sparse yet statistically sufficient
ground truth enables robust training. Compared to existing datasets, ours
presents greater diversity in driving scenarios and lower depth density,
creating new challenges for generalization. Benchmark experiments with standard
monocular depth estimation models validate the dataset's utility and highlight
substantial performance gaps in challenging conditions, establishing a new
platform for advancing depth estimation research.

</details>


### [74] [OmViD: Omni-supervised active learning for video action detection](https://arxiv.org/abs/2508.13983)
*Aayush Rana,Akash Kumar,Vibhav Vineet,Yogesh S Rawat*

Main category: cs.CV

TL;DR: 论文研究了视频动作检测中不同标注类型的影响，提出了一种主动学习策略和3D超像素方法，显著降低了标注成本。


<details>
  <summary>Details</summary>
Motivation: 视频动作检测需要密集的时空标注，成本高且困难。本文旨在分析不同标注类型对检测的影响，并探索如何高效利用不同标注。

Method: 提出主动学习策略确定视频所需标注类型，并引入3D超像素方法生成伪标签进行训练。

Result: 在UCF101-24和JHMDB-21数据集上验证，显著降低标注成本且性能损失最小。

Conclusion: 通过灵活选择标注类型和生成伪标签，可高效降低视频动作检测的标注成本。

Abstract: Video action detection requires dense spatio-temporal annotations, which are
both challenging and expensive to obtain. However, real-world videos often vary
in difficulty and may not require the same level of annotation. This paper
analyzes the appropriate annotation types for each sample and their impact on
spatio-temporal video action detection. It focuses on two key aspects: 1) how
to obtain varying levels of annotation for videos, and 2) how to learn action
detection from different annotation types. The study explores video-level tags,
points, scribbles, bounding boxes, and pixel-level masks. First, a simple
active learning strategy is proposed to estimate the necessary annotation type
for each video. Then, a novel spatio-temporal 3D-superpixel approach is
introduced to generate pseudo-labels from these annotations, enabling effective
training. The approach is validated on UCF101-24 and JHMDB-21 datasets,
significantly cutting annotation costs with minimal performance loss.

</details>


### [75] [Physics-Based 3D Simulation for Synthetic Data Generation and Failure Analysis in Packaging Stability Assessment](https://arxiv.org/abs/2508.13989)
*Samuel Seligardi,Pietro Musoni,Eleonora Iotti,Gianluca Contesso,Alessandro Dal Palù*

Main category: cs.CV

TL;DR: 提出了一种可控且精确的物理模拟系统，用于模拟托盘运动行为，结合深度学习预测碰撞测试，减少物理测试需求。


<details>
  <summary>Details</summary>
Motivation: 物流行业需求增长，塑料包装的环保替代品研究需求增加，需确保运输安全。

Method: 开发3D图形虚拟环境模拟托盘行为，训练深度神经网络评估模拟视频作为碰撞测试预测器。

Result: 系统减少物理测试需求，降低成本与环境影响，提高测量精度。

Conclusion: 创新方法结合模拟与深度学习，提升托盘安全分析的实用性和效率。

Abstract: The design and analysis of pallet setups are essential for ensuring safety of
packages transportation. With rising demands in the logistics sector, the
development of automated systems utilizing advanced technologies has become
increasingly crucial. Moreover, the widespread use of plastic wrapping has
motivated researchers to investigate eco-friendly alternatives that still
adhere to safety standards. We present a fully controllable and accurate
physical simulation system capable of replicating the behavior of moving
pallets. It features a 3D graphics-based virtual environment that supports a
wide range of configurations, including variable package layouts, different
wrapping materials, and diverse dynamic conditions. This innovative approach
reduces the need for physical testing, cutting costs and environmental impact
while improving measurement accuracy for analyzing pallet dynamics.
Additionally, we train a deep neural network to evaluate the rendered videos
generated by our simulator, as a crash-test predictor for pallet
configurations, further enhancing the system's utility in safety analysis.

</details>


### [76] [Self-Supervised Sparse Sensor Fusion for Long Range Perception](https://arxiv.org/abs/2508.13995)
*Edoardo Palladin,Samuel Brucker,Filippo Ghilotti,Praveen Narayanan,Mario Bijelic,Felix Heide*

Main category: cs.CV

TL;DR: 论文提出了一种高效的3D编码方法，扩展了自动驾驶车辆在高速公路上的感知距离至250米，显著提升了目标检测和LiDAR预测的性能。


<details>
  <summary>Details</summary>
Motivation: 城市驾驶的感知范围（50-100米）不足以满足高速公路高速行驶的需求（需250米），且现有方法在长距离感知上存在内存和计算成本问题。

Method: 基于稀疏表示，引入高效的多模态和时间特征3D编码，以及自监督预训练方案，利用未标注的相机-LiDAR数据进行大规模学习。

Result: 感知距离扩展至250米，目标检测mAP提升26.6%，LiDAR预测的Chamfer Distance降低30.5%。

Conclusion: 该方法有效解决了长距离感知的挑战，为高速公路自动驾驶提供了更可靠的解决方案。

Abstract: Outside of urban hubs, autonomous cars and trucks have to master driving on
intercity highways. Safe, long-distance highway travel at speeds exceeding 100
km/h demands perception distances of at least 250 m, which is about five times
the 50-100m typically addressed in city driving, to allow sufficient planning
and braking margins. Increasing the perception ranges also allows to extend
autonomy from light two-ton passenger vehicles to large-scale forty-ton trucks,
which need a longer planning horizon due to their high inertia. However, most
existing perception approaches focus on shorter ranges and rely on Bird's Eye
View (BEV) representations, which incur quadratic increases in memory and
compute costs as distance grows. To overcome this limitation, we built on top
of a sparse representation and introduced an efficient 3D encoding of
multi-modal and temporal features, along with a novel self-supervised
pre-training scheme that enables large-scale learning from unlabeled
camera-LiDAR data. Our approach extends perception distances to 250 meters and
achieves an 26.6% improvement in mAP in object detection and a decrease of
30.5% in Chamfer Distance in LiDAR forecasting compared to existing methods,
reaching distances up to 250 meters. Project Page:
https://light.princeton.edu/lrs4fusion/

</details>


### [77] [ResPlan: A Large-Scale Vector-Graph Dataset of 17,000 Residential Floor Plans](https://arxiv.org/abs/2508.14006)
*Mohamed Abouagour,Eleftherios Garyfallidis*

Main category: cs.CV

TL;DR: ResPlan是一个包含17,000个详细、结构丰富且真实的住宅平面图的大规模数据集，旨在推动空间AI研究。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据集（如RPLAN和MSD）在视觉保真度和结构多样性上的不足，提供更真实和非理想化的住宅布局。

Method: 数据集包含精确的建筑元素和功能空间标注，提供几何和基于图的格式，并附带开源几何清理和标注优化流程。

Result: ResPlan在规模、真实性和可用性上显著提升，支持多种应用（如机器人、生成AI、虚拟现实等）。

Conclusion: ResPlan为开发和评估下一代空间智能系统提供了坚实的基础。

Abstract: We introduce ResPlan, a large-scale dataset of 17,000 detailed, structurally
rich, and realistic residential floor plans, created to advance spatial AI
research. Each plan includes precise annotations of architectural elements
(walls, doors, windows, balconies) and functional spaces (such as kitchens,
bedrooms, and bathrooms). ResPlan addresses key limitations of existing
datasets such as RPLAN (Wu et al., 2019) and MSD (van Engelenburg et al., 2024)
by offering enhanced visual fidelity and greater structural diversity,
reflecting realistic and non-idealized residential layouts. Designed as a
versatile, general-purpose resource, ResPlan supports a wide range of
applications including robotics, reinforcement learning, generative AI, virtual
and augmented reality, simulations, and game development. Plans are provided in
both geometric and graph-based formats, enabling direct integration into
simulation engines and fast 3D conversion. A key contribution is an open-source
pipeline for geometry cleaning, alignment, and annotation refinement.
Additionally, ResPlan includes structured representations of room connectivity,
supporting graph-based spatial reasoning tasks. Finally, we present comparative
analyses with existing benchmarks and outline several open benchmark tasks
enabled by ResPlan. Ultimately, ResPlan offers a significant advance in scale,
realism, and usability, providing a robust foundation for developing and
benchmarking next-generation spatial intelligence systems.

</details>


### [78] [Online 3D Gaussian Splatting Modeling with Novel View Selection](https://arxiv.org/abs/2508.14014)
*Byeonggwon Lee,Junkyu Park,Khang Truong Giang,Soohwan Song*

Main category: cs.CV

TL;DR: 提出了一种通过自适应视图选择改进3D高斯泼溅模型完整性的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖关键帧，导致场景重建不完整，且在线处理限制了多帧或大量训练迭代的使用。

Method: 通过在线分析重建质量，选择最佳非关键帧进行额外训练，结合关键帧和非关键帧优化不完整区域。

Result: 实验结果表明，该方法在复杂户外场景中优于现有技术。

Conclusion: 该方法显著提升了3D高斯泼溅模型的完整性和性能。

Abstract: This study addresses the challenge of generating online 3D Gaussian Splatting
(3DGS) models from RGB-only frames. Previous studies have employed dense SLAM
techniques to estimate 3D scenes from keyframes for 3DGS model construction.
However, these methods are limited by their reliance solely on keyframes, which
are insufficient to capture an entire scene, resulting in incomplete
reconstructions. Moreover, building a generalizable model requires
incorporating frames from diverse viewpoints to achieve broader scene coverage.
However, online processing restricts the use of many frames or extensive
training iterations. Therefore, we propose a novel method for high-quality 3DGS
modeling that improves model completeness through adaptive view selection. By
analyzing reconstruction quality online, our approach selects optimal
non-keyframes for additional training. By integrating both keyframes and
selected non-keyframes, the method refines incomplete regions from diverse
viewpoints, significantly enhancing completeness. We also present a framework
that incorporates an online multi-view stereo approach, ensuring consistency in
3D information throughout the 3DGS modeling process. Experimental results
demonstrate that our method outperforms state-of-the-art methods, delivering
exceptional performance in complex outdoor scenes.

</details>


### [79] [Backdooring Self-Supervised Contrastive Learning by Noisy Alignment](https://arxiv.org/abs/2508.14015)
*Tuo Chen,Jie Gui,Minjing Dong,Ju Jia,Lanting Fang,Jian Liu*

Main category: cs.CV

TL;DR: 论文提出了一种名为Noisy Alignment（NA）的方法，通过显式抑制有毒图像中的噪声成分，有效提升了数据投毒后门攻击（DPCLs）的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的DPCLs方法因依赖脆弱的隐式共现关系以及对有毒图像中判别性特征抑制不足，效果有限。

Method: NA通过策略性地操纵对比学习的随机裁剪机制，将其建模为图像布局优化问题，并理论推导最优参数。

Result: NA在保持干净数据准确性的同时，实现了优于现有DPCLs的性能，并对常见后门防御具有鲁棒性。

Conclusion: Noisy Alignment是一种简单有效的方法，显著提升了DPCLs的攻击效果。

Abstract: Self-supervised contrastive learning (CL) effectively learns transferable
representations from unlabeled data containing images or image-text pairs but
suffers vulnerability to data poisoning backdoor attacks (DPCLs). An adversary
can inject poisoned images into pretraining datasets, causing compromised CL
encoders to exhibit targeted misbehavior in downstream tasks. Existing DPCLs,
however, achieve limited efficacy due to their dependence on fragile implicit
co-occurrence between backdoor and target object and inadequate suppression of
discriminative features in backdoored images. We propose Noisy Alignment (NA),
a DPCL method that explicitly suppresses noise components in poisoned images.
Inspired by powerful training-controllable CL attacks, we identify and extract
the critical objective of noisy alignment, adapting it effectively into
data-poisoning scenarios. Our method implements noisy alignment by
strategically manipulating contrastive learning's random cropping mechanism,
formulating this process as an image layout optimization problem with
theoretically derived optimal parameters. The resulting method is simple yet
effective, achieving state-of-the-art performance compared to existing DPCLs,
while maintaining clean-data accuracy. Furthermore, Noisy Alignment
demonstrates robustness against common backdoor defenses. Codes can be found at
https://github.com/jsrdcht/Noisy-Alignment.

</details>


### [80] [InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing](https://arxiv.org/abs/2508.14033)
*Shaoshu Yang,Zhe Kong,Feng Gao,Meng Cheng,Xiangyu Liu,Yong Zhang,Zhuoliang Kang,Wenhan Luo,Xunliang Cai,Ran He,Xiaoming Wei*

Main category: cs.CV

TL;DR: 提出了一种稀疏帧视频配音新范式InfiniteTalk，解决了传统方法仅编辑嘴部区域导致的不协调问题，实现了全身体运动同步。


<details>
  <summary>Details</summary>
Motivation: 传统视频配音技术局限于嘴部区域编辑，导致面部表情和身体动作不协调，影响观看体验。

Method: 提出稀疏帧视频配音范式，保留关键帧以维持身份和动作，设计InfiniteTalk架构实现长序列配音。

Result: 在HDTF、CelebV-HQ和EMTD数据集上表现优异，视觉真实性和动作同步性领先。

Conclusion: InfiniteTalk通过自适应条件化和优化控制强度，实现了高质量的全身体运动同步配音。

Abstract: Recent breakthroughs in video AIGC have ushered in a transformative era for
audio-driven human animation. However, conventional video dubbing techniques
remain constrained to mouth region editing, resulting in discordant facial
expressions and body gestures that compromise viewer immersion. To overcome
this limitation, we introduce sparse-frame video dubbing, a novel paradigm that
strategically preserves reference keyframes to maintain identity, iconic
gestures, and camera trajectories while enabling holistic, audio-synchronized
full-body motion editing. Through critical analysis, we identify why naive
image-to-video models fail in this task, particularly their inability to
achieve adaptive conditioning. Addressing this, we propose InfiniteTalk, a
streaming audio-driven generator designed for infinite-length long sequence
dubbing. This architecture leverages temporal context frames for seamless
inter-chunk transitions and incorporates a simple yet effective sampling
strategy that optimizes control strength via fine-grained reference frame
positioning. Comprehensive evaluations on HDTF, CelebV-HQ, and EMTD datasets
demonstrate state-of-the-art performance. Quantitative metrics confirm superior
visual realism, emotional coherence, and full-body motion synchronization.

</details>


### [81] [Distilled-3DGS:Distilled 3D Gaussian Splatting](https://arxiv.org/abs/2508.14037)
*Lintao Xiang,Xinkai Chen,Jianhuang Lai,Guangcong Wang*

Main category: cs.CV

TL;DR: 提出了一种针对3D高斯泼溅（3DGS）的知识蒸馏框架，通过聚合多种教师模型的输出来优化轻量级学生模型，提升了渲染质量和存储效率。


<details>
  <summary>Details</summary>
Motivation: 3DGS在高保真渲染时需要大量3D高斯分布，导致高内存和存储消耗，因此需要一种轻量化方法。

Method: 提出知识蒸馏框架，使用多种教师模型（包括原始3DGS、噪声增强和dropout正则化版本）指导学生模型优化，并引入结构相似性损失以保持几何一致性。

Result: 在多个数据集上的实验表明，Distilled-3DGS在渲染质量和存储效率上优于现有方法。

Conclusion: Distilled-3DGS是一种简单有效的框架，显著提升了3DGS的轻量化性能。

Abstract: 3D Gaussian Splatting (3DGS) has exhibited remarkable efficacy in novel view
synthesis (NVS). However, it suffers from a significant drawback: achieving
high-fidelity rendering typically necessitates a large number of 3D Gaussians,
resulting in substantial memory consumption and storage requirements. To
address this challenge, we propose the first knowledge distillation framework
for 3DGS, featuring various teacher models, including vanilla 3DGS,
noise-augmented variants, and dropout-regularized versions. The outputs of
these teachers are aggregated to guide the optimization of a lightweight
student model. To distill the hidden geometric structure, we propose a
structural similarity loss to boost the consistency of spatial geometric
distributions between the student and teacher model. Through comprehensive
quantitative and qualitative evaluations across diverse datasets, the proposed
Distilled-3DGS, a simple yet effective framework without bells and whistles,
achieves promising rendering results in both rendering quality and storage
efficiency compared to state-of-the-art methods. Project page:
https://distilled3dgs.github.io . Code:
https://github.com/lt-xiang/Distilled-3DGS .

</details>


### [82] [Beyond Simple Edits: Composed Video Retrieval with Dense Modifications](https://arxiv.org/abs/2508.14039)
*Omkar Thawakar,Dmitry Demidov,Ritesh Thawkar,Rao Muhammad Anwer,Mubarak Shah,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: 论文提出了一个名为Dense-WebVid-CoVR的新数据集和一个基于交叉注意力的模型，用于解决细粒度组合视频检索任务，并在所有指标上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有检索框架难以处理细粒度组合查询和时间变化的复杂性，限制了其在细粒度设置中的检索能力。

Method: 通过交叉注意力融合视觉和文本信息，使用基于文本编码器的对齐方法，实现查询修改与目标视频的精确匹配。

Result: 模型在视觉+文本设置中达到了71.3%的Recall@1，比现有最佳方法高出3.4%。

Conclusion: 提出的数据集和模型在利用详细视频描述和密集修改文本方面表现出色，显著提升了组合视频检索的性能。

Abstract: Composed video retrieval is a challenging task that strives to retrieve a
target video based on a query video and a textual description detailing
specific modifications. Standard retrieval frameworks typically struggle to
handle the complexity of fine-grained compositional queries and variations in
temporal understanding limiting their retrieval ability in the fine-grained
setting. To address this issue, we introduce a novel dataset that captures both
fine-grained and composed actions across diverse video segments, enabling more
detailed compositional changes in retrieved video content. The proposed
dataset, named Dense-WebVid-CoVR, consists of 1.6 million samples with dense
modification text that is around seven times more than its existing
counterpart. We further develop a new model that integrates visual and textual
information through Cross-Attention (CA) fusion using grounded text encoder,
enabling precise alignment between dense query modifications and target videos.
The proposed model achieves state-of-the-art results surpassing existing
methods on all metrics. Notably, it achieves 71.3\% Recall@1 in visual+text
setting and outperforms the state-of-the-art by 3.4\%, highlighting its
efficacy in terms of leveraging detailed video descriptions and dense
modification texts. Our proposed dataset, code, and model are available at
:https://github.com/OmkarThawakar/BSE-CoVR

</details>


### [83] [LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos](https://arxiv.org/abs/2508.14041)
*Chin-Yang Lin,Cheng Sun,Fu-En Yang,Min-Hung Chen,Yen-Yu Lin,Yu-Lun Liu*

Main category: cs.CV

TL;DR: LongSplat是一个用于从长视频中合成新视角的框架，解决了相机姿态漂移、几何初始化不准确和内存限制等问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在长视频新视角合成中面临的相机姿态漂移、几何初始化不准确和内存限制等挑战。

Method: 提出了增量联合优化、鲁棒的姿态估计模块和高效的八叉树锚点形成机制。

Result: 在多个基准测试中取得了最先进的结果，显著提升了渲染质量、姿态准确性和计算效率。

Conclusion: LongSplat通过创新的方法解决了长视频新视角合成的关键问题，性能优于现有方法。

Abstract: LongSplat addresses critical challenges in novel view synthesis (NVS) from
casually captured long videos characterized by irregular camera motion, unknown
camera poses, and expansive scenes. Current methods often suffer from pose
drift, inaccurate geometry initialization, and severe memory limitations. To
address these issues, we introduce LongSplat, a robust unposed 3D Gaussian
Splatting framework featuring: (1) Incremental Joint Optimization that
concurrently optimizes camera poses and 3D Gaussians to avoid local minima and
ensure global consistency; (2) a robust Pose Estimation Module leveraging
learned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that
converts dense point clouds into anchors based on spatial density. Extensive
experiments on challenging benchmarks demonstrate that LongSplat achieves
state-of-the-art results, substantially improving rendering quality, pose
accuracy, and computational efficiency compared to prior approaches. Project
page: https://linjohnss.github.io/longsplat/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [84] [Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL](https://arxiv.org/abs/2508.13167)
*Weizhen Li,Jianbo Lin,Zhuosong Jiang,Jingyi Cao,Xinpeng Liu,Jiayu Zhang,Zhenqiang Huang,Qianben Chen,Weichen Sun,Qiexiang Wang,Hongxuan Lu,Tianrui Qin,Chenghao Zhu,Yi Yao,Shuying Fan,Xiaowan Li,Tiannan Wang,Pai Liu,King Zhu,He Zhu,Dingfeng Shi,Piaohong Wang,Yeyi Guan,Xiangru Tang,Minghao Liu,Yuchen Eleanor Jiang,Jian Yang,Jiaheng Liu,Ge Zhang,Wangchunshu Zhou*

Main category: cs.AI

TL;DR: 提出了一种名为Chain-of-Agents（CoA）的新范式，通过多智能体蒸馏框架和强化学习，实现了端到端的复杂问题解决能力，并开发了Agent Foundation Models（AFMs）。


<details>
  <summary>Details</summary>
Motivation: 现有基于手动提示/工作流工程的多智能体系统计算效率低、能力有限且无法从数据为中心的学习中受益。

Method: 通过多智能体蒸馏框架将先进的多智能体系统蒸馏为CoA轨迹进行监督微调，再通过强化学习提升模型能力。

Result: AFM在多个基准测试中取得了最先进的性能。

Conclusion: CoA和AFM为未来智能体模型和智能体强化学习研究提供了坚实基础。

Abstract: Recent advances in large language models (LLMs) and multi-agent systems have
demonstrated remarkable capabilities in complex problem-solving tasks such as
deep research, vibe coding, and mathematical reasoning. However, most existing
multi-agent systems are built upon manual prompt/workflow engineering with
sophisticated agent frameworks, making them computationally inefficient, less
capable, and can not benefit from data-centric learning. In this work, we
introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables
native end-to-end complex problem-solving in the same way as a multi-agent
system (i.e., multi-turn problem solving with multiple tools and multiple
agents) within one model. In chain-of-agents problem-solving, the model
dynamically activates different tool agents and role-playing agents to simulate
multi-agent collaboration in an end-to-end fashion. To elicit end-to-end
chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent
distillation framework to distill state-of-the-art multi-agent systems into
chain-of-agents trajectories for agentic supervised fine-tuning. We then use
agentic reinforcement learning on verifiable agentic tasks to further improve
the models' capabilities on chain-of-agents problem solving. We call the
resulting models Agent Foundation Models (AFMs). Our empirical studies
demonstrate that AFM establishes new state-of-the-art performance across
diverse benchmarks in both web agent and code agent settings. We make the
entire research, including the model weights, code for training and evaluation,
and the training data, fully open-sourced, which offers a solid starting point
for future research on agent models and agentic RL.

</details>


### [85] [Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context](https://arxiv.org/abs/2508.13171)
*Tao An*

Main category: cs.AI

TL;DR: Cognitive Workspace 是一种超越传统 RAG 的新范式，通过模拟人类外部记忆的认知机制，解决了 LLMs 在上下文管理中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前 LLMs 的上下文管理存在根本性限制，即使扩展上下文窗口也无法解决被动检索系统缺乏动态任务驱动的问题。

Method: 提出 Cognitive Workspace，包含主动记忆管理、分层认知缓冲区和任务驱动的上下文优化三大创新。

Result: 实验表明，Cognitive Workspace 实现了 58.6% 的平均记忆重用率，显著优于传统 RAG，且效率提升 17-18%。

Conclusion: Cognitive Workspace 标志着从信息检索到真正认知增强的根本转变，为 LLM 系统提供了定量证据。

Abstract: Large Language Models (LLMs) face fundamental limitations in context
management despite recent advances extending context windows to millions of
tokens. We propose Cognitive Workspace, a novel paradigm that transcends
traditional Retrieval-Augmented Generation (RAG) by emulating human cognitive
mechanisms of external memory use. Drawing from cognitive science foundations
including Baddeley's working memory model, Clark's extended mind thesis, and
Hutchins' distributed cognition framework, we demonstrate that current passive
retrieval systems fail to capture the dynamic, task-driven nature of human
memory management. Our analysis of 2024-2025 developments reveals that while
techniques like Infini-attention and StreamingLLM achieve impressive context
lengths, they lack the metacognitive awareness and active planning capabilities
essential for true cognitive extension. Cognitive Workspace addresses these
limitations through three core innovations: (1) active memory management with
deliberate information curation, (2) hierarchical cognitive buffers enabling
persistent working states, and (3) task-driven context optimization that
dynamically adapts to cognitive demands. Empirical validation demonstrates
Cognitive Workspace achieves an average 58.6% memory reuse rate (ranging from
54-60% across different tasks) compared to 0% for traditional RAG, with 17-18%
net efficiency gain despite 3.3x higher operation counts. Statistical analysis
confirms these advantages with p < 0.001 and Cohen's d > 23 across multiple
task types, establishing the first quantitative evidence for active memory
superiority in LLM systems. We present a comprehensive theoretical framework
synthesizing insights from 50+ recent papers, positioning Cognitive Workspace
as a fundamental shift from information retrieval to genuine cognitive
augmentation.

</details>


### [86] [AlphaEval: A Comprehensive and Efficient Evaluation Framework for Formula Alpha Mining](https://arxiv.org/abs/2508.13174)
*Hongjun Ding,Binqi Chen,Jinsheng Huang,Taian Guo,Zhengyang Mao,Guoyi Shao,Lutong Zou,Luchen Liu,Ming Zhang*

Main category: cs.AI

TL;DR: AlphaEval是一个统一、可并行且无需回测的评估框架，用于自动化alpha挖掘模型，解决了现有评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（如回测和基于相关性的指标）存在计算密集、忽略关键属性（如稳定性、鲁棒性、多样性）以及闭源问题，阻碍了alpha挖掘领域的进展。

Method: 提出AlphaEval框架，从预测能力、稳定性、鲁棒性、金融逻辑和多样性五个维度全面评估alpha质量。

Result: 实验表明，AlphaEval在评估一致性上与回测相当，但更高效且提供更全面的洞察，并能识别优于传统方法的alpha。

Conclusion: AlphaEval为alpha挖掘提供了更高效、全面的评估工具，并通过开源促进可重复性和社区参与。

Abstract: Formula alpha mining, which generates predictive signals from financial data,
is critical for quantitative investment. Although various algorithmic
approaches-such as genetic programming, reinforcement learning, and large
language models-have significantly expanded the capacity for alpha discovery,
systematic evaluation remains a key challenge. Existing evaluation metrics
predominantly include backtesting and correlation-based measures. Backtesting
is computationally intensive, inherently sequential, and sensitive to specific
strategy parameters. Correlation-based metrics, though efficient, assess only
predictive ability and overlook other crucial properties such as temporal
stability, robustness, diversity, and interpretability. Additionally, the
closed-source nature of most existing alpha mining models hinders
reproducibility and slows progress in this field. To address these issues, we
propose AlphaEval, a unified, parallelizable, and backtest-free evaluation
framework for automated alpha mining models. AlphaEval assesses the overall
quality of generated alphas along five complementary dimensions: predictive
power, stability, robustness to market perturbations, financial logic, and
diversity. Extensive experiments across representative alpha mining algorithms
demonstrate that AlphaEval achieves evaluation consistency comparable to
comprehensive backtesting, while providing more comprehensive insights and
higher efficiency. Furthermore, AlphaEval effectively identifies superior
alphas compared to traditional single-metric screening approaches. All
implementations and evaluation tools are open-sourced to promote
reproducibility and community engagement.

</details>


### [87] [Fitting Ontologies and Constraints to Relational Structures](https://arxiv.org/abs/2508.13176)
*Simon Hosemann,Jean Christoph Jung,Carsten Lutz,Sebastian Rudolph*

Main category: cs.AI

TL;DR: 研究了如何根据正负示例拟合本体和约束，分析了多种描述逻辑和TGDs的计算复杂性、算法设计和拟合大小，并探讨了有限基的存在性。


<details>
  <summary>Details</summary>
Motivation: 研究如何根据给定的正负示例拟合本体和约束，以支持知识表示和推理。

Method: 使用描述逻辑$\mathcal{E\mkern-2mu L}$和$\mathcal{E\mkern-2mu LI}$以及多种TGDs（如全、守卫、边界守卫等）作为语言，分析其计算复杂性和算法设计。

Result: 确定了拟合问题的计算复杂性，设计了算法，并分析了拟合本体和TGDs的大小。发现$\mathcal{E\mkern-2mu L}$、$\mathcal{E\mkern-2mu LI}$等存在有限基，而其他TGDs则不一定。

Conclusion: 研究为知识表示和推理提供了理论基础，揭示了不同语言在拟合问题上的表现差异。

Abstract: We study the problem of fitting ontologies and constraints to positive and
negative examples that take the form of a finite relational structure. As
ontology and constraint languages, we consider the description logics
$\mathcal{E\mkern-2mu L}$ and $\mathcal{E\mkern-2mu LI}$ as well as several
classes of tuple-generating dependencies (TGDs): full, guarded,
frontier-guarded, frontier-one, and unrestricted TGDs as well as inclusion
dependencies. We pinpoint the exact computational complexity, design
algorithms, and analyze the size of fitting ontologies and TGDs. We also
investigate the related problem of constructing a finite basis of concept
inclusions / TGDs for a given set of finite structures. While finite bases
exist for $\mathcal{E\mkern-2mu L}$, $\mathcal{E\mkern-2mu LI}$, guarded TGDs,
and inclusion dependencies, they in general do not exist for full,
frontier-guarded and frontier-one TGDs.

</details>


### [88] [A Hardware-oriented Approach for Efficient Active Inference Computation and Deployment](https://arxiv.org/abs/2508.13177)
*Nikola Pižurica,Nikola Milović,Igor Jovančević,Conor Heins,Miguel de Prado*

Main category: cs.AI

TL;DR: 提出了一种结合pymdp灵活性和高效性的方法，通过统一稀疏计算图优化硬件执行效率，显著降低延迟和内存占用。


<details>
  <summary>Details</summary>
Motivation: Active Inference (AIF) 在决策制定中表现优异，但其计算和内存需求限制了在资源受限环境中的部署。

Method: 整合pymdp的灵活性和高效性，设计统一稀疏计算图以优化硬件执行效率。

Result: 延迟降低超过2倍，内存占用减少高达35%。

Conclusion: 该方法推动了高效AIF代理在实时和嵌入式应用中的部署。

Abstract: Active Inference (AIF) offers a robust framework for decision-making, yet its
computational and memory demands pose challenges for deployment, especially in
resource-constrained environments. This work presents a methodology that
facilitates AIF's deployment by integrating pymdp's flexibility and efficiency
with a unified, sparse, computational graph tailored for hardware-efficient
execution. Our approach reduces latency by over 2x and memory by up to 35%,
advancing the deployment of efficient AIF agents for real-time and embedded
applications.

</details>


### [89] [The Interpretability Analysis of the Model Can Bring Improvements to the Text-to-SQL Task](https://arxiv.org/abs/2508.13178)
*Cong Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种结合模型可解释性分析和执行引导策略的CESQL模型，用于提升文本到SQL转换的准确性和泛化能力，特别是在WHERE子句的语义解析中。


<details>
  <summary>Details</summary>
Motivation: 提升文本到SQL模型在真实应用中的基础能力和泛化能力，减少对条件列数据和人工标注训练数据的依赖。

Method: 集成模型可解释性分析和执行引导策略，结合过滤调整、逻辑关联优化和模型融合，设计CESQL模型。

Result: 在WikiSQL数据集上表现优异，显著提高了预测准确性。

Conclusion: 该研究为处理复杂查询和真实数据库环境中的不规则数据提供了新思路。

Abstract: To elevate the foundational capabilities and generalization prowess of the
text-to-SQL model in real-world applications, we integrate model
interpretability analysis with execution-guided strategy for semantic parsing
of WHERE clauses in SQL queries. Furthermore, we augment this approach with
filtering adjustments, logical correlation refinements, and model fusion,
culminating in the design of the CESQL model that facilitates conditional
enhancement. Our model excels on the WikiSQL dataset, which is emblematic of
single-table database query tasks, markedly boosting the accuracy of prediction
outcomes. When predicting conditional values in WHERE clauses, we have not only
minimized our dependence on data within the condition columns of tables but
also circumvented the impact of manually labeled training data. Our hope is
that this endeavor to enhance accuracy in processing basic database queries
will offer fresh perspectives for research into handling complex queries and
scenarios featuring irregular data in real-world database environments.

</details>


### [90] [Search-Time Data Contamination](https://arxiv.org/abs/2508.13180)
*Ziwen Han,Meher Mankikar,Julian Michael,Zifan Wang*

Main category: cs.AI

TL;DR: 论文提出了一种新的数据污染形式——搜索时间污染（STC），影响基于搜索的LLM代理的评估，并提出了应对措施。


<details>
  <summary>Details</summary>
Motivation: 研究动机是识别并解决评估搜索型LLM代理时出现的数据污染问题，尤其是当测试问题及其答案在检索过程中被直接发现时，导致代理通过复制而非推理来回答问题，从而影响评估的可靠性。

Method: 方法包括分析搜索代理的日志，发现HuggingFace平台上的评估数据集被检索到，并通过实验验证STC的影响。此外，还进行了消融实验以探索其他可能的污染源。

Result: 结果显示，约3%的问题中，搜索代理直接从HuggingFace获取了测试数据集及其标签。当HuggingFace被屏蔽后，受污染子集的准确率下降了约15%。

Conclusion: 结论是提出了基准设计和结果报告的最佳实践，以应对STC问题，并公开了实验日志以促进评估结果的审计。

Abstract: Data contamination refers to the leakage of evaluation data into model
training data, resulting in overfitting to supposedly held-out test sets and
compromising test validity. We identify an analogous issue, search-time
contamination (STC), in evaluating search-based LLM agents which use tools to
gather information from online sources when answering user queries. STC occurs
when the retrieval step surfaces a source containing the test question (or a
near-duplicate) alongside its answer, enabling agents to copy rather than
genuinely infer or reason, undermining benchmark integrity. We find that
HuggingFace, an online platform hosting evaluation datasets, appears among
retrieved sources in search based agent logs. Consequently, agents often
explicitly acknowledge discovering question answer pairs from HuggingFace
within their reasoning chains. On three commonly used capability benchmarks:
Humanity's Last Exam (HLE), SimpleQA, and GPQA, we demonstrate that for
approximately 3% of questions, search-based agents directly find the datasets
with ground truth labels on HuggingFace. When millions of evaluation queries
target the same benchmark, even small, repeated leaks can accelerate the
benchmark's obsolescence, shortening its intended lifecycle. After HuggingFace
is blocked, we observe a drop in accuracy on the contaminated subset of
approximately 15%. We further show through ablation experiments that publicly
accessible evaluation datasets on HuggingFace may not be the sole source of
STC. To this end, we conclude by proposing best practices for benchmark design
and result reporting to address this novel form of leakage and ensure
trustworthy evaluation of search-based LLM agents. To facilitate the auditing
of evaluation results, we also publicly release the complete logs from our
experiments.

</details>


### [91] [QuickMerge++: Fast Token Merging with Autoregressive Prior](https://arxiv.org/abs/2508.13204)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: QuickMerge是一种轻量级令牌合并框架，通过动态选择令牌和熵预算估计器，提高生成效率。


<details>
  <summary>Details</summary>
Motivation: 生成模型中令牌级计算成本成为瓶颈，现有令牌选择方法多为静态或模态特定，且不兼容自回归生成。

Method: 提出QuickMerge框架，动态选择令牌，基于注意力范数幅度和熵预算估计器，并引入轻量级Transformer先验。

Result: 在多模态领域评估中，QuickMerge显著减少令牌数量，性能优于学习令牌器和固定补丁基线。

Conclusion: QuickMerge通过语义显著性估计、灵活令牌预算和自回归对齐，实现了高效准确的生成。

Abstract: As generative models scale to larger inputs across language, vision, and
video domains, the cost of token-level computation has become a key bottleneck.
While prior work suggests that only a subset of tokens significantly influence
downstream predictions, most token selection methods are static,
modality-specific, or incompatible with autoregressive generation. In this
paper, we propose QuickMerge, a lightweight token merging framework designed
for efficient next-token prediction.
  QuickMerge dynamically selects a reduced number of tokens based on attention
norm magnitude, guided by an entropy-based budget estimator. To preserve
autoregressive compatibility, we introduce a lightweight transformer prior
trained over the merged token sequence. By combining semantic salience
estimation, flexible token budgets, and AR alignment, QuickMerge enables
accurate generation with fewer tokens.
  We evaluate QuickMerge across multi-modality domains, demonstrating
consistent improvements in compute-accuracy tradeoffs. Specifically, QuickMerge
reduces token counts sustantially while matching as well as exceeding the
performance of learned tokenizers and fixed-patch baselines.

</details>


### [92] [AI sustains higher strategic tension than humans in chess](https://arxiv.org/abs/2508.13213)
*Adamo Cerioli,Edward D. Lee,Vito D. P. Servedio*

Main category: cs.AI

TL;DR: 论文通过棋类游戏研究战略决策中的短期机会与长期目标的权衡，提出了一种基于网络的棋子互动度量方法，发现AI比人类更能维持高水平的战略紧张。


<details>
  <summary>Details</summary>
Motivation: 研究战略决策中短期机会与长期目标的权衡，比较人类与AI在棋类游戏中的动态差异。

Method: 提出基于网络的棋子互动度量方法，量化棋盘上的战略紧张程度，并分析其在人类与AI对局中的演化。

Result: AI玩家比精英人类玩家更能长时间维持高水平的战略紧张；人类玩家的战略紧张随技能水平（Elo评分）呈阶梯式增长。

Conclusion: AI与人类在战略决策上存在显著差异，AI更擅长处理复杂、平衡的战术局面，这可能对AI在复杂战略环境中的应用有启示。

Abstract: Strategic decision-making involves managing the tension between immediate
opportunities and long-term objectives. We study this trade-off in chess by
characterizing and comparing dynamics between human vs human and AI vs AI
games. We propose a network-based metric of piece-to-piece interaction to
quantify the ongoing strategic tension on the board. Its evolution in games
reveals that the most competitive AI players sustain higher levels of strategic
tension for longer durations than elite human players. Cumulative tension
varies with algorithmic complexity for AI and correspondingly in human-played
games increases abruptly with expertise at about 1600 Elo and again at 2300
Elo. The profiles reveal different approaches. Highly competitive AI tolerates
interconnected positions balanced between offensive and defensive tactics over
long periods. Human play, in contrast, limits tension and game complexity,
which may reflect cognitive limitations and adaptive strategies. The difference
may have implications for AI usage in complex, strategic environments.

</details>


### [93] [Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information](https://arxiv.org/abs/2508.13250)
*Zeyu Zhang,Yang Zhang,Haoran Tan,Rui Li,Xu Chen*

Main category: cs.AI

TL;DR: 论文提出了一种多跳个性化推理任务，探索不同记忆机制在个性化信息多跳推理中的表现，并提出了HybridMem方法以解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有记忆方法在处理复杂任务时难以进行多跳推理，限制了用户个性化信息的有效利用。

Method: 定义了多跳个性化推理任务，构建了数据集和统一评估框架，实现了多种显式和隐式记忆方法，并提出了HybridMem混合方法。

Result: 通过实验验证了HybridMem方法的有效性，并分析了不同记忆方法的优缺点。

Conclusion: HybridMem方法在多跳个性化推理任务中表现优异，为研究社区提供了新的解决方案和数据集。

Abstract: In large language model-based agents, memory serves as a critical capability
for achieving personalization by storing and utilizing users' information.
Although some previous studies have adopted memory to implement user
personalization, they typically focus on preference alignment and simple
question-answering. However, in the real world, complex tasks often require
multi-hop reasoning on a large amount of user information, which poses
significant challenges for current memory approaches. To address this
limitation, we propose the multi-hop personalized reasoning task to explore how
different memory mechanisms perform in multi-hop reasoning over personalized
information. We explicitly define this task and construct a dataset along with
a unified evaluation framework. Then, we implement various explicit and
implicit memory methods and conduct comprehensive experiments. We evaluate
their performance on this task from multiple perspectives and analyze their
strengths and weaknesses. Besides, we explore hybrid approaches that combine
both paradigms and propose the HybridMem method to address their limitations.
We demonstrate the effectiveness of our proposed model through extensive
experiments. To benefit the research community, we release this project at
https://github.com/nuster1128/MPR.

</details>


### [94] ["DIVE" into Hydrogen Storage Materials Discovery with AI Agents](https://arxiv.org/abs/2508.13251)
*Di Zhang,Xue Jia,Tran Ba Hung,Seong Hoon Jang,Linda Zhang,Ryuhei Sato,Yusuke Hashimoto,Toyoto Sato,Kiyoe Konno,Shin-ichi Orimo,Hao Li*

Main category: cs.AI

TL;DR: DIVE多智能体工作流通过系统化读取和组织科学文献中的图形数据，显著提高了数据提取的准确性和覆盖范围，为AI驱动的材料发现提供了新范式。


<details>
  <summary>Details</summary>
Motivation: 科学文献中的大量材料数据被困在非结构化的图表中，阻碍了基于LLM的AI代理在自动化材料设计中的应用。

Method: 提出DIVE多智能体工作流，系统化读取和组织科学文献中的图形数据，专注于固态储氢材料。

Result: DIVE显著提高了数据提取的准确性和覆盖范围，比商业模型提升10-15%，比开源模型提升30%以上。基于4,000篇文献的30,000条数据，建立了快速逆向设计工作流。

Conclusion: DIVE工作流和代理设计可广泛应用于多种材料，为AI驱动的材料发现提供了新范式。

Abstract: Data-driven artificial intelligence (AI) approaches are fundamentally
transforming the discovery of new materials. Despite the unprecedented
availability of materials data in the scientific literature, much of this
information remains trapped in unstructured figures and tables, hindering the
construction of large language model (LLM)-based AI agent for automated
materials design. Here, we present the Descriptive Interpretation of Visual
Expression (DIVE) multi-agent workflow, which systematically reads and
organizes experimental data from graphical elements in scientific literatures.
We focus on solid-state hydrogen storage materials-a class of materials central
to future clean-energy technologies and demonstrate that DIVE markedly improves
the accuracy and coverage of data extraction compared to the direct extraction
by multimodal models, with gains of 10-15% over commercial models and over 30%
relative to open-source models. Building on a curated database of over 30,000
entries from 4,000 publications, we establish a rapid inverse design workflow
capable of identifying previously unreported hydrogen storage compositions in
two minutes. The proposed AI workflow and agent design are broadly transferable
across diverse materials, providing a paradigm for AI-driven materials
discovery.

</details>


### [95] [CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support](https://arxiv.org/abs/2508.13256)
*Yuting Zhang,Karina V. Bunting,Asgher Champsi,Xiaoxia Wang,Wenqi Lu,Alexander Thorley,Sandeep S Hothi,Zhaowen Qiu,Dipak Kotecha,Jinming Duan*

Main category: cs.AI

TL;DR: CardAIc-Agents框架通过多模态和自适应工具支持，提升心血管疾病早期检测的效率和个性化。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，医疗资源不足，AI应用受限。

Method: 提出CardAIc-Agents框架，结合外部工具和自适应策略，支持动态任务执行。

Result: 实验显示CardAIc-Agents优于主流视觉语言模型和现有代理系统。

Conclusion: 该框架为心血管疾病AI应用提供了高效、个性化的解决方案。

Abstract: Cardiovascular diseases (CVDs) remain the foremost cause of mortality
worldwide, a burden worsened by a severe deficit of healthcare workers.
Artificial intelligence (AI) agents have shown potential to alleviate this gap
via automated early detection and proactive screening, yet their clinical
application remains limited by: 1) prompt-based clinical role assignment that
relies on intrinsic model capabilities without domain-specific tool support; or
2) rigid sequential workflows, whereas clinical care often requires adaptive
reasoning that orders specific tests and, based on their results, guides
personalised next steps; 3) general and static knowledge bases without
continuous learning capability; and 4) fixed unimodal or bimodal inputs and
lack of on-demand visual outputs when further clarification is needed. In
response, a multimodal framework, CardAIc-Agents, was proposed to augment
models with external tools and adaptively support diverse cardiac tasks.
Specifically, a CardiacRAG agent generated general plans from updatable cardiac
knowledge, while the chief agent integrated tools to autonomously execute these
plans and deliver decisions. To enable adaptive and case-specific
customization, a stepwise update strategy was proposed to dynamically refine
plans based on preceding execution results, once the task was assessed as
complex. In addition, a multidisciplinary discussion tool was introduced to
interpret challenging cases, thereby supporting further adaptation. When
clinicians raised concerns, visual review panels were provided to assist final
validation. Experiments across three datasets showed the efficiency of
CardAIc-Agents compared to mainstream Vision-Language Models (VLMs),
state-of-the-art agentic systems, and fine-tuned VLMs.

</details>


### [96] [Towards Unified Multimodal Financial Forecasting: Integrating Sentiment Embeddings and Market Indicators via Cross-Modal Attention](https://arxiv.org/abs/2508.13327)
*Sarthak Khanna,Armin Berger,David Berghaus,Tobias Deusser,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.AI

TL;DR: STONK是一种多模态框架，结合数值市场指标和情感增强的新闻嵌入，用于改进股票走势预测。


<details>
  <summary>Details</summary>
Motivation: 解决孤立分析的局限性，通过结合数值和文本数据提升预测准确性。

Method: 通过特征拼接和跨模态注意力机制整合数值与文本嵌入。

Result: 回测显示STONK优于仅使用数值的基线模型。

Conclusion: 该框架为可扩展的多模态金融预测提供了基于证据的指导。

Abstract: We propose STONK (Stock Optimization using News Knowledge), a multimodal
framework integrating numerical market indicators with sentiment-enriched news
embeddings to improve daily stock-movement prediction. By combining numerical &
textual embeddings via feature concatenation and cross-modal attention, our
unified pipeline addresses limitations of isolated analyses. Backtesting shows
STONK outperforms numeric-only baselines. A comprehensive evaluation of fusion
strategies and model configurations offers evidence-based guidance for scalable
multimodal financial forecasting. Source code is available on GitHub

</details>


### [97] [HiFo-Prompt: Prompting with Hindsight and Foresight for LLM-based Automatic Heuristic Design](https://arxiv.org/abs/2508.13333)
*Chentong Chen,Mengyuan Zhong,Jianyong Sun,Ye Fan,Jialong Shi*

Main category: cs.AI

TL;DR: HiFo-Prompt框架通过前瞻和后瞻提示策略提升LLM在进化计算中的启发式设计能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based AHD方法因静态操作符和缺乏知识积累机制而效果受限。

Method: HiFo-Prompt结合Foresight和Hindsight提示策略，动态调整搜索并积累成功启发式。

Result: 实验显示HiFo-Prompt在启发式质量、收敛速度和查询效率上显著优于现有方法。

Conclusion: HiFo-Prompt通过知识积累和动态调整，显著提升了LLM在AHD中的表现。

Abstract: LLM-based Automatic Heuristic Design (AHD) within Evolutionary Computation
(EC) frameworks has shown promising results. However, its effectiveness is
hindered by the use of static operators and the lack of knowledge accumulation
mechanisms. We introduce HiFo-Prompt, a framework that guides LLMs with two
synergistic prompting strategies: Foresight and Hindsight. Foresight-based
prompts adaptively steer the search based on population dynamics, managing the
exploration-exploitation trade-off. In addition, hindsight-based prompts mimic
human expertise by distilling successful heuristics from past generations into
fundamental, reusable design principles. This dual mechanism transforms
transient discoveries into a persistent knowledge base, enabling the LLM to
learn from its own experience. Empirical results demonstrate that HiFo-Prompt
significantly outperforms state-of-the-art LLM-based AHD methods, generating
higher-quality heuristics while achieving substantially faster convergence and
superior query efficiency.

</details>


### [98] [LOOP: A Plug-and-Play Neuro-Symbolic Framework for Enhancing Planning in Autonomous Systems](https://arxiv.org/abs/2508.13371)
*Ronit Virwani,Ruchika Suryawanshi*

Main category: cs.AI

TL;DR: LOOP is a neuro-symbolic planning framework that improves reliability by enabling iterative communication between neural and symbolic components, achieving an 85.8% success rate on benchmark domains.


<details>
  <summary>Details</summary>
Motivation: Current neural planning approaches struggle with complex domains, while classical planners lack flexibility. Existing neuro-symbolic methods miss iterative refinement opportunities.

Method: LOOP integrates 13 neural features (e.g., graph neural networks, multi-agent validation) and iteratively refines PDDL specifications with symbolic feedback.

Result: LOOP achieved an 85.8% success rate, outperforming LLM+P (55.0%), LLM-as-Planner (19.2%), and Tree-of-Thoughts (3.3%).

Conclusion: Reliable planning requires neural and symbolic components to iteratively communicate, not just translate. LOOP provides a blueprint for trustworthy autonomous systems.

Abstract: Planning is one of the most critical tasks in autonomous systems, where even
a small error can lead to major failures or million-dollar losses. Current
state-of-the-art neural planning approaches struggle with complex domains,
producing plans with missing preconditions, inconsistent goals, and
hallucinations. While classical planners provide logical guarantees, they lack
the flexibility and natural language understanding capabilities needed for
modern autonomous systems. Existing neuro-symbolic approaches use one-shot
translation from natural language to formal plans, missing the opportunity for
neural and symbolic components to work and refine solutions together. To
address this gap, we develop LOOP -- a novel neuro-symbolic planning framework
that treats planning as an iterative conversation between neural and symbolic
components rather than simple translation. LOOP integrates 13 coordinated
neural features including graph neural networks for spatial relationships,
multi-agent validation for consensus-based correctness, hierarchical
decomposition for complex task management, and causal memory that learns from
both successes and failures. Unlike existing approaches, LOOP generates PDDL
specifications, refines them iteratively based on symbolic feedback, and builds
a causal knowledge base from execution traces. LOOP was evaluated on six
standard IPC benchmark domains, where it achieved 85.8% success rate compared
to LLM+P (55.0%), LLM-as-Planner (19.2%), and Tree-of-Thoughts (3.3%). This
work shows that the key to reliable planning is not in choosing between neural
networks or symbolic reasoners but it lies in making them actually ``talk'' to
each other during the entire process. LOOP provides a thorough blueprint for
building autonomous systems that can finally be trusted with critical
real-world applications.

</details>


### [99] [SPANER: Shared Prompt Aligner for Multimodal Semantic Representation](https://arxiv.org/abs/2508.13387)
*Thye Shan Ng,Caren Soyeon Han,Eun-Jung Holden*

Main category: cs.AI

TL;DR: SPANER是一种模态无关的PEFT框架，通过共享提示机制将不同模态的输入嵌入到统一的语义空间中，提升跨模态泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注任务特定增益，忽视了多模态嵌入空间的结构，导致模态特定表示孤立，限制了跨模态泛化。

Method: SPANER采用共享提示机制作为概念锚点，使语义相关实例在空间上收敛，支持无缝集成新模态。

Result: 在视觉-语言和音频-视觉基准测试中，SPANER展示了竞争力的少样本检索性能，同时保持了高语义一致性。

Conclusion: 研究表明，对齐嵌入结构比仅调整适配器权重对可扩展的多模态学习更为重要。

Abstract: Recent advances in multimodal Parameter-Efficient Fine-Tuning (PEFT) have
significantly improved performance on downstream tasks such as few-shot
retrieval. However, most existing approaches focus on task-specific gains while
neglecting the structure of the multimodal embedding space. As a result,
modality-specific representations often remain isolated, limiting cross-modal
generalisation. In this work, we introduce Shared Prompt AligNER (SPANER), a
modality-agnostic PEFT framework designed to embed inputs from diverse
modalities into a unified semantic space. At its core, SPANER employs a shared
prompt mechanism that acts as a conceptual anchor, enabling semantically
related instances to converge spatially regardless of modality. This shared
prompt design is inherently extensible, supporting the seamless integration of
additional modalities, such as audio, without altering the core architecture.
Through comprehensive experiments across vision-language and audio-visual
benchmarks, SPANER demonstrates competitive few-shot retrieval performance
while preserving high semantic coherence in the learned embedding space. Our
results highlight the importance of aligning embedding structures, rather than
merely tuning adapter weights, for scalable multimodal learning.

</details>


### [100] [TASER: Table Agents for Schema-guided Extraction and Recommendation](https://arxiv.org/abs/2508.13404)
*Nicole Cho,Kirsty Fielding,William Watson,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: TASER是一个持续学习的表格提取系统，用于处理复杂的金融表格，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现实中的金融表格数据杂乱且分散，传统方法难以有效提取信息。

Method: TASER通过代理系统进行表格检测、分类、提取和推荐，并结合持续学习优化模式。

Result: TASER在表格检测上比Table Transformer提升10.1%，持续学习使可操作建议增加104.3%。

Conclusion: TASER展示了基于代理的模式引导提取系统在金融表格处理中的潜力。

Abstract: Real-world financial documents report essential information about an entity's
financial holdings that can span millions of different financial instrument
types. Yet, these details are often buried in messy, multi-page, fragmented
tables - for example, 99.4% of the tables in our dataset have no bounding boxes
with the maximum number of rows amounting to 426 per table across 44 pages. To
tackle these unique challenges from real-world tables, we present a
continuously learning, agentic table extraction system, TASER (Table Agents for
Schema-guided Extraction and Recommendation) that extracts highly unstructured,
multi-page, heterogeneous tables into normalized, schema-conforming outputs.
Our table agents execute on table detection, classification, extraction, and
recommendations by leveraging an initial schema. Then, our Recommender Agent
reviews the outputs, recommends schema revisions, and decides on the final
recommendations, enabling TASER to outperform existing table detection models
such as Table Transformer by 10.1%. Within this continuous learning process, we
highlight that larger batch sizes result in a 104.3% increase in schema
recommendations that are actionable and utilized, resulting in a 9.8% increase
in extracted holdings - highlighting the importance of a continuous learning
process. To train TASER, we have manually labeled 22,584 pages (28,150,449
tokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of
the first real financial table datasets. We release our dataset TASERTab to
enable the research community to access real-world financial tables and
outputs. Our results highlight the promise of agentic, schema-guided extraction
systems for robust understanding of real-world financial tables.

</details>


### [101] [Virtuous Machines: Towards Artificial General Science](https://arxiv.org/abs/2508.13421)
*Gabrielle Wehr,Reuben Rideaux,Amaya J. Fox,David R. Lightfoot,Jason Tangen,Jason B. Mattingley,Shane E. Ehrhardt*

Main category: cs.AI

TL;DR: AI系统能够自主完成科学研究流程，包括假设生成、数据收集和论文撰写，展示了其在科学发现中的潜力，但仍存在概念细微差别和理论解释的局限性。


<details>
  <summary>Details</summary>
Motivation: 科学文献的指数增长和领域专业化限制了研究者跨学科知识整合的能力，促使探索更通用的AI系统以加速科学发现。

Method: 开发了一个领域无关的、自主的AI系统，能够独立设计并执行心理学研究，包括数据收集、分析流程开发和论文撰写。

Result: AI系统成功完成了三项心理学研究，展示了其在理论推理和方法严谨性上与研究人员的可比性。

Conclusion: 这是迈向能够通过真实实验测试假设的AI的一步，但也提出了关于科学理解和科学贡献归属的重要问题。

Abstract: Artificial intelligence systems are transforming scientific discovery by
accelerating specific research tasks, from protein structure prediction to
materials design, yet remain confined to narrow domains requiring substantial
human oversight. The exponential growth of scientific literature and increasing
domain specialisation constrain researchers' capacity to synthesise knowledge
across disciplines and develop unifying theories, motivating exploration of
more general-purpose AI systems for science. Here we show that a
domain-agnostic, agentic AI system can independently navigate the scientific
workflow - from hypothesis generation through data collection to manuscript
preparation. The system autonomously designed and executed three psychological
studies on visual working memory, mental rotation, and imagery vividness,
executed one new online data collection with 288 participants, developed
analysis pipelines through 8-hour+ continuous coding sessions, and produced
completed manuscripts. The results demonstrate the capability of AI scientific
discovery pipelines to conduct non-trivial research with theoretical reasoning
and methodological rigour comparable to experienced researchers, though with
limitations in conceptual nuance and theoretical interpretation. This is a step
toward embodied AI that can test hypotheses through real-world experiments,
accelerating discovery by autonomously exploring regions of scientific space
that human cognitive and resource constraints might otherwise leave unexplored.
It raises important questions about the nature of scientific understanding and
the attribution of scientific credit.

</details>


### [102] [STPFormer: A State-of-the-Art Pattern-Aware Spatio-Temporal Transformer for Traffic Forecasting](https://arxiv.org/abs/2508.13433)
*Jiayu Fang,Zhiqi Shao,S T Boris Choy,Junbin Gao*

Main category: cs.AI

TL;DR: STPFormer是一种时空模式感知Transformer，通过统一且可解释的表示学习实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决时空交通预测中复杂的时间模式、动态空间结构和多样输入格式的挑战。

Method: 集成四个模块：TPA（时间模式编码）、SSA（空间序列学习）、STGM（跨域对齐）和Attention Mixer（多尺度融合）。

Result: 在五个真实数据集上取得SOTA结果，并通过消融实验和可视化验证其有效性和泛化性。

Conclusion: STPFormer通过创新的模块设计，显著提升了时空交通预测的性能。

Abstract: Spatio-temporal traffic forecasting is challenging due to complex temporal
patterns, dynamic spatial structures, and diverse input formats. Although
Transformer-based models offer strong global modeling, they often struggle with
rigid temporal encoding and weak space-time fusion. We propose STPFormer, a
Spatio-Temporal Pattern-Aware Transformer that achieves state-of-the-art
performance via unified and interpretable representation learning. It
integrates four modules: Temporal Position Aggregator (TPA) for pattern-aware
temporal encoding, Spatial Sequence Aggregator (SSA) for sequential spatial
learning, Spatial-Temporal Graph Matching (STGM) for cross-domain alignment,
and an Attention Mixer for multi-scale fusion. Experiments on five real-world
datasets show that STPFormer consistently sets new SOTA results, with ablation
and visualizations confirming its effectiveness and generalizability.

</details>


### [103] [Discrete Optimization of Min-Max Violation and its Applications Across Computational Sciences](https://arxiv.org/abs/2508.13437)
*Cheikh Ahmed,Mahdi Mostajabdaveh,Samin Aref,Zirui Zhou*

Main category: cs.AI

TL;DR: 论文提出了一种离散最小最大违规（DMMV）优化问题，并开发了一种GPU加速的启发式算法，在三个实际应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 研究DMMV问题的动机是为了解决具有最坏性能要求的广泛优化问题。

Method: 通过数学定义DMMV问题并开发GPU加速的启发式算法来解决。

Result: 在语言模型量化、离散层析成像和FIR滤波器设计中分别实现了14%、16%和50%的性能提升。

Conclusion: DMMV作为一种上下文无关的优化问题具有广泛适用性，GPU加速的启发式算法显著提升了性能。

Abstract: We introduce the Discrete Min-Max Violation (DMMV) as a general optimization
problem which seeks an assignment of discrete values to variables that
minimizes the largest constraint violation. This context-free mathematical
formulation is applicable to a wide range of use cases that have worst-case
performance requirements. After defining the DMMV problem mathematically, we
explore its properties to establish a foundational understanding. To tackle
DMMV instance sizes of practical relevance, we develop a GPU-accelerated
heuristic that takes advantage of the mathematical properties of DMMV for
speeding up the solution process. We demonstrate the versatile applicability of
our heuristic by solving three optimization problems as use cases: (1)
post-training quantization of language models, (2) discrete tomography, and (3)
Finite Impulse Response (FIR) filter design. In quantization without outlier
separation, our heuristic achieves 14% improvement on average over existing
methods. In discrete tomography, it reduces reconstruction error by 16% under
uniform noise and accelerates computations by a factor of 6 on GPU. For FIR
filter design, it nearly achieves 50% ripple reduction compared to using the
commercial integer optimization solver, Gurobi. Our comparative results point
to the benefits of studying DMMV as a context-free optimization problem and the
advantages that our proposed heuristic offers on three distinct problems. Our
GPU-accelerated heuristic will be made open-source to further stimulate
research on DMMV and its other applications. The code is available at
https://anonymous.4open.science/r/AMVM-5F3E/

</details>


### [104] [LM Agents May Fail to Act on Their Own Risk Knowledge](https://arxiv.org/abs/2508.13465)
*Yuzhi Tang,Tianxiao Li,Elizabeth Li,Chris J. Maddison,Honghua Dong,Yangjun Ruan*

Main category: cs.AI

TL;DR: 论文研究了语言模型代理在安全关键场景中的风险意识与执行能力之间的差距，提出了一个评估框架，并开发了一种风险验证器以减少风险行为。


<details>
  <summary>Details</summary>
Motivation: 语言模型代理在自动化任务中表现出潜力，但在安全关键场景中存在严重风险，其风险意识与执行能力之间存在显著差距。

Method: 开发了一个评估框架，从三个维度（风险知识、风险识别、风险行为）评估代理的安全性，并设计了一个风险验证器和抽象器来减少风险行为。

Result: 评估显示代理的风险知识与实际执行能力存在显著差距，风险验证器系统将风险行为减少了55.3%。

Conclusion: 单纯提升模型能力或计算资源无法解决安全问题，需要独立的风险验证机制来增强安全性。

Abstract: Language model (LM) agents have demonstrated significant potential for
automating real-world tasks, yet they pose a diverse array of potential, severe
risks in safety-critical scenarios. In this work, we identify a significant gap
between LM agents' risk awareness and safety execution abilities: while they
often answer "Yes" to queries like "Is executing `sudo rm -rf /*' dangerous?",
they will likely fail to identify such risks in instantiated trajectories or
even directly perform these risky actions when acting as agents. To
systematically investigate this, we develop a comprehensive evaluation
framework to examine agents' safety across three progressive dimensions: 1)
their knowledge about potential risks, 2) their ability to identify
corresponding risks in execution trajectories, and 3) their actual behaviors to
avoid executing these risky actions. Our evaluation reveals two critical
performance gaps that resemble the generator-validator gaps observed in LMs:
while agents demonstrate near-perfect risk knowledge ($>98\%$ pass rates), they
fail to apply this knowledge when identifying risks in actual scenarios (with
performance dropping by $>23\%$) and often still execute risky actions ($<26\%$
pass rates). Notably, this trend persists across more capable LMs as well as in
specialized reasoning models like DeepSeek-R1, indicating that simply scaling
model capabilities or inference compute does not inherently resolve safety
concerns. Instead, we take advantage of these observed gaps to develop a risk
verifier that independently critiques the proposed actions by agents, with an
abstractor that converts specific execution trajectories into abstract
descriptions where LMs can more effectively identify the risks. Our overall
system achieves a significant reduction of risky action execution by $55.3\%$
over vanilla-prompted agents.

</details>


### [105] [CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter](https://arxiv.org/abs/2508.13530)
*Junyeong Park,Hyeonseo Cho,Sungjin Ahn*

Main category: cs.AI

TL;DR: CrafterDojo是一套基础模型和工具，旨在将Crafter环境转变为轻量级、适合快速原型设计的测试平台，用于通用具身智能体研究。


<details>
  <summary>Details</summary>
Motivation: Minecraft虽然复杂且数据丰富，但速度慢且工程开销大，不适合快速原型设计；Crafter虽轻量但缺乏基础模型支持，限制了其应用范围。

Method: 提出了CrafterDojo，包括CrafterVPT、CrafterCLIP和CrafterSteve-1等基础模型，分别用于行为先验、视觉语言对齐和指令跟随，并提供了数据集生成工具和参考实现。

Result: CrafterDojo为Crafter环境提供了基础模型支持，使其成为轻量级且功能丰富的测试平台。

Conclusion: CrafterDojo填补了Crafter环境在基础模型支持上的空白，为通用具身智能体研究提供了高效的原型设计工具。

Abstract: Developing general-purpose embodied agents is a core challenge in AI.
Minecraft provides rich complexity and internet-scale data, but its slow speed
and engineering overhead make it unsuitable for rapid prototyping. Crafter
offers a lightweight alternative that retains key challenges from Minecraft,
yet its use has remained limited to narrow tasks due to the absence of
foundation models that have driven progress in the Minecraft setting. In this
paper, we present CrafterDojo, a suite of foundation models and tools that
unlock the Crafter environment as a lightweight, prototyping-friendly, and
Minecraft-like testbed for general-purpose embodied agent research. CrafterDojo
addresses this by introducing CrafterVPT, CrafterCLIP, and CrafterSteve-1 for
behavior priors, vision-language grounding, and instruction following,
respectively. In addition, we provide toolkits for generating behavior and
caption datasets (CrafterPlay and CrafterCaption), reference agent
implementations, benchmark evaluations, and a complete open-source codebase.

</details>


### [106] [Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance](https://arxiv.org/abs/2508.13579)
*Yue Fang,Yuxin Guo,Jiaran Gao,Hongxin Ding,Xinke Jiang,Weibin Liao,Yongxin Xu,Yinghao Zhu,Zhibang Yang,Liantao Ma,Junfeng Zhao,Yasha Wang*

Main category: cs.AI

TL;DR: EAG-RL是一种新颖的两阶段训练框架，通过专家注意力引导增强LLMs在电子健康记录（EHR）推理中的能力。


<details>
  <summary>Details</summary>
Motivation: LLMs在医学文本理解方面表现出色，但在基于EHR的预测任务中表现不佳，主要由于难以建模时间结构化和高维数据。现有方法依赖混合范式，未能提升LLMs的推理能力。

Method: EAG-RL首先通过专家引导的蒙特卡洛树搜索构建高质量推理轨迹，初始化LLM策略；然后通过强化学习优化策略，使其注意力与专家模型识别的临床特征对齐。

Result: 在两个真实EHR数据集上的实验表明，EAG-RL平均提升LLMs的EHR推理能力14.62%，并增强了对特征扰动和未见临床领域的鲁棒性。

Conclusion: EAG-RL展示了在临床预测任务中实际部署的潜力。

Abstract: Improving large language models (LLMs) for electronic health record (EHR)
reasoning is essential for enabling accurate and generalizable clinical
predictions. While LLMs excel at medical text understanding, they underperform
on EHR-based prediction tasks due to challenges in modeling temporally
structured, high-dimensional data. Existing approaches often rely on hybrid
paradigms, where LLMs serve merely as frozen prior retrievers while downstream
deep learning (DL) models handle prediction, failing to improve the LLM's
intrinsic reasoning capacity and inheriting the generalization limitations of
DL models. To this end, we propose EAG-RL, a novel two-stage training framework
designed to intrinsically enhance LLMs' EHR reasoning ability through expert
attention guidance, where expert EHR models refer to task-specific DL models
trained on EHR data. Concretely, EAG-RL first constructs high-quality, stepwise
reasoning trajectories using expert-guided Monte Carlo Tree Search to
effectively initialize the LLM's policy. Then, EAG-RL further optimizes the
policy via reinforcement learning by aligning the LLM's attention with
clinically salient features identified by expert EHR models. Extensive
experiments on two real-world EHR datasets show that EAG-RL improves the
intrinsic EHR reasoning ability of LLMs by an average of 14.62%, while also
enhancing robustness to feature perturbations and generalization to unseen
clinical domains. These results demonstrate the practical potential of EAG-RL
for real-world deployment in clinical prediction tasks. Our code have been
available at https://github.com/devilran6/EAG-RL.

</details>


### [107] [Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation](https://arxiv.org/abs/2508.13587)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Liming Zheng,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: 论文提出了一种多模态结构化强化学习方法（MSRL），用于解决图表到代码生成任务中的性能瓶颈问题，通过结合文本和视觉反馈的多粒度奖励系统，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有监督微调（SFT）方法在图表到代码生成任务中存在性能瓶颈，需要更有效的强化学习策略来奖励结构化输出。

Method: 提出MSRL方法，结合文本级规则奖励和视觉级模型奖励，采用两阶段课程学习策略。

Result: MSRL显著突破了SFT的性能瓶颈，在ChartMimic和ReachQA基准上分别提升了6.2%和9.9%。

Conclusion: MSRL通过多模态结构化奖励系统有效解决了图表到代码生成任务中的性能瓶颈问题，性能接近先进闭源模型。

Abstract: While reinforcement learning (RL) has proven highly effective for general
reasoning in vision-language models, its application to tasks requiring
in-depth understanding of information-rich images and generation of structured
outputs remains underexplored. Chart-to-code generation exemplifies this
challenge, demanding complex reasoning over visual charts to generate
structured code. Supervised fine-tuning (SFT) alone is often insufficient,
highlighting the need for effective RL strategies that appropriately reward
structured outputs. We systematically investigate the performance plateau in
SFT through large-scale experiments and propose Multimodal Structured
Reinforcement Learning (MSRL) for chart-to-code generation, which substantially
breaks through this plateau. We construct the largest training corpus to date,
containing 3 million chart-code pairs from real-world arXiv tables to mitigate
simplistic patterns of prior synthetic data. Despite reaching state-of-the-art
performance, our experiments show that scaling SFT data eventually hits a
plateau where further increases yield negligible improvements. Our MSRL method
leverages a multi-granularity structured reward system using multimodal textual
and visual feedback. At the textual level, rule-based rewards validate
fine-grained code details. At the visual level, model-based rewards assess
structural similarity by rendering generated code into images and employing an
evaluator model. We implement this within a two-stage curriculum for training
stability. Results demonstrate that MSRL significantly breaks the SFT plateau,
improving high-level metrics by 6.2% and 9.9% on ChartMimic and ReachQA
benchmarks respectively, achieving competitive performance with advanced
closed-source models.

</details>


### [108] [V2P: From Background Suppression to Center Peaking for Robust GUI Grounding Task](https://arxiv.org/abs/2508.13634)
*Jikai Chen,Long Chen,Dong Wang,Leilei Gan,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: 论文提出了一种名为Valley-to-Peak (V2P)的方法，通过抑制注意力机制和基于Fitts定律的2D高斯热图，解决了GUI元素定位中的背景干扰和中心-边缘区分问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法忽略了空间交互不确定性和视觉语义层次结构，而现有方法在处理背景区域和中心-边缘区分上仍有不足。

Method: V2P结合了抑制注意力机制和2D高斯热图建模，以减少背景干扰并区分中心与边缘。

Result: 在ScreenSpot-v2和ScreenSpot-Pro基准测试中，V2P分别达到92.3%和50.5%的性能。

Conclusion: V2P通过其创新方法有效提升了GUI元素定位的精确性，具有广泛适用性。

Abstract: Precise localization of GUI elements is crucial for the development of GUI
agents. Traditional methods rely on bounding box or center-point regression,
neglecting spatial interaction uncertainty and visual-semantic hierarchies.
Recent methods incorporate attention mechanisms but still face two key issues:
(1) ignoring processing background regions causes attention drift from the
desired area, and (2) uniform labeling fails to distinguish between center and
edges of the target UI element, leading to click imprecision. Inspired by how
humans visually process and interact with GUI elements, we propose the
Valley-to-Peak (V2P) method to address these issues. To mitigate background
distractions, V2P introduces a suppression attention mechanism that minimizes
the model's focus on irrelevant regions to highlight the intended region. For
the issue of center-edge distinction, V2P applies a Fitts' Law-inspired
approach by modeling GUI interactions as 2D Gaussian heatmaps where the weight
gradually decreases from the center towards the edges. The weight distribution
follows a Gaussian function, with the variance determined by the target's size.
Consequently, V2P effectively isolates the target area and teaches the model to
concentrate on the most essential point of the UI element. The model trained by
V2P achieves the performance with 92.3% and 50.5% on two benchmarks
ScreenSpot-v2 and ScreenSpot-Pro. Ablations further confirm each component's
contribution, highlighting V2P's generalizability for precise GUI grounding
tasks.

</details>


### [109] [Interactive Query Answering on Knowledge Graphs with Soft Entity Constraints](https://arxiv.org/abs/2508.13663)
*Daniel Daza,Alberto Bernardi,Luca Costabello,Christophe Gueret,Masoud Mansoury,Michael Cochez,Martijn Schut*

Main category: cs.AI

TL;DR: 论文提出了一种神经查询重排序器（NQR），用于在不完整知识图谱中处理带有软约束的查询，通过交互式调整答案分数。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注一阶逻辑查询，而实际查询常涉及模糊或上下文相关的软约束，如属性偏好或相关类别。

Method: 提出NQR，通过增量示例调整查询答案分数，扩展了现有QA基准以包含软约束数据集。

Result: 实验表明NQR能有效捕捉软约束，同时保持稳健的查询回答性能。

Conclusion: NQR填补了现有方法在处理软约束查询上的不足，为不完整知识图谱的查询提供了新思路。

Abstract: Methods for query answering over incomplete knowledge graphs retrieve
entities that are likely to be answers, which is particularly useful when such
answers cannot be reached by direct graph traversal due to missing edges.
However, existing approaches have focused on queries formalized using
first-order-logic. In practice, many real-world queries involve constraints
that are inherently vague or context-dependent, such as preferences for
attributes or related categories. Addressing this gap, we introduce the problem
of query answering with soft constraints. We propose a Neural Query Reranker
(NQR) designed to adjust query answer scores by incorporating soft constraints
without disrupting the original answers to a query. NQR operates interactively,
refining answers based on incremental examples of preferred and non-preferred
entities. We extend existing QA benchmarks by generating datasets with soft
constraints. Our experiments demonstrate that NQR can capture soft constraints
while maintaining robust query answering performance.

</details>


### [110] [ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings](https://arxiv.org/abs/2508.13672)
*Rehan Raza,Guanjin Wang,Kevin Wong,Hamid Laga,Marco Fisichella*

Main category: cs.AI

TL;DR: 提出了一种基于实例迁移学习的LIME框架（ITL-LIME），通过利用相关源域的真实实例来提高在数据受限环境中的解释保真度和稳定性。


<details>
  <summary>Details</summary>
Motivation: LIME在数据稀缺情况下可能因随机扰动和采样导致解释不稳定和偏离真实数据分布，影响解释的准确性。

Method: ITL-LIME引入实例迁移学习，通过聚类源域并检索相关实例，结合对比学习编码器加权，训练替代模型。

Result: ITL-LIME在数据受限环境中提高了解释的保真度和稳定性。

Conclusion: ITL-LIME通过利用源域实例和对比学习机制，有效解决了LIME在数据稀缺情况下的局限性。

Abstract: Explainable Artificial Intelligence (XAI) methods, such as Local
Interpretable Model-Agnostic Explanations (LIME), have advanced the
interpretability of black-box machine learning models by approximating their
behavior locally using interpretable surrogate models. However, LIME's inherent
randomness in perturbation and sampling can lead to locality and instability
issues, especially in scenarios with limited training data. In such cases, data
scarcity can result in the generation of unrealistic variations and samples
that deviate from the true data manifold. Consequently, the surrogate model may
fail to accurately approximate the complex decision boundary of the original
model. To address these challenges, we propose a novel Instance-based Transfer
Learning LIME framework (ITL-LIME) that enhances explanation fidelity and
stability in data-constrained environments. ITL-LIME introduces instance
transfer learning into the LIME framework by leveraging relevant real instances
from a related source domain to aid the explanation process in the target
domain. Specifically, we employ clustering to partition the source domain into
clusters with representative prototypes. Instead of generating random
perturbations, our method retrieves pertinent real source instances from the
source cluster whose prototype is most similar to the target instance. These
are then combined with the target instance's neighboring real instances. To
define a compact locality, we further construct a contrastive learning-based
encoder as a weighting mechanism to assign weights to the instances from the
combined set based on their proximity to the target instance. Finally, these
weighted source and target instances are used to train the surrogate model for
explanation purposes.

</details>


### [111] [Knowledge Graph Completion for Action Prediction on Situational Graphs -- A Case Study on Household Tasks](https://arxiv.org/abs/2508.13675)
*Mariam Arustashvili,Jörg Deigmöller,Heiko Paulheim*

Main category: cs.AI

TL;DR: 论文研究了家庭行为知识图谱，指出标准链接预测算法不适用于情境知识图谱。


<details>
  <summary>Details</summary>
Motivation: 家庭行为知识图谱对控制家用机器人和分析视频片段有益，但视频提取的信息通常不完整，需补全知识图谱以增强情境理解。

Method: 研究了情境知识图谱的特殊性，并与标准链接预测算法进行比较。

Result: 许多链接预测算法不适合情境知识图谱，甚至无法超越简单基线。

Conclusion: 情境知识图谱需要专门设计的算法，标准链接预测方法效果不佳。

Abstract: Knowledge Graphs are used for various purposes, including business
applications, biomedical analyses, or digital twins in industry 4.0. In this
paper, we investigate knowledge graphs describing household actions, which are
beneficial for controlling household robots and analyzing video footage. In the
latter case, the information extracted from videos is notoriously incomplete,
and completing the knowledge graph for enhancing the situational picture is
essential. In this paper, we show that, while a standard link prediction
problem, situational knowledge graphs have special characteristics that render
many link prediction algorithms not fit for the job, and unable to outperform
even simple baselines.

</details>


### [112] [MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model](https://arxiv.org/abs/2508.13676)
*Yu Li,Zulong Chen,Wenjian Xu,Hong Wen,Yipeng Yu,Man Lung Yiu,Yuyu Yin*

Main category: cs.AI

TL;DR: MHSNet是一个多级身份验证框架，用于检测第三方简历与公司人才库中的简历之间的重复，通过对比学习微调BGE-M3，并利用MoE生成多级表示以提高准确性。


<details>
  <summary>Details</summary>
Motivation: 第三方简历通常不完整且不准确，影响公司人才库的质量，因此需要一种有效的重复检测方法。

Method: 提出MHSNet框架，通过对比学习微调BGE-M3，利用MoE生成多级稀疏和密集表示，计算多级语义相似度。

Result: 实验验证了MHSNet在处理不完整简历和检测重复简历方面的有效性。

Conclusion: MHSNet能够有效提升第三方简历的质量，丰富公司人才库。

Abstract: To maintain the company's talent pool, recruiters need to continuously search
for resumes from third-party websites (e.g., LinkedIn, Indeed). However,
fetched resumes are often incomplete and inaccurate. To improve the quality of
third-party resumes and enrich the company's talent pool, it is essential to
conduct duplication detection between the fetched resumes and those already in
the company's talent pool. Such duplication detection is challenging due to the
semantic complexity, structural heterogeneity, and information incompleteness
of resume texts. To this end, we propose MHSNet, an multi-level identity
verification framework that fine-tunes BGE-M3 using contrastive learning. With
the fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse and
dense representations for resumes, enabling the computation of corresponding
multi-level semantic similarities. Moreover, the state-aware Mixture-of-Experts
(MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimental
results verify the effectiveness of MHSNet

</details>


### [113] [Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models](https://arxiv.org/abs/2508.13678)
*Xiao-Wen Yang,Jie-Jing Shao,Lan-Zhe Guo,Bo-Wen Zhang,Zhi Zhou,Lin-Han Jia,Wang-Zhou Dai,Yu-Feng Li*

Main category: cs.AI

TL;DR: 本文综述了神经符号方法在增强大语言模型（LLM）推理能力方面的最新进展，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在各种任务中表现出色，但其推理能力仍是关键挑战，提升推理能力是实现AGI的重要里程碑。

Method: 从Symbolic->LLM、LLM->Symbolic和LLM+Symbolic三个角度探讨神经符号方法。

Result: 总结了神经符号方法在增强LLM推理能力中的应用，并提出了未来挑战和方向。

Conclusion: 神经符号方法是提升LLM推理能力的有效途径，未来研究需解决关键挑战。

Abstract: Large Language Models (LLMs) have shown promising results across various
tasks, yet their reasoning capabilities remain a fundamental challenge.
Developing AI systems with strong reasoning capabilities is regarded as a
crucial milestone in the pursuit of Artificial General Intelligence (AGI) and
has garnered considerable attention from both academia and industry. Various
techniques have been explored to enhance the reasoning capabilities of LLMs,
with neuro-symbolic approaches being a particularly promising way. This paper
comprehensively reviews recent developments in neuro-symbolic approaches for
enhancing LLM reasoning. We first present a formalization of reasoning tasks
and give a brief introduction to the neurosymbolic learning paradigm. Then, we
discuss neuro-symbolic methods for improving the reasoning capabilities of LLMs
from three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic.
Finally, we discuss several key challenges and promising future directions. We
have also released a GitHub repository including papers and resources related
to this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy.

</details>


### [114] [The DeepLog Neurosymbolic Machine](https://arxiv.org/abs/2508.13697)
*Vincent Derkinderen,Robin Manhaeve,Rik Adriaensen,Lucas Van Praet,Lennert De Smet,Giuseppe Marra,Luc De Raedt*

Main category: cs.AI

TL;DR: DeepLog是一个理论和操作框架，用于神经符号AI，提供构建块和抽象表示，支持多种神经符号系统。


<details>
  <summary>Details</summary>
Motivation: 为神经符号AI提供一个通用的理论和操作框架，抽象化常用表示和计算机制。

Method: DeepLog包括两部分：1）DeepLog语言，用于指定神经符号模型和推理任务；2）计算层面的扩展代数电路。

Result: DeepLog展示了其通用性和效率，通过实验比较不同逻辑、架构和硬件实现。

Conclusion: DeepLog作为一个神经符号抽象机器，通过其语言和计算层面的结合，实现了高效且通用的神经符号AI建模。

Abstract: We contribute a theoretical and operational framework for neurosymbolic AI
called DeepLog. DeepLog introduces building blocks and primitives for
neurosymbolic AI that make abstraction of commonly used representations and
computational mechanisms used in neurosymbolic AI. DeepLog can represent and
emulate a wide range of neurosymbolic systems. It consists of two key
components. The first is the DeepLog language for specifying neurosymbolic
models and inference tasks. This language consists of an annotated neural
extension of grounded first-order logic, and makes abstraction of the type of
logic, e.g. boolean, fuzzy or probabilistic, and whether logic is used in the
architecture or in the loss function. The second DeepLog component is situated
at the computational level and uses extended algebraic circuits as
computational graphs. Together these two components are to be considered as a
neurosymbolic abstract machine, with the DeepLog language as the intermediate
level of abstraction and the circuits level as the computational one. DeepLog
is implemented in software, relies on the latest insights in implementing
algebraic circuits on GPUs, and is declarative in that it is easy to obtain
different neurosymbolic models by making different choices for the underlying
algebraic structures and logics. The generality and efficiency of the DeepLog
neurosymbolic machine is demonstrated through an experimental comparison
between 1) different fuzzy and probabilistic logics, 2) between using logic in
the architecture or in the loss function, and 3) between a standalone CPU-based
implementation of a neurosymbolic AI system and a DeepLog GPU-based one.

</details>


### [115] [CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through Causality-Driven Planning](https://arxiv.org/abs/2508.13721)
*Minh Hoang Nguyen,Van Dai Do,Dung Nguyen,Thin Nguyen,Hung Le*

Main category: cs.AI

TL;DR: CausalPlan框架通过整合显式结构因果推理，改进了LLM代理在协作任务中的规划和协调能力，减少了无效动作并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM代理在协作任务中因依赖表面相关性而产生无效或不连贯动作的问题，提升其在动态环境中的规划和协调能力。

Method: 提出CausalPlan框架，包含Structural Causal Action (SCA)模型，通过学习因果图指导LLM生成的动作选择，无需微调LLM本身。

Result: 在Overcooked-AI基准测试中，CausalPlan显著减少了无效动作，提升了AI-AI和人类-AI协作性能，优于强化学习基线。

Conclusion: 因果驱动的规划对部署高效、可解释和通用的多智能体LLM系统具有重要价值。

Abstract: Large language model (LLM) agents-especially smaller, open-source
models-often produce causally invalid or incoherent actions in collaborative
tasks due to their reliance on surface-level correlations rather than grounded
causal reasoning. This limitation undermines their performance in terms of
coordination and planning in dynamic environments. We address this challenge
with CausalPlan, a two-phase framework that integrates explicit structural
causal reasoning into the LLM planning process. At the core of CausalPlan is
the Structural Causal Action (SCA) model, which learns a causal graph from
agent trajectories to capture how prior actions and current environment states
influence future decisions. This structure is then used to guide action
selection by assigning causal scores to LLM-generated proposals, reweighting
them accordingly, or falling back to causally grounded alternatives when
needed. By embedding this causal knowledge directly into the decision loop,
CausalPlan constrains planning to intervention-consistent behaviours without
requiring fine-tuning of the LLM itself. We evaluate CausalPlan on the
Overcooked-AI benchmark across five multi-agent coordination tasks and four
LLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B, and Llama-70B.
Experimental results show that CausalPlan consistently reduces invalid actions
and improves collaboration in both AI-AI and human-AI settings, outperforming
strong reinforcement learning baselines. Our findings highlight the value of
causality-driven planning for deploying efficient, interpretable, and
generalisable multi-agent LLM systems.

</details>


### [116] [Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making](https://arxiv.org/abs/2508.13754)
*Liuxin Bao,Zhihao Peng,Xiaofei Zhou,Runmin Cong,Jiyong Zhang,Yixuan Yuan*

Main category: cs.AI

TL;DR: 提出了一个名为EMRC的框架，通过多LLM协作提升医疗决策的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 单LLM方法在医疗决策中存在知识局限和静态训练数据的不足，无法有效整合临床信息。

Method: EMRC框架分为两个阶段：专家感知的LLM招募和基于信心与对抗的多代理协作。

Result: 在三个公共MDM数据集上，EMRC优于现有单LLM和多LLM方法，例如在MMLU-Pro-Health数据集上准确率达到74.45%。

Conclusion: EMRC框架通过专家感知的代理招募和代理互补性，显著提升了医疗决策的性能。

Abstract: Medical Decision-Making (MDM) is a complex process requiring substantial
domain-specific expertise to effectively synthesize heterogeneous and
complicated clinical information. While recent advancements in Large Language
Models (LLMs) show promise in supporting MDM, single-LLM approaches are limited
by their parametric knowledge constraints and static training corpora, failing
to robustly integrate the clinical information. To address this challenge, we
propose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC)
framework to enhance the accuracy and reliability of MDM systems. It operates
in two stages: (i) expertise-aware agent recruitment and (ii) confidence- and
adversarial-driven multi-agent collaboration. Specifically, in the first stage,
we use a publicly available corpus to construct an LLM expertise table for
capturing expertise-specific strengths of multiple LLMs across medical
department categories and query difficulty levels. This table enables the
subsequent dynamic selection of the optimal LLMs to act as medical expert
agents for each medical query during the inference phase. In the second stage,
we employ selected agents to generate responses with self-assessed confidence
scores, which are then integrated through the confidence fusion and adversarial
validation to improve diagnostic reliability. We evaluate our EMRC framework on
three public MDM datasets, where the results demonstrate that our EMRC
outperforms state-of-the-art single- and multi-LLM methods, achieving superior
diagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC
achieves 74.45% accuracy, representing a 2.69% improvement over the
best-performing closed-source model GPT- 4-0613, which demonstrates the
effectiveness of our expertise-aware agent recruitment strategy and the agent
complementarity in leveraging each LLM's specialized capabilities.

</details>


### [117] [Quantifier Instantiations: To Mimic or To Revolt?](https://arxiv.org/abs/2508.13811)
*Jan Jakubův,Mikoláš Janota*

Main category: cs.AI

TL;DR: 提出了一种基于概率上下文无关文法的动态量化公式实例化方法，以平衡利用和探索。


<details>
  <summary>Details</summary>
Motivation: 量化公式对SMT求解器具有挑战性，现有实例化技术互补但仍有改进空间。

Method: 通过将观察到的实例化视为潜在语言的样本，使用概率上下文无关文法生成新术语。

Result: 新方法既能模仿成功的实例化，又能通过反转概率探索多样性。

Conclusion: 该方法在量化推理中实现了利用与探索的平衡，提升了SMT求解器的性能。

Abstract: Quantified formulas pose a significant challenge for Satisfiability Modulo
Theories (SMT) solvers due to their inherent undecidability. Existing
instantiation techniques, such as e-matching, syntax-guided, model-based,
conflict-based, and enumerative methods, often complement each other. This
paper introduces a novel instantiation approach that dynamically learns from
these techniques during solving. By treating observed instantiations as samples
from a latent language, we use probabilistic context-free grammars to generate
new, similar terms. Our method not only mimics successful past instantiations
but also explores diversity by optionally inverting learned term probabilities,
aiming to balance exploitation and exploration in quantifier reasoning.

</details>


### [118] [Revisiting RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG System Collaboration](https://arxiv.org/abs/2508.13828)
*Yifei Chen,Guanting Dong,Yutao Zhu,Zhicheng Dou*

Main category: cs.AI

TL;DR: 论文探讨了如何通过集成多个RAG系统来提升下游任务的适应性，从理论和机制两个角度进行了分析。


<details>
  <summary>Details</summary>
Motivation: 单一RAG框架难以适应广泛的下游任务，因此研究如何利用多个RAG系统的优势成为重要课题。

Method: 从信息熵角度理论分析RAG集成框架，并在管道和模块层面进行机制分析，选择了四种管道和三种模块解决七个研究问题。

Result: 实验表明，无论是管道还是模块层面，集成多个RAG系统都具有普适性和鲁棒性。

Conclusion: 该研究为多RAG系统集成奠定了基础。

Abstract: Retrieval-Augmented Generation (RAG) technology has been widely applied in
recent years. However, despite the emergence of various RAG frameworks, a
single RAG framework still cannot adapt well to a broad range of downstream
tasks. Therefore, how to leverage the advantages of multiple RAG systems has
become an area worth exploring. To address this issue, we have conducted a
comprehensive and systematic investigation into ensemble methods based on RAG
systems. Specifically, we have analyzed the RAG ensemble framework from both
theoretical and mechanistic analysis perspectives. From the theoretical
analysis, we provide the first explanation of the RAG ensemble framework from
the perspective of information entropy. In terms of mechanism analysis, we have
explored the RAG ensemble framework from both the pipeline and module levels.
We carefully select four different pipelines (Branching, Iterative, Loop, and
Agentic) and three different modules (Generator, Retriever, and Reranker) to
solve seven different research questions. The experiments show that aggregating
multiple RAG systems is both generalizable and robust, whether at the pipeline
level or the module level. Our work lays the foundation for similar research on
the multi-RAG system ensemble.

</details>


### [119] [Improved Generalized Planning with LLMs through Strategy Refinement and Reflection](https://arxiv.org/abs/2508.13876)
*Katharina Stein,Nils Hodel,Daniel Fišer,Jörg Hoffmann,Michael Katz,Alexander Koller*

Main category: cs.AI

TL;DR: LLMs生成Python程序表示PDDL规划中的广义计划，新方法通过伪代码自动调试和程序变体选择提升计划质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中策略错误导致广义计划错误的问题，提升生成计划的准确性和可靠性。

Method: 生成伪代码并自动调试，扩展Python调试阶段加入反思步骤，生成多个程序变体并选择最佳。

Result: 在17个基准域中显著提升广义计划质量，12个域的程序能解决所有生成任务。

Conclusion: 新方法通过伪代码调试和程序变体选择有效提升广义计划的准确性和可靠性。

Abstract: LLMs have recently been used to generate Python programs representing
generalized plans in PDDL planning, i.e., plans that generalize across the
tasks of a given PDDL domain. Previous work proposed a framework consisting of
three steps: the LLM first generates a summary and then a strategy for the
domain, both in natural language, and then implements that strategy as a Python
program, that gets debugged on example planning tasks. In that work, only one
strategy is generated and passed directly to the program generation. If the
strategy is incorrect, its implementation will therefore result in an incorrect
generalized plan. Here, we introduce an approach that generates the strategy in
the form of pseudocode and enables automatic debugging of the pseudocode, hence
allowing us to identify and fix errors prior to the generation of the
generalized plan itself. Additionally, we extend the Python debugging phase
with a reflection step prompting the LLM to pinpoint the reason for the
observed plan failure. Finally, we take inspiration from LLM code generation to
produce several program variants and pick the best one. Running experiments on
17 benchmark domains, we show that these extensions substantially improve (and
never deteriorate) the quality of the generalized plans. In 12 of the domains,
our best Python programs solve all tasks that can be generated with the
respective instance generator.

</details>


### [120] [Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback](https://arxiv.org/abs/2508.13915)
*Yihao Ang,Yifan Bao,Lei Jiang,Jiajie Tao,Anthony K. H. Tung,Lukasz Szpruch,Hao Ni*

Main category: cs.AI

TL;DR: TS-Agent是一个模块化代理框架，旨在自动化和增强金融应用中时间序列建模工作流程，通过模型选择、代码优化和微调三个阶段，结合上下文推理和实验反馈，显著优于现有AutoML和代理基线。


<details>
  <summary>Details</summary>
Motivation: 金融市场中时间序列数据对决策至关重要，但现有AutoML框架缺乏对领域特定需求和动态目标的适应性和响应能力。LLMs的推理和动态代码生成能力为解决这一问题提供了可能。

Method: TS-Agent通过三个阶段（模型选择、代码优化和微调）构建结构化迭代决策流程，利用规划代理和知识库指导探索，提高可解释性并减少错误传播。

Result: 在金融预测和合成数据生成任务中，TS-Agent在准确性、鲁棒性和决策可追溯性方面显著优于现有AutoML和代理基线。

Conclusion: TS-Agent通过模块化代理框架实现了时间序列建模工作流程的自动化和增强，满足了金融服务等高风险环境对自适应学习、调试和透明审计的需求。

Abstract: Time-series data is central to decision-making in financial markets, yet
building high-performing, interpretable, and auditable models remains a major
challenge. While Automated Machine Learning (AutoML) frameworks streamline
model development, they often lack adaptability and responsiveness to
domain-specific needs and evolving objectives. Concurrently, Large Language
Models (LLMs) have enabled agentic systems capable of reasoning, memory
management, and dynamic code generation, offering a path toward more flexible
workflow automation. In this paper, we introduce \textsf{TS-Agent}, a modular
agentic framework designed to automate and enhance time-series modeling
workflows for financial applications. The agent formalizes the pipeline as a
structured, iterative decision process across three stages: model selection,
code refinement, and fine-tuning, guided by contextual reasoning and
experimental feedback. Central to our architecture is a planner agent equipped
with structured knowledge banks, curated libraries of models and refinement
strategies, which guide exploration, while improving interpretability and
reducing error propagation. \textsf{TS-Agent} supports adaptive learning,
robust debugging, and transparent auditing, key requirements for high-stakes
environments such as financial services. Empirical evaluations on diverse
financial forecasting and synthetic data generation tasks demonstrate that
\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic
baselines, achieving superior accuracy, robustness, and decision traceability.

</details>


### [121] [The Collaboration Paradox: Why Generative AI Requires Both Strategic Intelligence and Operational Stability in Supply Chain Management](https://arxiv.org/abs/2508.13942)
*Soumyadeep Dhar*

Main category: cs.AI

TL;DR: 研究发现AI驱动的协作代理在供应链中可能引发‘协作悖论’，导致系统崩溃，并提出了一种双层框架来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 研究AI代理在多级供应链中的战略行为，尤其是协作设计可能导致的意外失败模式。

Method: 通过计算实验和供应链模拟，分析AI代理的行为，并提出双层框架（高层策略设定和低层协作执行）。

Result: 发现‘协作悖论’导致系统崩溃，但双层框架能有效提升系统稳定性。

Conclusion: 研究揭示了协作AI代理的潜在风险，并提供了设计稳定AI驱动系统的蓝图。

Abstract: The rise of autonomous, AI-driven agents in economic settings raises critical
questions about their emergent strategic behavior. This paper investigates
these dynamics in the cooperative context of a multi-echelon supply chain, a
system famously prone to instabilities like the bullwhip effect. We conduct
computational experiments with generative AI agents, powered by Large Language
Models (LLMs), within a controlled supply chain simulation designed to isolate
their behavioral tendencies. Our central finding is the "collaboration
paradox": a novel, catastrophic failure mode where theoretically superior
collaborative AI agents, designed with Vendor-Managed Inventory (VMI)
principles, perform even worse than non-AI baselines. We demonstrate that this
paradox arises from an operational flaw where agents hoard inventory, starving
the system. We then show that resilience is only achieved through a synthesis
of two distinct layers: high-level, AI-driven proactive policy-setting to
establish robust operational targets, and a low-level, collaborative execution
protocol with proactive downstream replenishment to maintain stability. Our
final framework, which implements this synthesis, can autonomously generate,
evaluate, and quantify a portfolio of viable strategic choices. The work
provides a crucial insight into the emergent behaviors of collaborative AI
agents and offers a blueprint for designing stable, effective AI-driven systems
for business analytics.

</details>


### [122] [ChronoLLM: Customizing Language Models for Physics-Based Simulation Code Generation](https://arxiv.org/abs/2508.13975)
*Jingquan Wang,Andrew Negrut,Harry Zhang,Khailanii Slaton,Shu Wang,Radu Serban,Jinlong Wu,Dan Negrut*

Main category: cs.AI

TL;DR: 论文探讨如何通过微调大型语言模型（LLM）使其成为专家使用PyChrono仿真工具的虚拟助手，生成仿真脚本并降低使用门槛。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用LLM帮助专家更高效地使用PyChrono仿真工具，生成脚本并提供API支持。

Method: 提出一个框架，通过微调开源和闭源LLM，提升生成PyChrono仿真脚本的质量。

Result: 生成的脚本质量显著提升，可作为用户优化的起点，并能回答API问题或推荐建模方法。

Conclusion: 该框架具有通用性，可推广到其他仿真工具，降低使用门槛。

Abstract: This contribution is concerned with the following issue: can pretrained large
language models (LLMs) be refined and customized to the point where they become
virtual assistants helping experts with the effective use of a simulation tool?
In this case study, the ``simulation tool'' considered is PyChrono, an open
source multi-physics dynamics engine for multibody systems. We present a
framework for refining and customizing both open- and closed-source LLMs to
harness the power of AI in generating scripts that perform PyChrono virtual
experiments. We refine and customize several classes of LLMs through a process
that leads to a quantifiable improvement in the quality of the generated
PyChrono simulation scripts. These scripts can range from simple
single-pendulum simulations to complex virtual experiments involving full
vehicles on deformable terrain. While the generated scripts are rarely perfect,
they often serve as strong starting points for the user to modify and improve
on. Additionally, the LLM can answer specific API questions about the
simulator, or recommend modeling approaches. The framework discussed is general
and can be applied to lower the entry barrier for simulation tools associated
with other application domains.

</details>


### [123] [A Biased Random Key Genetic Algorithm for Solving the Longest Run Subsequence Problem](https://arxiv.org/abs/2508.14020)
*Christian Blum,Pedro Pinacho-Davidson*

Main category: cs.AI

TL;DR: 本文提出了一种基于BRKGA的LRS问题解决方案，并与其他方法（如Max-Min Ant System和CPLEX）进行了比较，结果表明BRKGA是目前最先进的技术。


<details>
  <summary>Details</summary>
Motivation: LRS问题是生物信息学中的一个NP难组合优化问题，尤其在基因组重组中具有重要作用。

Method: 使用BRKGA，并特别关注将灰度值向量转换为有效解的效率。同时开发了Max-Min Ant System和CPLEX求解器进行比较。

Result: BRKGA是目前解决LRS问题的最先进技术，但在处理大字母表输入字符串时仍有改进空间。

Conclusion: BRKGA在LRS问题上表现优异，但未来研究可进一步优化其在处理大字母表输入时的性能。

Abstract: The longest run subsequence (LRS) problem is an NP-hard combinatorial
optimization problem belonging to the class of subsequence problems from
bioinformatics. In particular, the problem plays a role in genome reassembly.
In this paper, we present a solution to the LRS problem using a Biased Random
Key Genetic Algorithm (BRKGA). Our approach places particular focus on the
computational efficiency of evaluating individuals, which involves converting
vectors of gray values into valid solutions to the problem. For comparison
purposes, a Max-Min Ant System is developed and implemented. This is in
addition to the application of the integer linear programming solver CPLEX for
solving all considered problem instances. The computation results show that the
proposed BRKGA is currently a state-of-the-art technique for the LRS problem.
Nevertheless, the results also show that there is room for improvement,
especially in the context of input strings based on large alphabet sizes.

</details>


### [124] [ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents](https://arxiv.org/abs/2508.14040)
*Hanyu Lai,Xiao Liu,Yanxiao Zhao,Han Xu,Hanchen Zhang,Bohao Jing,Yanyu Ren,Shuntian Yao,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: ComputerRL是一个用于自主桌面智能的框架，通过API-GUI范式统一程序化API调用和直接GUI交互，解决了机器代理与以人为中心的桌面环境之间的不匹配问题。通过分布式RL基础设施和Entropulse训练策略，实现了大规模在线RL训练，并在OSWorld基准测试中取得了48.1%的最新准确率。


<details>
  <summary>Details</summary>
Motivation: 解决机器代理在复杂桌面环境中操作效率低和不稳定的问题，提升自主桌面智能的通用性和性能。

Method: 采用API-GUI范式，结合分布式RL基础设施和Entropulse训练策略（交替强化学习和监督微调），以支持大规模在线RL训练。

Result: 在OSWorld基准测试中，基于GLM-4-9B-0414的AutoGLM-OS-9B模型达到了48.1%的最新准确率。

Conclusion: ComputerRL框架及其方法显著提升了桌面自动化中通用代理的性能，为自主桌面智能提供了可扩展且稳健的解决方案。

Abstract: We introduce ComputerRL, a framework for autonomous desktop intelligence that
enables agents to operate complex digital workspaces skillfully. ComputerRL
features the API-GUI paradigm, which unifies programmatic API calls and direct
GUI interaction to address the inherent mismatch between machine agents and
human-centric desktop environments. Scaling end-to-end RL training is crucial
for improvement and generalization across diverse desktop tasks, yet remains
challenging due to environmental inefficiency and instability in extended
training. To support scalable and robust training, we develop a distributed RL
infrastructure capable of orchestrating thousands of parallel virtual desktop
environments to accelerate large-scale online RL. Furthermore, we propose
Entropulse, a training strategy that alternates reinforcement learning with
supervised fine-tuning, effectively mitigating entropy collapse during extended
training runs. We employ ComputerRL on open models GLM-4-9B-0414 and
Qwen2.5-14B, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B
based on GLM-4-9B-0414 achieves a new state-of-the-art accuracy of 48.1%,
demonstrating significant improvements for general agents in desktop
automation. The algorithm and framework are adopted in building AutoGLM (Liu et
al., 2024a)

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [125] [Uncertainty-Aware Learning Policy for Reliable Pulmonary Nodule Detection on Chest X-Ray](https://arxiv.org/abs/2508.13236)
*Hyeonjin Choi,Jinse Kim,Dong-yeon Yoo,Ju-sung Sun,Jung-won Lee*

Main category: eess.IV

TL;DR: 论文提出了一种不确定性感知学习策略，通过结合医生的背景知识和胸部X光病灶信息，提高AI诊断的准确性和可信度。


<details>
  <summary>Details</summary>
Motivation: 由于医生对AI诊断的不确定性持怀疑态度，限制了AI在临床中的广泛应用。这种不信任源于AI缺乏足够的背景知识来支持诊断。

Method: 采用不确定性感知学习策略，结合医生的背景知识和胸部X光病灶信息进行训练。数据集包括2,517张无病灶图像和656张结节图像。

Result: 模型在IoU 0.2 / FPPI 2下达到92%的准确率，敏感性比基线模型提高10%，同时不确定性（熵）降低0.2。

Conclusion: 通过结合医生的背景知识，提出的方法有效减少了AI诊断的不确定性，提高了诊断的准确性和可信度。

Abstract: Early detection and rapid intervention of lung cancer are crucial.
Nonetheless, ensuring an accurate diagnosis is challenging, as physicians'
ability to interpret chest X-rays varies significantly depending on their
experience and degree of fatigue. Although medical AI has been rapidly
advancing to assist in diagnosis, physicians' trust in such systems remains
limited, preventing widespread clinical adoption. This skepticism fundamentally
stems from concerns about its diagnostic uncertainty. In clinical diagnosis,
physicians utilize extensive background knowledge and clinical experience. In
contrast, medical AI primarily relies on repetitive learning of the target
lesion to generate diagnoses based solely on that data. In other words, medical
AI does not possess sufficient knowledge to render a diagnosis, leading to
diagnostic uncertainty. Thus, this study suggests an Uncertainty-Aware Learning
Policy that can address the issue of knowledge deficiency by learning the
physicians' background knowledge alongside the Chest X-ray lesion information.
We used 2,517 lesion-free images and 656 nodule images, all obtained from Ajou
University Hospital. The proposed model attained 92% (IoU 0.2 / FPPI 2) with a
10% enhancement in sensitivity compared to the baseline model while also
decreasing entropy as a measure of uncertainty by 0.2.

</details>
