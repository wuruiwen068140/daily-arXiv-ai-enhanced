<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench](https://arxiv.org/abs/2508.00081)
*Fred Mutisya,Shikoh Gitau,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha*

Main category: cs.AI

TL;DR: HealthBench存在依赖专家意见而非临床证据的问题，可能导致偏见。作者提出基于临床实践指南（CPGs）的改进方案，以提升全球相关性和公平性。


<details>
  <summary>Details</summary>
Motivation: HealthBench依赖专家意见，可能引入区域偏见和个体差异，尤其在低收入地区问题更突出。需要更全球化和公平的基准。

Method: 提出基于版本控制CPGs的奖励函数，结合系统评价和GRADE证据评级，通过强化学习改进评分。

Result: 改进方案旨在提升医学语言模型的临床可信度、伦理性和全球适用性。

Conclusion: 通过CPGs重新定义奖励机制，结合透明性和医生参与，目标是开发更可靠、公平的医学语言模型。

Abstract: HealthBench, a benchmark designed to measure the capabilities of AI systems
for health better (Arora et al., 2025), has advanced medical language model
evaluation through physician-crafted dialogues and transparent rubrics.
However, its reliance on expert opinion, rather than high-tier clinical
evidence, risks codifying regional biases and individual clinician
idiosyncrasies, further compounded by potential biases in automated grading
systems. These limitations are particularly magnified in low- and middle-income
settings, where issues like sparse neglected tropical disease coverage and
region-specific guideline mismatches are prevalent.
  The unique challenges of the African context, including data scarcity,
inadequate infrastructure, and nascent regulatory frameworks, underscore the
urgent need for more globally relevant and equitable benchmarks. To address
these shortcomings, we propose anchoring reward functions in version-controlled
Clinical Practice Guidelines (CPGs) that incorporate systematic reviews and
GRADE evidence ratings.
  Our roadmap outlines "evidence-robust" reinforcement learning via
rubric-to-guideline linkage, evidence-weighted scoring, and contextual override
logic, complemented by a focus on ethical considerations and the integration of
delayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,
while preserving HealthBench's transparency and physician engagement, we aim to
foster medical language models that are not only linguistically polished but
also clinically trustworthy, ethically sound, and globally relevant.

</details>


### [2] [Hyperproperty-Constrained Secure Reinforcement Learning](https://arxiv.org/abs/2508.00106)
*Ernest Bonnah,Luan Viet Nguyen,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: 本文提出了一种基于HyperTWTL的安全强化学习方法（SecRL），通过动态Boltzmann softmax RL学习满足HyperTWTL约束的安全最优策略，并在机器人任务中验证了其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注时序逻辑约束的安全强化学习（SRL），但缺乏基于超属性（hyperproperties）的安全感知强化学习（RL）探索。本文旨在填补这一研究空白。

Method: 将智能体的动态建模为马尔可夫决策过程（MDP），并将不透明性/安全约束形式化为HyperTWTL，提出了一种动态Boltzmann softmax RL方法。

Result: 通过机器人拾取与交付任务的案例研究，验证了方法的有效性和可扩展性，并优于两种基线RL算法。

Conclusion: 本文提出的HyperTWTL约束SecRL方法在安全感知强化学习中表现出色，为相关领域提供了新的解决方案。

Abstract: Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a
domain-specific formal specification language known for its effectiveness in
compactly representing security, opacity, and concurrency properties for
robotics applications. This paper focuses on HyperTWTL-constrained secure
reinforcement learning (SecRL). Although temporal logic-constrained safe
reinforcement learning (SRL) is an evolving research problem with several
existing literature, there is a significant research gap in exploring
security-aware reinforcement learning (RL) using hyperproperties. Given the
dynamics of an agent as a Markov Decision Process (MDP) and opacity/security
constraints formalized as HyperTWTL, we propose an approach for learning
security-aware optimal policies using dynamic Boltzmann softmax RL while
satisfying the HyperTWTL constraints. The effectiveness and scalability of our
proposed approach are demonstrated using a pick-up and delivery robotic mission
case study. We also compare our results with two other baseline RL algorithms,
showing that our proposed method outperforms them.

</details>


### [3] [No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence](https://arxiv.org/abs/2508.00116)
*Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: 论文探讨了AI在工业环境中的应用挑战，提出通过对象中心过程挖掘（OCPM）将AI与过程智能（PI）结合，以优化端到端操作流程。


<details>
  <summary>Details</summary>
Motivation: 组织在工业环境中成功应用AI面临挑战，尤其是端到端操作流程的诊断和改进。

Method: 采用对象中心过程挖掘（OCPM）作为连接数据和过程的桥梁，结合生成、预测和规范性AI。

Result: OCPM是连接数据和过程的关键，能够支持多种AI形式，形成过程智能（PI）。

Conclusion: AI需要PI来优化操作流程，OCPM与多种AI形式的结合为工业应用提供了新机会。

Abstract: The uptake of Artificial Intelligence (AI) impacts the way we work, interact,
do business, and conduct research. However, organizations struggle to apply AI
successfully in industrial settings where the focus is on end-to-end
operational processes. Here, we consider generative, predictive, and
prescriptive AI and elaborate on the challenges of diagnosing and improving
such processes. We show that AI needs to be grounded using Object-Centric
Process Mining (OCPM). Process-related data are structured and
organization-specific and, unlike text, processes are often highly dynamic.
OCPM is the missing link connecting data and processes and enables different
forms of AI. We use the term Process Intelligence (PI) to refer to the
amalgamation of process-centric data-driven techniques able to deal with a
variety of object and event types, enabling AI in an organizational context.
This paper explains why AI requires PI to improve operational processes and
highlights opportunities for successfully combining OCPM and generative,
predictive, and prescriptive AI.

</details>


### [4] [Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis](https://arxiv.org/abs/2508.00129)
*Agustín Borda,Juan Bautista Cabral,Gonzalo Giarda,Diego Nicolás Gimenez Irusta,Paula Pacheco,Alvaro Roy Schachner*

Main category: cs.AI

TL;DR: 论文提出了三种检测多准则决策分析中排名反转问题的测试方法，并在Scikit-Criteria库中实现。


<details>
  <summary>Details</summary>
Motivation: 排名反转是多准则决策分析中的严重问题，影响决策方法的有效性，因此需要一种机制来评估方法的性能。

Method: 提出了三种测试方法，并在Scikit-Criteria库中实现，同时讨论了通用场景下的实现挑战和设计考虑。

Result: 实现了三种测试方法，解决了通用场景下的实现问题。

Conclusion: 这些方法在多准则决策方法的评估中可能发挥重要作用。

Abstract: In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem
that can greatly affect the results of a Multi-Criteria Decision Method against
a particular set of alternatives. It is therefore useful to have a mechanism
that allows one to measure the performance of a method on a set of
alternatives. This idea could be taken further to build a global ranking of the
effectiveness of different methods to solve a problem. In this paper, we
present three tests that detect the presence of Rank Reversals, along with
their implementation in the Scikit-Criteria library. We also address the
complications that arise when implementing these tests for general scenarios
and the design considerations we made to handle them. We close with a
discussion about how these additions could play a major role in the judgment of
multi-criteria decision methods for problem solving.

</details>


### [5] [SHACL Validation under Graph Updates (Extended Paper)](https://arxiv.org/abs/2508.00137)
*Shqiponja Ahmetaj,George Konstantinidis,Magdalena Ortiz,Paolo Pareti,Mantas Simkus*

Main category: cs.AI

TL;DR: 研究SHACL在RDF图更新下的静态验证问题，提出一种基于SHACL的更新语言，并通过回归技术将更新操作嵌入SHACL约束中。


<details>
  <summary>Details</summary>
Motivation: 解决RDF图在更新后仍满足SHACL规范的问题，为演化中的RDF图提供推理基础。

Method: 使用回归技术将更新操作嵌入SHACL约束，分析静态验证问题的计算复杂性。

Result: 证明静态验证问题可归约为SHACL约束的（不）可满足性，并实现原型进行初步实验。

Conclusion: 静态验证问题在SHACL及其关键片段中具有可行性，为RDF图的动态管理提供了理论基础和工具支持。

Abstract: SHACL (SHApe Constraint Language) is a W3C standardized constraint language
for RDF graphs. In this paper, we study SHACL validation in RDF graphs under
updates. We present a SHACL-based update language that can capture intuitive
and realistic modifications on RDF graphs and study the problem of static
validation under such updates. This problem asks to verify whether every graph
that validates a SHACL specification will still do so after applying a given
update sequence. More importantly, it provides a basis for further services for
reasoning about evolving RDF graphs. Using a regression technique that embeds
the update actions into SHACL constraints, we show that static validation under
updates can be reduced to (un)satisfiability of constraints in (a minor
extension of) SHACL. We analyze the computational complexity of the static
validation problem for SHACL and some key fragments. Finally, we present a
prototype implementation that performs static validation and other static
analysis tasks on SHACL constraints and demonstrate its behavior through
preliminary experiments.

</details>


### [6] [Co-Producing AI: Toward an Augmented, Participatory Lifecycle](https://arxiv.org/abs/2508.00138)
*Rashid Mushkani,Hugo Berard,Toumadher Ammar,Cassandre Chatonnier,Shin Koseki*

Main category: cs.AI

TL;DR: 论文提出了一种基于设计正义和参与式AI的AI生产流程重构方法，强调共同生产、多样性、公平性和多学科协作。


<details>
  <summary>Details</summary>
Motivation: AI算法对文化边缘群体的不公平影响需要更根本的解决方案，而不仅仅是技术或伦理指南。

Method: 提出了一个增强的AI生命周期，包含五个相互关联的阶段：共同框架、共同设计、共同实施、共同部署和共同维护。

Result: 通过四个多学科研讨会验证了该生命周期，并强调了分布式权威和迭代知识交换的重要性。

Conclusion: 论文将提出的生命周期与主要伦理框架联系起来，并指出了扩展参与式治理的关键研究问题。

Abstract: Despite efforts to mitigate the inherent risks and biases of artificial
intelligence (AI) algorithms, these algorithms can disproportionately impact
culturally marginalized groups. A range of approaches has been proposed to
address or reduce these risks, including the development of ethical guidelines
and principles for responsible AI, as well as technical solutions that promote
algorithmic fairness. Drawing on design justice, expansive learning theory, and
recent empirical work on participatory AI, we argue that mitigating these harms
requires a fundamental re-architecture of the AI production pipeline. This
re-design should center co-production, diversity, equity, inclusion (DEI), and
multidisciplinary collaboration. We introduce an augmented AI lifecycle
consisting of five interconnected phases: co-framing, co-design,
co-implementation, co-deployment, and co-maintenance. The lifecycle is informed
by four multidisciplinary workshops and grounded in themes of distributed
authority and iterative knowledge exchange. Finally, we relate the proposed
lifecycle to several leading ethical frameworks and outline key research
questions that remain for scaling participatory governance.

</details>


### [7] [Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation](https://arxiv.org/abs/2508.00143)
*Danielle R. Thomas,Conrad Borchers,Kenneth R. Koedinger*

Main category: cs.AI

TL;DR: 论文主张减少对人工标注一致性的依赖，提出五种补充评估方法以提高教育数据的有效性和预测性。


<details>
  <summary>Details</summary>
Motivation: 人类评估者存在偏见和不可靠性，传统的一致性指标（如Cohen's kappa）不足以确保标注数据的质量，阻碍了教育数据的有效分类。

Method: 提出五种补充评估方法，包括多标签标注方案、专家评估和闭环验证等，强调外部有效性。

Result: 这些方法能生成更有效的训练数据和模型，提升学生学习效果和可操作性。

Conclusion: 呼吁重新思考标注质量，优先考虑有效性和教育影响，而非仅依赖一致性。

Abstract: Humans can be notoriously imperfect evaluators. They are often biased,
unreliable, and unfit to define "ground truth." Yet, given the surging need to
produce large amounts of training data in educational applications using AI,
traditional inter-rater reliability (IRR) metrics like Cohen's kappa remain
central to validating labeled data. IRR remains a cornerstone of many machine
learning pipelines for educational data. Take, for example, the classification
of tutors' moves in dialogues or labeling open responses in machine-graded
assessments. This position paper argues that overreliance on human IRR as a
gatekeeper for annotation quality hampers progress in classifying data in ways
that are valid and predictive in relation to improving learning. To address
this issue, we highlight five examples of complementary evaluation methods,
such as multi-label annotation schemes, expert-based approaches, and
close-the-loop validity. We argue that these approaches are in a better
position to produce training data and subsequent models that produce improved
student learning and more actionable insights than IRR approaches alone. We
also emphasize the importance of external validity, for example, by
establishing a procedure of validating tutor moves and demonstrating that it
works across many categories of tutor actions (e.g., providing hints). We call
on the field to rethink annotation quality and ground truth--prioritizing
validity and educational impact over consensus alone.

</details>


### [8] [Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power](https://arxiv.org/abs/2508.00159)
*Jobst Heitzig,Ram Potham*

Main category: cs.AI

TL;DR: 论文探讨通过设计AI目标函数来平衡人类与AI的权力，以促进安全和福祉。


<details>
  <summary>Details</summary>
Motivation: 权力是AI安全的核心概念，如何在AI与人类互动中管理权力平衡是关键。

Method: 采用部分公理化方法设计参数化目标函数，考虑人类有限理性和社会规范。

Result: 提出算法计算权力指标，并通过多智能体强化学习近似实现。

Conclusion: 软最大化人类权力指标可能是比直接效用目标更安全的AI设计方向。

Abstract: Power is a key concept in AI safety: power-seeking as an instrumental goal,
sudden or gradual disempowerment of humans, power balance in human-AI
interaction and international AI governance. At the same time, power as the
ability to pursue diverse goals is essential for wellbeing.
  This paper explores the idea of promoting both safety and wellbeing by
forcing AI agents explicitly to empower humans and to manage the power balance
between humans and AI agents in a desirable way. Using a principled, partially
axiomatic approach, we design a parametrizable and decomposable objective
function that represents an inequality- and risk-averse long-term aggregate of
human power. It takes into account humans' bounded rationality and social
norms, and, crucially, considers a wide variety of possible human goals.
  We derive algorithms for computing that metric by backward induction or
approximating it via a form of multi-agent reinforcement learning from a given
world model. We exemplify the consequences of (softly) maximizing this metric
in a variety of paradigmatic situations and describe what instrumental
sub-goals it will likely imply. Our cautious assessment is that softly
maximizing suitable aggregate metrics of human power might constitute a
beneficial objective for agentic AI systems that is safer than direct
utility-based objectives.

</details>


### [9] [RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization](https://arxiv.org/abs/2508.00222)
*Yihong Dong,Xue Jiang,Yongding Tao,Huanyu Liu,Kechi Zhang,Lili Mou,Rongyu Cao,Yingwei Ma,Jue Chen,Binhua Li,Zhi Jin,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: RL-PLUS是一种新方法，通过结合内部推理和外部数据，解决了RLVR在大型语言模型中的能力边界问题，并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: RLVR虽然提升了大型语言模型的复杂推理能力，但由于其固有的策略和稀疏奖励，无法突破基础模型的能力边界，甚至可能导致能力边界崩溃。

Method: RL-PLUS结合了多重重要性采样和基于探索的优势函数，以解决外部数据的分布不匹配问题，并引导模型探索高价值路径。

Result: RL-PLUS在六个数学推理基准测试中表现最优，并在六个分布外推理任务中表现优异，平均相对提升21.1%至69.2%。

Conclusion: RL-PLUS有效解决了能力边界崩溃问题，并在多种模型家族中实现了显著且一致的性能提升。

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has significantly
advanced the complex reasoning abilities of Large Language Models (LLMs).
However, it struggles to break through the inherent capability boundaries of
the base LLM, due to its inherently on-policy strategy with LLM's immense
action space and sparse reward. Further, RLVR can lead to the capability
boundary collapse, narrowing the LLM's problem-solving scope. To address this
problem, we propose RL-PLUS, a novel approach that synergizes internal
exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve
stronger reasoning capabilities and surpass the boundaries of base models.
RL-PLUS integrates two core components: Multiple Importance Sampling to address
for distributional mismatch from external data, and an Exploration-Based
Advantage Function to guide the model towards high-value, unexplored reasoning
paths. We provide both theoretical analysis and extensive experiments to
demonstrate the superiority and generalizability of our approach. The results
show that RL-PLUS achieves state-of-the-art performance compared with existing
RLVR methods on six math reasoning benchmarks and exhibits superior performance
on six out-of-distribution reasoning tasks. It also achieves consistent and
significant gains across diverse model families, with average relative
improvements ranging from 21.1\% to 69.2\%. Moreover, Pass@k curves across
multiple benchmarks indicate that RL-PLUS effectively resolves the capability
boundary collapse problem.

</details>


### [10] [MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning](https://arxiv.org/abs/2508.00271)
*Hongjin Qian,Zheng Liu*

Main category: cs.AI

TL;DR: MetaAgent是一种基于学习-实践原则的智能代理范式，通过持续自我改进和工具学习提升知识发现能力。


<details>
  <summary>Details</summary>
Motivation: 旨在通过实践和自我反思提升代理的推理和工具使用能力，实现无需模型参数调整的持续改进。

Method: MetaAgent从基础能力出发，通过生成自然语言请求、工具路由、自我反思和知识库构建逐步优化。

Result: 在GAIA、WebWalkerQA和BrowseCamp等基准测试中表现优异，超越基线并媲美端到端训练代理。

Conclusion: MetaAgent展示了自进化代理系统在通用知识发现中的潜力，为未来智能代理发展提供了新方向。

Abstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the
principle of learning-by-doing, where expertise is developed through hands-on
practice and continual self-improvement. MetaAgent starts with a minimal
workflow, equipped only with basic reasoning and adaptive help-seeking
abilities. When a knowledge gap is encountered, MetaAgent generates natural
language help requests, which are routed to the most suitable external tool by
a dedicated tool router. As MetaAgent solves tasks, it continually conducts
self-reflection and answer verification, distilling actionable experience into
concise texts that are dynamically incorporated into future task contexts.
Besides, MetaAgent autonomously builds in-house tools and a persistent
knowledge base by organizing its tool-use history, further enhancing its
ability to retrieve and integrate relevant information We term this continual,
data-driven process as \textit{meta tool learning}, through which MetaAgent
incrementally refines its reasoning and tool-use strategies, without changing
model parameters or requiring further post-training. Evaluated on challenging
knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,
MetaAgent consistently outperforms workflow-based baselines and matches or
exceeds end-to-end trained agents, demonstrating the promise of self-evolving
agentic systems for robust, general-purpose knowledge discovery. We provide our
source codes in https://github.com/qhjqhj00/MetaAgent.

</details>


### [11] [Mind the Gap: The Divergence Between Human and LLM-Generated Tasks](https://arxiv.org/abs/2508.00282)
*Yi-Long Lu,Jiajun Song,Chunhui Zhang,Wei Wang*

Main category: cs.AI

TL;DR: 比较人类与LLM（GPT-4o）任务生成行为，发现人类受心理驱动影响，而LLM生成的任务更抽象、缺乏社交和物理性，显示其与人类认知的差距。


<details>
  <summary>Details</summary>
Motivation: 探究LLM是否能模拟人类基于心理驱动的任务生成行为。

Method: 通过任务生成实验比较人类与LLM（GPT-4o）的行为差异。

Result: LLM生成的任务更抽象、缺乏社交和物理性，且与人类心理驱动无关。

Conclusion: LLM与人类认知存在核心差距，需融入内在动机和物理基础以设计更人类化的代理。

Abstract: Humans constantly generate a diverse range of tasks guided by internal
motivations. While generative agents powered by large language models (LLMs)
aim to simulate this complex behavior, it remains uncertain whether they
operate on similar cognitive principles. To address this, we conducted a
task-generation experiment comparing human responses with those of an LLM agent
(GPT-4o). We find that human task generation is consistently influenced by
psychological drivers, including personal values (e.g., Openness to Change) and
cognitive style. Even when these psychological drivers are explicitly provided
to the LLM, it fails to reflect the corresponding behavioral patterns. They
produce tasks that are markedly less social, less physical, and thematically
biased toward abstraction. Interestingly, while the LLM's tasks were perceived
as more fun and novel, this highlights a disconnect between its linguistic
proficiency and its capacity to generate human-like, embodied goals.We conclude
that there is a core gap between the value-driven, embodied nature of human
cognition and the statistical patterns of LLMs, highlighting the necessity of
incorporating intrinsic motivation and physical grounding into the design of
more human-aligned agents.

</details>


### [12] [Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning](https://arxiv.org/abs/2508.00323)
*Jianyi Zhang,Xu Ji,Ziyin Zhou,Yuchen Zhou,Shubo Shi,Haoyu Wu,Zhen Li,Shizhao Liu*

Main category: cs.AI

TL;DR: ReasonBench是首个专注于结构化图形推理任务的评估基准，用于评估视觉语言模型（VLMs）在复杂图形推理中的表现，揭示了当前模型的局限性，并提出优化策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注简单图形，而VLMs在复杂图形推理和抽象问题解决方面表现不足，因此需要更全面的评估工具。

Method: 提出ReasonBench基准，包含1,613个真实智力测试问题，评估11种主流VLMs，并提出DiaCoT和ReasonTune优化策略。

Result: 实验显示当前VLMs存在显著局限性，优化策略使模型性能提升33.5%。

Conclusion: ReasonBench为复杂图形推理提供了全面评估工具，优化策略显著提升了VLMs性能。

Abstract: Evaluating the performance of visual language models (VLMs) in graphic
reasoning tasks has become an important research topic. However, VLMs still
show obvious deficiencies in simulating human-level graphic reasoning
capabilities, especially in complex graphic reasoning and abstract problem
solving, which are less studied and existing studies only focus on simple
graphics. To evaluate the performance of VLMs in complex graphic reasoning, we
propose ReasonBench, the first evaluation benchmark focused on structured
graphic reasoning tasks, which includes 1,613 questions from real-world
intelligence tests. ReasonBench covers reasoning dimensions related to
location, attribute, quantity, and multi-element tasks, providing a
comprehensive evaluation of the performance of VLMs in spatial, relational, and
abstract reasoning capabilities. We benchmark 11 mainstream VLMs (including
closed-source and open-source models) and reveal significant limitations of
current models. Based on these findings, we propose a dual optimization
strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability
of reasoning by decomposing layers, and ReasonTune enhances the task
adaptability of model reasoning through training, all of which improves VLM
performance by 33.5\%. All experimental data and code are in the repository:
https://huggingface.co/datasets/cistine/ReasonBench.

</details>


### [13] [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/abs/2508.00324)
*Yeonjun In,Wonjoong Kim,Sangwu Park,Chanyoung Park*

Main category: cs.AI

TL;DR: R1-Act是一种简单高效的后训练方法，通过结构化推理过程显式触发安全知识，显著提升大型推理模型的安全性，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 研究发现大型推理模型已具备足够的安全知识，但在推理过程中未能激活，导致安全隐患。

Method: 提出R1-Act方法，通过少量训练数据和短时间训练显式触发模型的安全知识。

Result: R1-Act在多个模型上表现出强大的安全性改进和实用性，仅需1000个训练样本和90分钟训练时间。

Conclusion: R1-Act是一种高效、可扩展且实用的方法，显著提升了模型的安全性。

Abstract: Although large reasoning models (LRMs) have demonstrated impressive
capabilities on complex tasks, recent studies reveal that these models
frequently fulfill harmful user instructions, raising significant safety
concerns. In this paper, we investigate the underlying cause of LRM safety
risks and find that models already possess sufficient safety knowledge but fail
to activate it during reasoning. Based on this insight, we propose R1-Act, a
simple and efficient post-training method that explicitly triggers safety
knowledge through a structured reasoning process. R1-Act achieves strong safety
improvements while preserving reasoning performance, outperforming prior
alignment methods. Notably, it requires only 1,000 training examples and 90
minutes of training on a single RTX A6000 GPU. Extensive experiments across
multiple LRM backbones and sizes demonstrate the robustness, scalability, and
practical efficiency of our approach.

</details>


### [14] [CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding](https://arxiv.org/abs/2508.00378)
*Shixin Yi,Lin Shang*

Main category: cs.AI

TL;DR: CoRGI框架通过视觉验证提升多模态推理的鲁棒性，减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 解决CoT提示在视觉语言模型中产生的缺乏视觉依据的幻觉问题。

Method: 提出CoRGI框架，分三阶段：生成文本推理链、提取视觉证据、综合生成答案。

Result: 在VCR基准测试中提升推理性能，人类评估显示更事实性和有帮助的解释。

Conclusion: 视觉证据对增强多模态推理鲁棒性至关重要。

Abstract: Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in
vision-language models (VLMs), but it often produces explanations that are
linguistically fluent yet lack grounding in visual content. We observe that
such hallucinations arise in part from the absence of an explicit verification
mechanism during multi-step reasoning. To address this, we propose
\textbf{CoRGI}(\textbf{C}hain \textbf{o}f \textbf{R}easoning with
\textbf{G}rounded \textbf{I}nsights), a modular framework that introduces
visual verification into the reasoning process. CoRGI follows a three-stage
pipeline: it first generates a textual reasoning chain, then extracts
supporting visual evidence for each reasoning step via a dedicated module
(VEVM), and finally synthesizes the textual rationale with visual evidence to
generate a grounded, verified answer. The framework can be integrated with
existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR
benchmark and find that it improves reasoning performance on two representative
open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm
the contribution of each step in the verification module, and human evaluations
suggest that CoRGI leads to more factual and helpful explanations. We also
examine alternative designs for the visual verification step and discuss
potential limitations of post-hoc verification frameworks. These findings
highlight the importance of grounding intermediate reasoning steps in visual
evidence to enhance the robustness of multimodal reasoning.

</details>


### [15] [Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation](https://arxiv.org/abs/2508.00401)
*Riddhi J. Pitliya,Ozan Catal,Toon Van de Maele,Corrado Pezzato,Tim Verbelen*

Main category: cs.AI

TL;DR: 提出了一种在多智能体合作中实现心智理论（ToM）的新方法，通过主动推理框架，使智能体能够推断其他智能体的信念和目标，从而更好地合作。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体合作方法依赖任务特定的共享生成模型或显式通信，限制了其通用性。本研究旨在通过ToM实现更通用的合作机制。

Method: 在主动推理框架中，智能体维护自身和他人的信念与目标的独立表示，并扩展推理树规划算法以递归推理联合策略空间。

Result: 在碰撞避免和觅食任务中，ToM智能体表现出更好的合作能力，能够避免碰撞并减少冗余努力。

Conclusion: 该方法不仅推动了人工智能的实际应用，还为ToM提供了计算层面的新见解。

Abstract: We present a novel approach to multi-agent cooperation by implementing theory
of mind (ToM) within active inference. ToM - the ability to understand that
others can have differing knowledge and goals - enables agents to reason about
others' beliefs while planning their own actions. Unlike previous active
inference approaches to multi-agent cooperation, our method neither relies on
task-specific shared generative models nor requires explicit communication,
while being generalisable. In our framework, the ToM-equipped agent maintains
distinct representations of its own and others' beliefs and goals. We extend
the sophisticated inference tree-based planning algorithm to systematically
explore joint policy spaces through recursive reasoning. Our approach is
evaluated through collision avoidance and foraging task simulations. Results
demonstrate that ToM-equipped agents cooperate better compared to non-ToM
counterparts by being able to avoid collisions and reduce redundant efforts.
Crucially, ToM agents accomplish this by inferring others' beliefs solely from
observable behaviour. This work advances practical applications in artificial
intelligence while providing computational insights into ToM.

</details>


### [16] [Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training](https://arxiv.org/abs/2508.00414)
*Tianqing Fang,Zhisong Zhang,Xiaoyang Wang,Rui Wang,Can Qin,Yuxuan Wan,Jun-Yu Ma,Ce Zhang,Jiaqi Chen,Xiyun Li,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: Cognitive Kernel-Pro是一个完全开源且免费的多模块代理框架，旨在推动高级AI代理的开发和评估。


<details>
  <summary>Details</summary>
Motivation: 当前代理系统多为闭源或依赖付费API，限制了研究的可访问性和可重复性。

Method: 系统研究了高质量训练数据的整理，包括查询、轨迹和可验证答案的构建，并探索了代理测试时的反思和投票策略。

Result: 在GAIA上评估，Cognitive Kernel-Pro在开源和免费代理中取得了最先进的结果，8B参数模型超越了之前的领先系统。

Conclusion: Cognitive Kernel-Pro为可访问的高能力AI代理设立了新的性能标准。

Abstract: General AI Agents are increasingly recognized as foundational frameworks for
the next generation of artificial intelligence, enabling complex reasoning, web
interaction, coding, and autonomous research capabilities. However, current
agent systems are either closed-source or heavily reliant on a variety of paid
APIs and proprietary tools, limiting accessibility and reproducibility for the
research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a
fully open-source and (to the maximum extent) free multi-module agent framework
designed to democratize the development and evaluation of advanced AI agents.
Within Cognitive Kernel-Pro, we systematically investigate the curation of
high-quality training data for Agent Foundation Models, focusing on the
construction of queries, trajectories, and verifiable answers across four key
domains: web, file, code, and general reasoning. Furthermore, we explore novel
strategies for agent test-time reflection and voting to enhance agent
robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving
state-of-the-art results among open-source and free agents. Notably, our
8B-parameter open-source model surpasses previous leading systems such as
WebDancer and WebSailor, establishing a new performance standard for
accessible, high-capability AI agents. Code is available at
https://github.com/Tencent/CognitiveKernel-Pro

</details>


### [17] [Thinking Machines: Mathematical Reasoning in the Age of LLMs](https://arxiv.org/abs/2508.00459)
*Andrea Asperti,Alberto Naibo,Claudio Sacerdoti Coen*

Main category: cs.AI

TL;DR: LLMs在结构化推理和符号任务中表现出色，但在形式数学（如定理证明）中进展缓慢，引发了对LLM推理和监督方式的探讨。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在形式数学中的局限性，分析其与代码合成的差异，并研究其内部是否跟踪逻辑状态。

Method: 综述当前领域的最新模型和基准，探讨三个核心问题：形式与非形式数学的训练权衡、证明生成脆弱性的原因、LLM是否真正表示逻辑状态。

Result: 指出LLMs在形式数学中的当前限制，并探讨可能的扩展方向。

Conclusion: 目标是明确当前技术的边界，并探索如何突破这些限制，而非划定硬性界限。

Abstract: Large Language Models (LLMs) have shown remarkable abilities in structured
reasoning and symbolic tasks, with coding emerging as a particular area of
strength. This success has sparked growing interest in applying LLMs to
mathematics, both in informal problem-solving and formal theorem proving.
However, progress in formal mathematics has proven to be significantly more
difficult, despite surface-level similarities between programming and proof
construction. This discrepancy raises important questions about how LLMs
``reason'', how they are supervised, and whether they internally track a notion
of computational or deductive state. In this article, we address the
state-of-the-art of the discipline, focusing on recent models and benchmarks,
and explore three central issues at the intersection of machine learning and
mathematical cognition: (i) the trade-offs between formal and informal
mathematics as training domains; (ii) the deeper reasons why proof generation
remains more brittle than code synthesis; (iii) and the question of whether
LLMs represent, or merely mimic, a notion of evolving logical state. Our goal
is not to draw hard boundaries, but to identify where the current limits lie,
and how they might be extended.

</details>


### [18] [Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking](https://arxiv.org/abs/2508.00500)
*Haoyu Wang,Chris M. Poskitt,Jun Sun,Jiali Wei*

Main category: cs.AI

TL;DR: Pro2Guard是一个基于概率可达性分析的主动运行时安全框架，用于预测和防止LLM代理的不安全行为。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的安全系统（如AgentSpec）缺乏预见性，难以处理长期依赖和分布偏移，因此需要一种更主动的方法来预防风险。

Method: Pro2Guard将代理行为抽象为符号状态，并从执行轨迹中学习离散时间马尔可夫链（DTMC），在运行时通过估计到达不安全状态的概率来预测风险。

Result: 在家庭代理和自动驾驶场景中，Pro2Guard分别实现了93.6%和100%的风险预测率，并能提前干预。

Conclusion: Pro2Guard通过主动预测和干预，显著提升了LLM代理的安全性，同时保持了任务完成率。

Abstract: Large Language Model (LLM) agents exhibit powerful autonomous capabilities
across domains such as robotics, virtual assistants, and web automation.
However, their stochastic behavior introduces significant safety risks that are
difficult to anticipate. Existing rule-based enforcement systems, such as
AgentSpec, focus on developing reactive safety rules, which typically respond
only when unsafe behavior is imminent or has already occurred. These systems
lack foresight and struggle with long-horizon dependencies and distribution
shifts. To address these limitations, we propose Pro2Guard, a proactive runtime
enforcement framework grounded in probabilistic reachability analysis.
Pro2Guard abstracts agent behaviors into symbolic states and learns a
Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it
anticipates future risks by estimating the probability of reaching unsafe
states, triggering interventions before violations occur when the predicted
risk exceeds a user-defined threshold. By incorporating semantic validity
checks and leveraging PAC bounds, Pro2Guard ensures statistical reliability
while approximating the underlying ground-truth model. We evaluate Pro2Guard
extensively across two safety-critical domains: embodied household agents and
autonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early
on up to 93.6% of unsafe tasks using low thresholds, while configurable modes
(e.g., reflect) allow balancing safety with task success, maintaining up to
80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%
prediction of traffic law violations and collisions, anticipating risks up to
38.66 seconds ahead.

</details>


### [19] [MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models](https://arxiv.org/abs/2508.00576)
*Zhanliang Wang,Kai Wang*

Main category: cs.AI

TL;DR: MultiSHAP是一个模型无关的可解释性框架，利用Shapley交互指数量化多模态AI模型中视觉和文本元素的协同效应，适用于开源和闭源模型。


<details>
  <summary>Details</summary>
Motivation: 多模态AI模型的“黑盒”特性在高风险应用中部署时存在可解释性和可信度问题，现有方法无法精确量化模态间的协同效应。

Method: 提出MultiSHAP框架，基于Shapley交互指数，对细粒度视觉和文本元素的成对交互进行归因分析。

Result: 实验证明MultiSHAP能准确捕捉跨模态推理机制，并提供实例级和数据集级的解释。

Conclusion: MultiSHAP为复杂多模态AI模型提供了一种通用的可解释性解决方案，且可扩展到两模态以上。

Abstract: Multimodal AI models have achieved impressive performance in tasks that
require integrating information from multiple modalities, such as vision and
language. However, their "black-box" nature poses a major barrier to deployment
in high-stakes applications where interpretability and trustworthiness are
essential. How to explain cross-modal interactions in multimodal AI models
remains a major challenge. While existing model explanation methods, such as
attention map and Grad-CAM, offer coarse insights into cross-modal
relationships, they cannot precisely quantify the synergistic effects between
modalities, and are limited to open-source models with accessible internal
weights. Here we introduce MultiSHAP, a model-agnostic interpretability
framework that leverages the Shapley Interaction Index to attribute multimodal
predictions to pairwise interactions between fine-grained visual and textual
elements (such as image patches and text tokens), while being applicable to
both open- and closed-source models. Our approach provides: (1) instance-level
explanations that reveal synergistic and suppressive cross-modal effects for
individual samples - "why the model makes a specific prediction on this input",
and (2) dataset-level explanation that uncovers generalizable interaction
patterns across samples - "how the model integrates information across
modalities". Experiments on public multimodal benchmarks confirm that MultiSHAP
faithfully captures cross-modal reasoning mechanisms, while real-world case
studies demonstrate its practical utility. Our framework is extensible beyond
two modalities, offering a general solution for interpreting complex multimodal
AI models.

</details>


### [20] [From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation](https://arxiv.org/abs/2508.00581)
*Ruiqing Ding,Qianfang Sun,Yongkang Leng,Hui Yin,Xiaojian Li*

Main category: cs.AI

TL;DR: 提出了一种多阶段LLM驱动框架，用于从复杂电子病历生成全面的预咨询问卷，解决了直接LLM方法在信息完整性、逻辑顺序和疾病级合成方面的不足。


<details>
  <summary>Details</summary>
Motivation: 预咨询是医疗保健交付的关键环节，但直接从复杂、大量的电子病历生成问卷存在挑战，尤其是信息完整性、逻辑顺序和疾病级合成的问题。

Method: 采用三阶段框架：1) 提取原子断言；2) 构建个人因果网络并合成疾病知识；3) 生成个性化及标准化问卷。

Result: 在真实电子病历数据集上评估，临床专家验证，方法在信息覆盖、诊断相关性、可理解性和生成时间上表现优越。

Conclusion: 该框架通过构建显式临床知识克服了直接方法的限制，展示了提升患者信息收集的潜力。

Abstract: Pre-consultation is a critical component of effective healthcare delivery.
However, generating comprehensive pre-consultation questionnaires from complex,
voluminous Electronic Medical Records (EMRs) is a challenging task. Direct
Large Language Model (LLM) approaches face difficulties in this task,
particularly regarding information completeness, logical order, and
disease-level synthesis. To address this issue, we propose a novel multi-stage
LLM-driven framework: Stage 1 extracts atomic assertions (key facts with
timing) from EMRs; Stage 2 constructs personal causal networks and synthesizes
disease knowledge by clustering representative networks from an EMR corpus;
Stage 3 generates tailored personal and standardized disease-specific
questionnaires based on these structured representations. This framework
overcomes limitations of direct methods by building explicit clinical
knowledge. Evaluated on a real-world EMR dataset and validated by clinical
experts, our method demonstrates superior performance in information coverage,
diagnostic relevance, understandability, and generation time, highlighting its
practical potential to enhance patient information collection.

</details>


### [21] [Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings](https://arxiv.org/abs/2508.00632)
*Alexia Jolicoeur-Martineau*

Main category: cs.AI

TL;DR: 论文提出AVR-Eval评估指标和AVR-Agent多智能体系统，用于生成和评估交互式音视频内容，发现当前模型在利用高质量资源和反馈方面与人类存在差距。


<details>
  <summary>Details</summary>
Motivation: 解决AI生成复杂交互式音视频内容（如游戏）时缺乏自动化评估指标和难以处理复杂内容的问题。

Method: 提出AVR-Eval评估指标，通过多模态模型比较音视频内容质量；开发AVR-Agent多智能体系统，生成并迭代优化JavaScript代码。

Result: AVR-Agent生成的内容在评估中表现优于单次生成内容，但模型未能有效利用自定义资源和反馈。

Conclusion: 当前模型在利用高质量资源和音视频反馈方面与人类存在显著差距，揭示了机器与人类内容创作的根本差异。

Abstract: While AI excels at generating text, audio, images, and videos, creating
interactive audio-visual content such as video games remains challenging.
Current LLMs can generate JavaScript games and animations, but lack automated
evaluation metrics and struggle with complex content that normally requires
teams of humans working for many months (multi-shot, multi-agents) using assets
made by artists. To tackle these issues, we built a new metric and a
multi-agent system.
  We propose AVR-Eval, a relative metric for multimedia content quality using
Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video,
and audio) compares the AVRs of two contents, with a text model reviewing
evaluations to determine superiority. We show that AVR-Eval properly identifies
good from broken or mismatched content.
  We built AVR-Agent, a multi-agent system generating JavaScript code from a
bank of multimedia assets (audio, images, 3D models). The coding agent selects
relevant assets, generates multiple initial codes, uses AVR-Eval to identify
the best version, and iteratively improves it through omni-modal agent feedback
from the AVR.
  We run experiments on games and animations with AVR-Eval (win rate of content
A against B). We find that content generated by AVR-Agent has a significantly
higher win rate against content made through one-shot generation. However,
models struggle to leverage custom assets and AVR feedback effectively, showing
no higher win rate. This reveals a critical gap: while humans benefit from
high-quality assets and audio-visual feedback, current coding models do not
seem to utilize these resources as effectively, highlighting fundamental
differences between human and machine content creation approaches.

</details>


### [22] [Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies](https://arxiv.org/abs/2508.00658)
*Chakattrai Sookkongwaree,Tattep Lakmuang,Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 论文提出了一种多频带变滞后格兰杰因果性（MB-VLGC）框架，解决了传统方法无法处理频带依赖性因果延迟的问题。


<details>
  <summary>Details</summary>
Motivation: 传统格兰杰因果性方法假设固定的滞后时间，而变滞后方法（VLGC）虽然允许滞后时间变化，但忽略了因果延迟可能在不同频带中不同的问题。

Method: 提出了MB-VLGC框架，显式建模频带依赖性因果延迟，并提供了理论证明和高效推理流程。

Result: 在合成和真实数据集上的实验表明，MB-VLGC显著优于现有方法。

Conclusion: MB-VLGC框架具有广泛适用性，适用于任何类型的时间序列数据。

Abstract: Understanding causal relationships in time series is fundamental to many
domains, including neuroscience, economics, and behavioral science. Granger
causality is one of the well-known techniques for inferring causality in time
series. Typically, Granger causality frameworks have a strong fix-lag
assumption between cause and effect, which is often unrealistic in complex
systems. While recent work on variable-lag Granger causality (VLGC) addresses
this limitation by allowing a cause to influence an effect with different time
lags at each time point, it fails to account for the fact that causal
interactions may vary not only in time delay but also across frequency bands.
For example, in brain signals, alpha-band activity may influence another region
with a shorter delay than slower delta-band oscillations. In this work, we
formalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a
novel framework that generalizes traditional VLGC by explicitly modeling
frequency-dependent causal delays. We provide a formal definition of MB-VLGC,
demonstrate its theoretical soundness, and propose an efficient inference
pipeline. Extensive experiments across multiple domains demonstrate that our
framework significantly outperforms existing methods on both synthetic and
real-world datasets, confirming its broad applicability to any type of time
series data. Code and datasets are publicly available.

</details>


### [23] [Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI](https://arxiv.org/abs/2508.00665)
*Maryam Mosleh,Marie Devlin,Ellis Solaiman*

Main category: cs.AI

TL;DR: 论文提出了一种结合传统XAI技术与生成式AI模型的混合框架，旨在为教育领域提供个性化、多模态的解释，以增强透明度和用户体验。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的自适应学习系统缺乏透明度，且大多数XAI技术忽视了用户角色和理解能力。

Method: 提出了一种混合框架，整合传统XAI技术、生成式AI模型和用户个性化，生成多模态、个性化的解释。

Result: 重新定义了可解释性为动态沟通过程，并提出了框架设计及研究方向。

Conclusion: 目标是推动可解释AI在增强透明度的同时支持以用户为中心的体验。

Abstract: Artificial intelligence-driven adaptive learning systems are reshaping
education through data-driven adaptation of learning experiences. Yet many of
these systems lack transparency, offering limited insight into how decisions
are made. Most explainable AI (XAI) techniques focus on technical outputs but
neglect user roles and comprehension. This paper proposes a hybrid framework
that integrates traditional XAI techniques with generative AI models and user
personalisation to generate multimodal, personalised explanations tailored to
user needs. We redefine explainability as a dynamic communication process
tailored to user roles and learning goals. We outline the framework's design,
key XAI limitations in education, and research directions on accuracy,
fairness, and personalisation. Our aim is to move towards explainable AI that
enhances transparency while supporting user-centred experiences.

</details>


### [24] [Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations](https://arxiv.org/abs/2508.00674)
*Banan Alkhateeb,Ellis Solaiman*

Main category: cs.AI

TL;DR: 论文提出了一种用户分段的上下文感知解释系统，通过可视化方法提升社交媒体推荐的可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体推荐系统缺乏针对用户特定需求的解释性，导致用户不理解推荐原因。

Method: 提出了一种视觉解释系统，根据用户需求和上下文提供不同形式的解释（如技术细节版和简化版）。

Result: 该系统首次在单一流程中联合调整解释风格（视觉 vs. 数字）和粒度（专家 vs. 普通用户）。

Conclusion: 通过30名X用户的公开试点验证其对决策和信任的影响。

Abstract: Social media platforms today strive to improve user experience through AI
recommendations, yet the value of such recommendations vanishes as users do not
understand the reasons behind them. This issue arises because explainability in
social media is general and lacks alignment with user-specific needs. In this
vision paper, we outline a user-segmented and context-aware explanation layer
by proposing a visual explanation system with diverse explanation methods. The
proposed system is framed by the variety of user needs and contexts, showing
explanations in different visualized forms, including a technically detailed
version for AI experts and a simplified one for lay users. Our framework is the
first to jointly adapt explanation style (visual vs. numeric) and granularity
(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will
validate its impact on decision-making and trust.

</details>


### [25] [Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics](https://arxiv.org/abs/2508.00784)
*Tom Or,Omri Azencot*

Main category: cs.AI

TL;DR: 提出一种基于预训练多模态模型的通用生成内容检测方法，在多种数据模态上实现高效、快速的检测。


<details>
  <summary>Details</summary>
Motivation: 现有检测器通常局限于特定生成器和数据模态，无法通用，亟需一种能跨模态和生成类别的鲁棒检测方法。

Method: 利用预训练多模态模型的潜在编码特征，训练线性分类器进行真假内容判别。

Result: 在音频和图像领域，该方法在少样本和计算效率方面均优于或匹配现有基线方法。

Conclusion: 预训练多模态模型的潜在编码能有效捕捉真假内容的区分信息，为通用生成内容检测提供了高效解决方案。

Abstract: Generative models achieve remarkable results in multiple data domains,
including images and texts, among other examples. Unfortunately, malicious
users exploit synthetic media for spreading misinformation and disseminating
deepfakes. Consequently, the need for robust and stable fake detectors is
pressing, especially when new generative models appear everyday. While the
majority of existing work train classifiers that discriminate between real and
fake information, such tools typically generalize only within the same family
of generators and data modalities, yielding poor results on other generative
classes and data domains. Towards a universal classifier, we propose the use of
large pre-trained multi-modal models for the detection of generative content.
Effectively, we show that the latent code of these models naturally captures
information discriminating real from fake. Building on this observation, we
demonstrate that linear classifiers trained on these features can achieve
state-of-the-art results across various modalities, while remaining
computationally efficient, fast to train, and effective even in few-shot
settings. Our work primarily focuses on fake detection in audio and images,
achieving performance that surpasses or matches that of strong baseline
methods.

</details>
